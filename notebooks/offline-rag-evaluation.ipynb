{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82e471cb-7aec-4c00-9eed-36e6f2447eb5",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "Loading documents and ground truth data for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59c8e85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "import minsearch\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1344ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 149 documents and 735 ground truth questions\n"
     ]
    }
   ],
   "source": [
    "# Load documents from processed JSON file\n",
    "with open('../data/processed/documents-with-ids.json', 'r') as f:\n",
    "    documents = json.load(f)\n",
    "\n",
    "# Load ground truth dataset for evaluation from CSV file\n",
    "df_ground_truth = pd.read_csv('../data/processed/ground-truth-retrieval.csv')\n",
    "ground_truth = df_ground_truth.to_dict(orient='records')\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents and {len(ground_truth)} ground truth questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f745e802-1cb5-4e77-afa0-a4e276270a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'location': 'Andhra_Pradesh',\n",
       " 'doc_id': 'd4402d82c0',\n",
       " 'content': \"Andhra Pradesh is known for its rich history, architecture and culture. Andhra Pradesh has a variety of tourist attractions including beaches, hills, wildlife, forests and temples. Like rest of the Southern India, the culture of Andhra Pradesh is essentially Dravidian, quite different from North India's Sanskrit Hindu culture. Andhra Pradesh was part of the British Madras presidency and then independent India's Madras State until 1953, when Andhra State was formed, with the capital being\",\n",
       " 'id': '450a9d36'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b830c7b-03a9-44be-93cd-3a0e7da12eed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What are the must-see religious temples to visit in Andhra Pradesh?',\n",
       " 'id': 'db0beb52'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ab3129",
   "metadata": {},
   "source": [
    "###  Joining ground-truth-retrieval.csv with documents-with-ids.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a41fe1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Complete question-document pairs: 735\n",
      "Ground truth questions without matching documents: 0\n"
     ]
    }
   ],
   "source": [
    "# Create dictionary for efficient document lookup\n",
    "documents_dict = {doc['id']: doc for doc in documents}\n",
    "\n",
    "# Create joined data with only complete question-document pairs\n",
    "complete_pairs = []\n",
    "\n",
    "for gt in ground_truth:\n",
    "    gt_id = gt['id']\n",
    "    \n",
    "    # Only include if we have both question and document\n",
    "    if gt_id in documents_dict:\n",
    "        doc_record = documents_dict[gt_id]\n",
    "        \n",
    "        # Create combined record\n",
    "        joined_record = {\n",
    "            'id': gt_id,\n",
    "            'question': gt['question'],\n",
    "            'location': doc_record['location'],\n",
    "            'doc_id': doc_record['doc_id'],\n",
    "            'content': doc_record['content']\n",
    "        }\n",
    "        \n",
    "        # Add any other fields from ground truth (if there are more columns)\n",
    "        for key, value in gt.items():\n",
    "            if key not in ['id', 'question']:\n",
    "                joined_record[key] = value\n",
    "        \n",
    "        # Add any other fields from documents (if there are more columns)\n",
    "        for key, value in doc_record.items():\n",
    "            if key not in ['id', 'location', 'doc_id', 'content']:\n",
    "                joined_record[key] = value\n",
    "        \n",
    "        complete_pairs.append(joined_record)\n",
    "\n",
    "print(f\"\\nComplete question-document pairs: {len(complete_pairs)}\")\n",
    "print(f\"Ground truth questions without matching documents: {len(ground_truth) - len(complete_pairs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4681662b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of complete question-document pairs:\n",
      "Pair 1:\n",
      "{\n",
      "  \"id\": \"4f80b327\",\n",
      "  \"question\": \"What are the must-see religious sites in Andhra Pradesh for pilgrims?\",\n",
      "  \"location\": \"Andhra_Pradesh\",\n",
      "  \"doc_id\": \"d4402d82c0\",\n",
      "  \"content\": \"Asia > South Asia > India > Southern India > Andhra Pradesh  \\n![0_image_0.png](0_image_0.png)\"\n",
      "}\n",
      "--------------------------------------------------\n",
      "Pair 2:\n",
      "{\n",
      "  \"id\": \"4f80b327\",\n",
      "  \"question\": \"Which natural attractions and caves can tourists explore in Andhra Pradesh?\",\n",
      "  \"location\": \"Andhra_Pradesh\",\n",
      "  \"doc_id\": \"d4402d82c0\",\n",
      "  \"content\": \"Asia > South Asia > India > Southern India > Andhra Pradesh  \\n![0_image_0.png](0_image_0.png)\"\n",
      "}\n",
      "--------------------------------------------------\n",
      "\n",
      "Recommendation: Use 'question_document_pairs.json' for your retrieval evaluation tasks.\n",
      "This file contains only complete question-document pairs that can be used for training/evaluation.\n"
     ]
    }
   ],
   "source": [
    "# Show sample of the final data\n",
    "print(f\"\\nSample of complete question-document pairs:\")\n",
    "for i, record in enumerate(complete_pairs[:2]):\n",
    "    print(f\"Pair {i+1}:\")\n",
    "    print(json.dumps(record, indent=2, ensure_ascii=False))\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(f\"\\nRecommendation: Use 'question_document_pairs.json' for your retrieval evaluation tasks.\")\n",
    "print(f\"This file contains only complete question-document pairs that can be used for training/evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93ccc55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved complete pairs to: '../data/processed/question_document_pairs.json'\n"
     ]
    }
   ],
   "source": [
    "# Save the single recommended file\n",
    "with open('../data/processed/question_document_pairs.json', 'w') as f:\n",
    "    json.dump(complete_pairs, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\nSaved complete pairs to: '../data/processed/question_document_pairs.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e585c6e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '4f80b327',\n",
       " 'question': 'What are the must-see religious sites in Andhra Pradesh for pilgrims?',\n",
       " 'location': 'Andhra_Pradesh',\n",
       " 'doc_id': 'd4402d82c0',\n",
       " 'content': 'Asia > South Asia > India > Southern India > Andhra Pradesh  \\n![0_image_0.png](0_image_0.png)'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_pairs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cfd91ec-8ec8-4570-bff7-228c5eb7327b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Northern Coast (Alluri Sitharama Raju, Anakapalli, East Godavari, Kakinada, Konaseema, Parvathipuram Manyam, Srikakulam, Visakhapatnam, Vizianagaram, Yanam) Central Coast (Eluru, Krishna, NTR, West Godavari) Southern Coast (Bapatla, Guntur, Nellore, Palnadu, Prakasam, Tirupati) Rayalaseema (Annamayya, Anantapur, Chittoor, Kadapa, Kurnool, Nandyal, Sri Sathya Sai)  \\n![0_image_1.png](0_image_1.png) interactive map'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_idx = {d['id']: d for d in complete_pairs}\n",
    "doc_idx['db0beb52']['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9042aa10-abed-4009-b7af-769d8b477cd0",
   "metadata": {},
   "source": [
    "## Index data\n",
    "\n",
    "We'll generate embeddings using [the sentence transformers](https://sbert.net/) library, if you don't have it, install it with pip:\n",
    "\n",
    "```bash\n",
    "pip install sentence-transformers\n",
    "```\n",
    "\n",
    "This is a different way of turning sentences into vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f61b1fe-f67a-4217-9fbe-5fd57eefd53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_name = 'multi-qa-MiniLM-L6-cos-v1'\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae16a806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your joined question-document pairs\n",
    "with open('../data/processed/question_document_pairs.json', 'r') as f:\n",
    "    documents = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "573d0d1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '4f80b327',\n",
       " 'question': 'What are the must-see religious sites in Andhra Pradesh for pilgrims?',\n",
       " 'location': 'Andhra_Pradesh',\n",
       " 'doc_id': 'd4402d82c0',\n",
       " 'content': 'Asia > South Asia > India > Southern India > Andhra Pradesh  \\n![0_image_0.png](0_image_0.png)'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bd33453-4885-4c59-a566-3f5f405e2622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6cc979db3cf4e5c90ef612f40c17b10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/735 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 735 vectors for 735 question-document pairs\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "vectors = []\n",
    "\n",
    "for doc in tqdm(documents):\n",
    "    question = doc['question']\n",
    "    text = doc['content']\n",
    "    # Combine question and content for better semantic representation\n",
    "    vector = model.encode(question + ' ' + text)\n",
    "    vectors.append(vector)\n",
    "print(f\"Generated {len(vectors)} vectors for {len(documents)} question-document pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ea8cb68-5ac7-40ac-b1ff-d9363effd903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "vectors = np.array(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2492ea39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.vector.VectorSearch at 0x1d231902c50>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from minsearch.vector import VectorSearch\n",
    "\n",
    "vindex = VectorSearch(keyword_fields=['location'])\n",
    "vindex.fit(vectors, documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666609c9-93d0-4167-8287-392572eaa762",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "244e2790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minsearch_vector_search(vector, location=None):\n",
    "    \"\"\"\n",
    "    Perform vector search using minsearch\n",
    "    Args:\n",
    "        vector: Query vector for similarity search\n",
    "        location: Optional location filter for search results\n",
    "    Returns:\n",
    "        List of search results\n",
    "    \"\"\"\n",
    "    filter_dict = {}\n",
    "    if location:\n",
    "        filter_dict['location'] = location\n",
    "    \n",
    "    return vindex.search(\n",
    "        vector,\n",
    "        filter_dict=filter_dict if filter_dict else None,\n",
    "        num_results=5\n",
    "    )\n",
    "def question_text_vector(q):\n",
    "    \"\"\"\n",
    "    Convert question to vector and perform search\n",
    "    Args:\n",
    "        q: Dictionary containing question and optionally location\n",
    "    Returns:\n",
    "        Search results from vector similarity search\n",
    "    \"\"\"\n",
    "    question = q['question']\n",
    "    location = q.get('location')  # Get location if provided\n",
    "\n",
    "    # Encode question to vector\n",
    "    v_q = model.encode(question)\n",
    "\n",
    "    return minsearch_vector_search(v_q, location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96a7f632-a928-4236-abff-ff2a73d6f463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '5da6b2ce',\n",
       "  'question': 'What are the must-visit temples in Andhra Pradesh for religious tourism?',\n",
       "  'location': 'Andhra_Pradesh',\n",
       "  'doc_id': 'd4402d82c0',\n",
       "  'content': '1  \\n![1_image_0.png](1_image_0.png)'},\n",
       " {'id': 'a88123db',\n",
       "  'question': 'What are the must-see religious temples in Andhra Pradesh?',\n",
       "  'location': 'Andhra_Pradesh',\n",
       "  'doc_id': 'd4402d82c0',\n",
       "  'content': 'Here are some of the most notable cities.'},\n",
       " {'id': 'db0beb52',\n",
       "  'question': 'What are the must-see religious temples to visit in Andhra Pradesh?',\n",
       "  'location': 'Andhra_Pradesh',\n",
       "  'doc_id': 'd4402d82c0',\n",
       "  'content': 'Northern Coast (Alluri Sitharama Raju, Anakapalli, East Godavari, Kakinada, Konaseema, Parvathipuram Manyam, Srikakulam, Visakhapatnam, Vizianagaram, Yanam) Central Coast (Eluru, Krishna, NTR, West Godavari) Southern Coast (Bapatla, Guntur, Nellore, Palnadu, Prakasam, Tirupati) Rayalaseema (Annamayya, Anantapur, Chittoor, Kadapa, Kurnool, Nandyal, Sri Sathya Sai)  \\n![0_image_1.png](0_image_1.png) interactive map'},\n",
       " {'id': 'b184cf08',\n",
       "  'question': 'What are the must-see temples to visit in Andhra Pradesh?',\n",
       "  'location': 'Andhra_Pradesh',\n",
       "  'doc_id': 'd4402d82c0',\n",
       "  'content': 'Odisha Karnataka Tamil Nadu Telangana Retrieved from \"https://en.wikivoyage.org/w/index.php?title=Andhra_Pradesh&oldid=5081274\"'},\n",
       " {'id': '18c12abb',\n",
       "  'question': 'What are the must-visit temples in Andhra Pradesh for cultural and religious tourism?',\n",
       "  'location': 'Andhra_Pradesh',\n",
       "  'doc_id': 'd4402d82c0',\n",
       "  'content': '![2_image_0.png](2_image_0.png)  \\n![2_image_1.png](2_image_1.png)  \\n30 19  \\n![2_image_2.png](2_image_2.png)  \\nAverage max. and min. temperatures in °C Precipitation+Snow totals in mm Climate data (https://en.climate-data.org/asia/i ndia/andhra-pradesh/vijayawada-715084/) of  \\n![2_image_3.png](2_image_3.png)  \\n![2_image_4.png](2_image_4.png)'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_text_vector(dict(\n",
    "    question='What are the must-see religious temples to visit in Andhra Pradesh?',\n",
    "    #location='Karnataka'\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443cb32b-5018-477a-b215-647a1ee0eb9c",
   "metadata": {},
   "source": [
    "## The RAG flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "055c5cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query, search_results):\n",
    "    prompt_template = \"\"\"\n",
    "You're a travel assistant bot that helps users plan their itinerary and discover amazing places to visit. \n",
    "Answer the QUESTION based on the CONTEXT from the travel database.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "When answering, consider:\n",
    "- Must-visit tourist attractions and landmarks\n",
    "- Cultural experiences and local traditions  \n",
    "- Historical significance of places\n",
    "- Best times to visit and travel tips\n",
    "- Local cuisine and specialties (if mentioned in context)\n",
    "- Transportation and accessibility information (if available)\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT: \n",
    "{context}\n",
    "\"\"\".strip()\n",
    "\n",
    "    context = \"\"\n",
    "    \n",
    "    for doc in search_results:\n",
    "        context = context + f\"location: {doc['location']}\\nquestion: {doc['question']}\\ncontent: {doc['content']}\\n\\n\"\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23f7bbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import requests\n",
    "\n",
    "load_dotenv()  # Loads variables from .env\n",
    "API_KEY = os.getenv(\"PERPLEXITY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a8c69c5",
   "metadata": {},
   "outputs": [],
   "source": [
    " #LLM function to support multiple models\n",
    "def llm(prompt, model='sonar'):\n",
    "    \"\"\"\n",
    "    Generate response using different LLM models\n",
    "    Args:\n",
    "        prompt: Input prompt for the model\n",
    "        model: Model to use ('sonar', 'sonar-pro', 'phi3')\n",
    "    Returns:\n",
    "        Generated response text\n",
    "    \"\"\"\n",
    "    if model in ['sonar', 'sonar-pro']:\n",
    "        # Perplexity API call\n",
    "        # calls perplexity API to generate response\n",
    "        response = requests.post(\n",
    "            'https://api.perplexity.ai/chat/completions',\n",
    "            headers={\n",
    "                'Authorization': f'Bearer {API_KEY}',\n",
    "                'Content-Type': 'application/json'\n",
    "            },\n",
    "            json={\n",
    "                'model': model,\n",
    "                'messages': [{\"role\": \"user\", \"content\": prompt}]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return response.json()['choices'][0]['message']['content']\n",
    "        else:\n",
    "            raise Exception(f\"Perplexity API call failed with status {response.status_code}: {response.text}\")\n",
    "    \n",
    "    elif model == 'phi3':\n",
    "        # Ollama API call for local phi3\n",
    "        # calls Ollama's phi3 API to generate response\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                'http://localhost:11434/api/generate',\n",
    "                json={\n",
    "                    'model': 'phi3',\n",
    "                    'prompt': prompt,\n",
    "                    'stream': False\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                return response.json()['response']\n",
    "            else:\n",
    "                raise Exception(f\"Ollama API call failed with status {response.status_code}: {response.text}\")\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            raise Exception(\"Could not connect to Ollama. Make sure Ollama is running on localhost:11434 and phi3 model is installed.\")\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model: {model}. Supported models: 'sonar', 'sonar-pro', 'phi3'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc8dfdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # calls perplexity API to generate response\n",
    "# def llm(prompt, model='sonar'):\n",
    "#     response = requests.post(\n",
    "#         'https://api.perplexity.ai/chat/completions',\n",
    "#         headers={\n",
    "#             'Authorization': f'Bearer {API_KEY}',\n",
    "#             'Content-Type': 'application/json'\n",
    "#         },\n",
    "#         json={\n",
    "#             'model': model,\n",
    "#             'messages': [{\"role\": \"user\", \"content\": prompt}]\n",
    "#         }\n",
    "#     )\n",
    "    \n",
    "#     if response.status_code == 200:\n",
    "#         return response.json()['choices'][0]['message']['content']\n",
    "#     else:\n",
    "#         raise Exception(f\"API call failed with status {response.status_code}: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4eef2ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve relevant documents and generate answer\n",
    "\n",
    "def rag(query: dict, model='sonar') -> str:\n",
    "    search_results = question_text_vector(query)\n",
    "    prompt = build_prompt(query['question'], search_results)\n",
    "    answer = llm(prompt, model=model)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e57cdeb-f5e2-4481-8e86-49d70d6adb74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What are the must-see religious temples to visit in Andhra Pradesh?',\n",
       " 'id': 'db0beb52'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e5a6ad63-5b55-49c2-a115-cdccc4874720",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Perplexity API call failed with status 401: <html>\r\n<head><title>401 Authorization Required</title></head>\r\n<body>\r\n<center><h1>401 Authorization Required</h1></center>\r\n<hr><center>openresty/1.27.4</center>\r\n<script>(function(){function c(){var b=a.contentDocument||a.contentWindow.document;if(b){var d=b.createElement('script');d.innerHTML=\"window.__CF$cv$params={r:'97080fb47aaedfa6',t:'MTc1NTQyMjA2OC4wMDAwMDA='};var a=document.createElement('script');a.nonce='';a.src='/cdn-cgi/challenge-platform/scripts/jsd/main.js';document.getElementsByTagName('head')[0].appendChild(a);\";b.getElementsByTagName('head')[0].appendChild(d)}}if(document.body){var a=document.createElement('iframe');a.height=1;a.width=1;a.style.position='absolute';a.style.top=0;a.style.left=0;a.style.border='none';a.style.visibility='hidden';document.body.appendChild(a);if('loading'!==document.readyState)c();else if(window.addEventListener)document.addEventListener('DOMContentLoaded',c);else{var e=document.onreadystatechange||function(){};document.onreadystatechange=function(b){e(b);'loading'!==document.readyState&&(document.onreadystatechange=e,c())}}}})();</script></body>\r\n</html>\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mground_truth\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msonar\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#testing\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[22], line 6\u001b[0m, in \u001b[0;36mrag\u001b[1;34m(query, model)\u001b[0m\n\u001b[0;32m      4\u001b[0m search_results \u001b[38;5;241m=\u001b[39m question_text_vector(query)\n\u001b[0;32m      5\u001b[0m prompt \u001b[38;5;241m=\u001b[39m build_prompt(query[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m], search_results)\n\u001b[1;32m----> 6\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m answer\n",
      "Cell \u001b[1;32mIn[20], line 29\u001b[0m, in \u001b[0;36mllm\u001b[1;34m(prompt, model)\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 29\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPerplexity API call failed with status \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mphi3\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;66;03m# Ollama API call for local phi3\u001b[39;00m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# calls Ollama's phi3 API to generate response\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mException\u001b[0m: Perplexity API call failed with status 401: <html>\r\n<head><title>401 Authorization Required</title></head>\r\n<body>\r\n<center><h1>401 Authorization Required</h1></center>\r\n<hr><center>openresty/1.27.4</center>\r\n<script>(function(){function c(){var b=a.contentDocument||a.contentWindow.document;if(b){var d=b.createElement('script');d.innerHTML=\"window.__CF$cv$params={r:'97080fb47aaedfa6',t:'MTc1NTQyMjA2OC4wMDAwMDA='};var a=document.createElement('script');a.nonce='';a.src='/cdn-cgi/challenge-platform/scripts/jsd/main.js';document.getElementsByTagName('head')[0].appendChild(a);\";b.getElementsByTagName('head')[0].appendChild(d)}}if(document.body){var a=document.createElement('iframe');a.height=1;a.width=1;a.style.position='absolute';a.style.top=0;a.style.left=0;a.style.border='none';a.style.visibility='hidden';document.body.appendChild(a);if('loading'!==document.readyState)c();else if(window.addEventListener)document.addEventListener('DOMContentLoaded',c);else{var e=document.onreadystatechange||function(){};document.onreadystatechange=function(b){e(b);'loading'!==document.readyState&&(document.onreadystatechange=e,c())}}}})();</script></body>\r\n</html>\r\n"
     ]
    }
   ],
   "source": [
    "rag(ground_truth[10], model='sonar') #testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c52c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag(ground_truth[10], model='sonar-pro') #testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a78d2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"Testing phi3 model:\")\n",
    "    print(rag(ground_truth[10], model='phi3')) # testing\n",
    "except Exception as e:\n",
    "    print(f\"Phi3 model test failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75123a0b-1aea-467c-84d9-ae5f6f6c2256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Northern Coast (Alluri Sitharama Raju, Anakapalli, East Godavari, Kakinada, Konaseema, Parvathipuram Manyam, Srikakulam, Visakhapatnam, Vizianagaram, Yanam) Central Coast (Eluru, Krishna, NTR, West Godavari) Southern Coast (Bapatla, Guntur, Nellore, Palnadu, Prakasam, Tirupati) Rayalaseema (Annamayya, Anantapur, Chittoor, Kadapa, Kurnool, Nandyal, Sri Sathya Sai)  \\n![0_image_1.png](0_image_1.png) interactive map'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_idx['db0beb52']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3109422f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(query, models=['sonar', 'sonar-pro', 'phi3']):\n",
    "    \"\"\"\n",
    "    Compare responses from different models for the same query\n",
    "    Args:\n",
    "        query: Dictionary containing question and optional location\n",
    "        models: List of models to compare\n",
    "    Returns:\n",
    "        Dictionary with model responses\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for model in models:\n",
    "        try:\n",
    "            print(f\"Testing {model}...\")\n",
    "            answer = rag(query, model=model)\n",
    "            results[model] = {\n",
    "                'answer': answer,\n",
    "                'status': 'success'\n",
    "            }\n",
    "        except Exception as e:\n",
    "            results[model] = {\n",
    "                'answer': None,\n",
    "                'status': 'error',\n",
    "                'error': str(e)\n",
    "            }\n",
    "            print(f\"Error with {model}: {e}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d22796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example comparison\n",
    "test_query = ground_truth[10]\n",
    "comparison_results = compare_models(test_query)\n",
    "\n",
    "for model, result in comparison_results.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Model: {model}\")\n",
    "    print(f\"Status: {result['status']}\")\n",
    "    if result['status'] == 'success':\n",
    "        print(f\"Answer: {result['answer'][:200]}...\")\n",
    "    else:\n",
    "        print(f\"Error: {result['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c26035e-ea80-4817-93a9-46a6af4eaf91",
   "metadata": {},
   "source": [
    "## Cosine similarity metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754db44d-5e73-4784-9369-707691fb9229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity: 0.4495590627193451\n"
     ]
    }
   ],
   "source": [
    "# Example of computing cosine similarity between two answers\n",
    "answer_orig = 'The must-see religious temples to visit in Andhra Pradesh include:\\n\\n- **Tirumala Venkateswara Temple** in Tirumala hills, dedicated to Lord Venkateswara. It is highly revered and attracts millions of devotees annually for its spiritual significance and rich traditions[1][3][4].\\n\\n- **Srisailam Mallikarjuna Temple**, a Jyotirlinga temple nestled in the Nallamala forests, dedicated to Lord Shiva and Goddess Parvati, known for its historical importance and Dravidian architecture[1][3][4].\\n\\n- **Sri Kalahasteeswara Temple**, known for Rahu-Ketu dosha remedies and famous for its striking Dravidian architecture, dedicated to Lord Shiva[1].\\n\\n- **Ahobilam Temple** complex located in the Nallamala hills, dedicated to Lord Narasimha in nine forms across nine shrines. It offers spiritual tranquility amidst natural beauty and is a prime pilgrimage site[1][3].\\n\\n- **Kanaka Durga Temple** in Vijayawada, dedicated to Goddess Durga. This temple is renowned for its Dravidian architectural style, religious legends, and location atop Indrakeeladri hills by the Krishna River. It is a major destination for religious tourism in Vijayawada[2][4].\\n\\n- Additional notable temples for religious tourism include **Sri Yagantiswamy (Uma Maheshwara) Temple** at Yaganti in Kurnool district, famous for its Shiva idol carved from a single stone and historical contributions from multiple South Indian dynasties[5].\\n\\nThese temples reflect Andhra Pradesh’s rich cultural heritage, blending architectural grandeur with profound religious significance, offering authentic spiritual experiences. The best time to visit is often during festival seasons specific to each temple, and many temples provide online booking for darshan and seva, enhancing accessibility for pilgrims[4].'\n",
    "answer_llm = 'Northern Coast (Alluri Sitharama Raju, Anakapalli, East Godavari, Kakinada, Konaseema, Parvathipuram Manyam, Srikakulam, Visakhapatnam, Vizianagaram, Yanam) Central Coast (Eluru, Krishna, NTR, West Godavari) Southern Coast (Bapatla, Guntur, Nellore, Palnadu, Prakasam, Tirupati) Rayalaseema (Annamayya, Anantapur, Chittoor, Kadapa, Kurnool, Nandyal, Sri Sathya Sai)  \\n![0_image_1.png](0_image_1.png) interactive map'\n",
    "\n",
    "# Encode both answers to vectors\n",
    "v_llm = model.encode(answer_llm)\n",
    "v_orig = model.encode(answer_orig)\n",
    "\n",
    "# Compute cosine similarity using dot product (vectors are normalized)\n",
    "similarity_score = v_llm.dot(v_orig)\n",
    "print(f\"Cosine similarity: {similarity_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffcee32-a7c0-45ea-aca2-845758ed3ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What are the must-see religious sites in Andhra Pradesh for pilgrims?',\n",
       " 'id': '4f80b327'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f7deed-9730-467d-afc7-f053b23fe8e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "735"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db605e72-f5cf-4402-ab78-4fd831a1c6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9df712",
   "metadata": {},
   "source": [
    "#### Tried parallel processing to optimize code - original code completion time displayed as >1 hour 30 mins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d1c6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADAPTIVE RATE LIMITING - Dynamically adjusts to API limits\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "import random\n",
    "from functools import wraps\n",
    "import threading\n",
    "\n",
    "# Global variables to track rate limiting\n",
    "rate_limit_counter = 0\n",
    "rate_limit_lock = threading.Lock()\n",
    "current_delay = 0.1  # Start with minimal delay\n",
    "max_workers_current = 3  # Will be adjusted dynamically\n",
    "\n",
    "def adaptive_rate_limit_retry(max_retries=3):\n",
    "    \"\"\"\n",
    "    Adaptive retry with increasing delays based on rate limit frequency\n",
    "    \"\"\"\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            global rate_limit_counter, current_delay\n",
    "            \n",
    "            for attempt in range(max_retries):\n",
    "                try:\n",
    "                    return func(*args, **kwargs)\n",
    "                except Exception as e:\n",
    "                    if \"429\" in str(e) or \"rate limit\" in str(e).lower():\n",
    "                        with rate_limit_lock:\n",
    "                            rate_limit_counter += 1\n",
    "                            \n",
    "                        if attempt < max_retries - 1:\n",
    "                            # Adaptive delay based on rate limit frequency\n",
    "                            base_delay = min(5.0, 1.0 + (rate_limit_counter * 0.1))\n",
    "                            delay = base_delay * (2 ** attempt) + random.uniform(0, 2)\n",
    "                            \n",
    "                            print(f\"Rate limit #{rate_limit_counter}, waiting {delay:.1f}s (attempt {attempt + 1}/{max_retries})\")\n",
    "                            time.sleep(delay)\n",
    "                            \n",
    "                            # Increase global delay if too many rate limits\n",
    "                            if rate_limit_counter % 10 == 0:\n",
    "                                current_delay = min(2.0, current_delay + 0.2)\n",
    "                                print(f\"Increased base delay to {current_delay:.1f}s due to frequent rate limits\")\n",
    "                                \n",
    "                            continue\n",
    "                    raise e\n",
    "            return None\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "@adaptive_rate_limit_retry(max_retries=4)\n",
    "def llm_with_adaptive_retry(prompt, model='sonar'):\n",
    "    \"\"\"LLM call with adaptive retry logic\"\"\"\n",
    "    return llm(prompt, model)\n",
    "\n",
    "def rag_with_adaptive_retry(query: dict, model='sonar') -> str:\n",
    "    \"\"\"RAG pipeline with adaptive retry logic\"\"\"\n",
    "    search_results = question_text_vector(query)\n",
    "    prompt = build_prompt(query['question'], search_results)\n",
    "    answer = llm_with_adaptive_retry(prompt, model=model)\n",
    "    return answer\n",
    "\n",
    "def process_single_record_with_model(item, model='sonar'):\n",
    "    \"\"\"Process record with specific model and adaptive delays\"\"\"\n",
    "    i, rec = item\n",
    "    \n",
    "    try:\n",
    "        # Adaptive delay based on current rate limiting (only for API models)\n",
    "        if model in ['sonar', 'sonar-pro']:\n",
    "            adaptive_delay = current_delay + random.uniform(0, current_delay)\n",
    "            time.sleep(adaptive_delay)\n",
    "        \n",
    "        answer_llm = rag_with_adaptive_retry(rec, model=model)\n",
    "        \n",
    "        doc_id = rec['id']\n",
    "        if doc_id in doc_idx:\n",
    "            original_doc = doc_idx[doc_id]\n",
    "            answer_orig = original_doc['content']\n",
    "\n",
    "            result = {\n",
    "                'answer_llm': answer_llm,\n",
    "                'answer_orig': answer_orig,\n",
    "                'document': doc_id,\n",
    "                'question': rec['question'],\n",
    "                'location': rec.get('location', ''),\n",
    "                'model': model,\n",
    "            }\n",
    "            return i, result\n",
    "        else:\n",
    "            return i, None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process record {i} with {model} after all retries: {e}\")\n",
    "        return i, None\n",
    "\n",
    "# %%\n",
    "def process_model_batch(ground_truth, model, output_file):\n",
    "    \"\"\"Process all ground truth data for a specific model\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing with model: {model}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    answers = {}\n",
    "    \n",
    "    # Get items to process\n",
    "    items_to_process = [(i, rec) for i, rec in enumerate(ground_truth)]\n",
    "    print(f\"Total records to process: {len(items_to_process)}\")\n",
    "\n",
    "    if len(items_to_process) > 0:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Adjust workers based on model type\n",
    "        if model == 'phi3':\n",
    "            MAX_WORKERS = 1  # Local model, single worker\n",
    "        else:\n",
    "            MAX_WORKERS = 2  # API models, reduced workers\n",
    "        \n",
    "        CHUNK_SIZE = 50  # Process 50 at a time\n",
    "        \n",
    "        for chunk_start in range(0, len(items_to_process), CHUNK_SIZE):\n",
    "            chunk_end = min(chunk_start + CHUNK_SIZE, len(items_to_process))\n",
    "            chunk_items = items_to_process[chunk_start:chunk_end]\n",
    "            \n",
    "            print(f\"\\nProcessing chunk {chunk_start//CHUNK_SIZE + 1}/{(len(items_to_process)-1)//CHUNK_SIZE + 1}\")\n",
    "            print(f\"Records {chunk_start}-{chunk_end - 1} of {len(ground_truth)} ({model})\")\n",
    "            \n",
    "            with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "                future_to_item = {\n",
    "                    executor.submit(process_single_record_with_model, item, model): item \n",
    "                    for item in chunk_items\n",
    "                }\n",
    "                \n",
    "                with tqdm(total=len(chunk_items), desc=f\"{model} Chunk {chunk_start//CHUNK_SIZE + 1}\") as pbar:\n",
    "                    for future in as_completed(future_to_item):\n",
    "                        try:\n",
    "                            i, result = future.result()\n",
    "                            if result is not None:\n",
    "                                answers[i] = result\n",
    "                            pbar.update(1)\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"Error in future: {e}\")\n",
    "                            pbar.update(1)\n",
    "            \n",
    "            # Break between chunks for API models\n",
    "            if model in ['sonar', 'sonar-pro'] and chunk_end < len(items_to_process):\n",
    "                cooldown_time = 3 + (rate_limit_counter * 0.05)\n",
    "                print(f\"Cooling down for {cooldown_time:.1f}s before next chunk...\")\n",
    "                time.sleep(cooldown_time)\n",
    "                \n",
    "            print(f\"Completed {len(answers)} out of {len(ground_truth)} total records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b63fb6d-f125-42b4-a634-52c3c17e582d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert to results list\n",
    "results = [None] * len(ground_truth)\n",
    "for i, val in answers.items():\n",
    "    results[i] = val.copy()\n",
    "    results[i].update(ground_truth[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c012b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.to_csv(output_file, index=False)\n",
    "print(f\"Saved results to: {output_file}\")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Total processing time for {model}: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65608159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each model\n",
    "models_to_test = [\n",
    "    ('sonar', '../data/results/results-sonar.csv'),\n",
    "    ('sonar-pro', '../data/results/results-sonar-pro.csv'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2699fc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only add phi3 if Ollama is available\n",
    "try:\n",
    "    test_response = requests.get('http://localhost:11434/api/version', timeout=5)\n",
    "    if test_response.status_code == 200:\n",
    "        models_to_test.append(('phi3', '../data/results/results-phi3.csv'))\n",
    "        print(\"Ollama detected - phi3 model will be included in comparison\")\n",
    "    else:\n",
    "        print(\"Ollama not responding - phi3 model will be skipped\")\n",
    "except:\n",
    "    print(\"Ollama not available - phi3 model will be skipped\")\n",
    "\n",
    "# Process each model\n",
    "model_results = {}\n",
    "for model, output_file in models_to_test:\n",
    "    try:\n",
    "        df_result = process_model_batch(ground_truth, model, output_file)\n",
    "        model_results[model] = df_result\n",
    "        print(f\"Successfully processed {len(df_result)} records with {model}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {model}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309ab2ab",
   "metadata": {},
   "source": [
    "#### Cosine Similarity Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88faea39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity(record):\n",
    "    \"\"\"Compute cosine similarity between original and generated answers\"\"\"\n",
    "    answer_orig = record['answer_orig']\n",
    "    answer_llm = record['answer_llm']\n",
    "    \n",
    "    if pd.isna(answer_orig) or pd.isna(answer_llm) or not answer_orig or not answer_llm:\n",
    "        return 0.0\n",
    "    \n",
    "    v_llm = model.encode(str(answer_llm))\n",
    "    v_orig = model.encode(str(answer_orig))\n",
    "    \n",
    "    return v_llm.dot(v_orig)\n",
    "\n",
    "\n",
    "# Compute similarities for all models\n",
    "for model_name, df in model_results.items():\n",
    "    print(f\"Computing similarities for {model_name}...\")\n",
    "    \n",
    "    similarities = []\n",
    "    results = df.to_dict(orient='records')\n",
    "    \n",
    "    for record in tqdm(results, desc=f\"Computing similarities for {model_name}\"):\n",
    "        sim = compute_similarity(record)\n",
    "        similarities.append(sim)\n",
    "    \n",
    "    df['cosine'] = similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2729037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save updated results with cosine similarities\n",
    "output_file = f'../data/results/results-{model_name}-cosine.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"{model_name} cosine similarity statistics:\")\n",
    "print(df['cosine'].describe())\n",
    "print(f\"Saved to: {output_file}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634007f8",
   "metadata": {},
   "source": [
    "#### Model Comparison Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e828d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison plots\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "for i, (model_name, df) in enumerate(model_results.items(), 1):\n",
    "    plt.subplot(1, len(model_results), i)\n",
    "    sns.histplot(df['cosine'], bins=30, alpha=0.7, label=model_name)\n",
    "    plt.title(f'{model_name} Cosine Similarity Distribution')\n",
    "    plt.xlabel('Cosine Similarity')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/results/model_comparison_distributions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Create combined comparison plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "for model_name, df in model_results.items():\n",
    "    sns.distplot(df['cosine'], hist=False, label=model_name)\n",
    "\n",
    "plt.title(\"Model Performance Comparison - Cosine Similarity\")\n",
    "plt.xlabel(\"A->Q->A' Cosine Similarity\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "plt.savefig('../data/results/model_comparison_combined.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print comparison summary\n",
    "print(\"\\nModel Comparison Summary:\")\n",
    "print(\"=\"*60)\n",
    "for model_name, df in model_results.items():\n",
    "    stats = df['cosine'].describe()\n",
    "    print(f\"{model_name.upper()}:\")\n",
    "    print(f\"  Mean: {stats['mean']:.4f}\")\n",
    "    print(f\"  Std:  {stats['std']:.4f}\")\n",
    "    print(f\"  Min:  {stats['min']:.4f}\")\n",
    "    print(f\"  Max:  {stats['max']:.4f}\")\n",
    "    print(f\"  Median: {stats['50%']:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35108d0",
   "metadata": {},
   "source": [
    "#### LLM-as-a-Judge Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd39fad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1_template = \"\"\"\n",
    "You are an expert evaluator for a Retrieval-Augmented Generation (RAG) system.\n",
    "Your task is to analyze the relevance of the generated answer compared to the original answer provided.\n",
    "Based on the relevance and similarity of the generated answer to the original answer, you will classify\n",
    "it as \"NON_RELEVANT\", \"PARTLY_RELEVANT\", or \"RELEVANT\".\n",
    "\n",
    "Here is the data for evaluation:\n",
    "\n",
    "Original Answer: {answer_orig}\n",
    "Generated Question: {question}\n",
    "Generated Answer: {answer_llm}\n",
    "\n",
    "Please analyze the content and context of the generated answer in relation to the original\n",
    "answer and provide your evaluation in parsable JSON without using code blocks:\n",
    "\n",
    "{{\n",
    "  \"Relevance\": \"NON_RELEVANT\" | \"PARTLY_RELEVANT\" | \"RELEVANT\",\n",
    "  \"Explanation\": \"[Provide a brief explanation for your evaluation]\"\n",
    "}}\n",
    "\"\"\".strip()\n",
    "\n",
    "prompt2_template = \"\"\"\n",
    "You are an expert evaluator for a Retrieval-Augmented Generation (RAG) system.\n",
    "Your task is to analyze the relevance of the generated answer to the given question.\n",
    "Based on the relevance of the generated answer, you will classify it\n",
    "as \"NON_RELEVANT\", \"PARTLY_RELEVANT\", or \"RELEVANT\".\n",
    "\n",
    "Here is the data for evaluation:\n",
    "\n",
    "Question: {question}\n",
    "Generated Answer: {answer_llm}\n",
    "\n",
    "Please analyze the content and context of the generated answer in relation to the question\n",
    "and provide your evaluation in parsable JSON without using code blocks:\n",
    "\n",
    "{{\n",
    "  \"Relevance\": \"NON_RELEVANT\" | \"PARTLY_RELEVANT\" | \"RELEVANT\",\n",
    "  \"Explanation\": \"[Provide a brief explanation for your evaluation]\"\n",
    "}}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e68c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate samples from each model\n",
    "def evaluate_model_sample(df, model_name, sample_size=150):\n",
    "    \"\"\"Evaluate a sample of results from a specific model\"\"\"\n",
    "    df_sample = df.sample(n=min(sample_size, len(df)), random_state=1)\n",
    "    samples = df_sample.to_dict(orient='records')\n",
    "    \n",
    "    print(f\"Evaluating {len(samples)} samples from {model_name}...\")\n",
    "    \n",
    "    evaluations = []\n",
    "    for record in tqdm(samples, desc=f\"Evaluating {model_name}\"):\n",
    "        prompt = prompt1_template.format(**record)\n",
    "        try:\n",
    "            evaluation = llm(prompt, model='sonar')  # Use sonar for evaluation\n",
    "            evaluations.append(evaluation)\n",
    "        except Exception as e:\n",
    "            print(f\"Evaluation error: {e}\")\n",
    "            evaluations.append('{\"Relevance\": \"ERROR\", \"Explanation\": \"Evaluation failed\"}')\n",
    "    \n",
    "    # Parse JSON evaluations\n",
    "    json_evaluations = []\n",
    "    for str_eval in evaluations:\n",
    "        try:\n",
    "            json_eval = json.loads(str_eval)\n",
    "            json_evaluations.append(json_eval)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSON parsing error: {e}\")\n",
    "            json_evaluations.append({\"Relevance\": \"ERROR\", \"Explanation\": \"JSON parsing failed\"})\n",
    "    \n",
    "    df_evaluations = pd.DataFrame(json_evaluations)\n",
    "    df_evaluations['model'] = model_name\n",
    "    \n",
    "    return df_evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6246f730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models\n",
    "all_evaluations = []\n",
    "\n",
    "for model_name, df in model_results.items():\n",
    "    try:\n",
    "        eval_df = evaluate_model_sample(df, model_name)\n",
    "        all_evaluations.append(eval_df)\n",
    "        \n",
    "        print(f\"\\n{model_name} evaluation results:\")\n",
    "        print(eval_df['Relevance'].value_counts())\n",
    "        \n",
    "        # Save individual evaluation results\n",
    "        eval_df.to_csv(f'../data/results/evaluations-{model_name}.csv', index=False)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to evaluate {model_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4342bfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all evaluations for comparison\n",
    "if all_evaluations:\n",
    "    df_all_evaluations = pd.concat(all_evaluations, ignore_index=True)\n",
    "    df_all_evaluations.to_csv('../data/results/evaluations-all-models.csv', index=False)\n",
    "    \n",
    "    # Create evaluation comparison\n",
    "    print(\"\\nEvaluation Summary by Model:\")\n",
    "    print(\"=\"*50)\n",
    "    evaluation_summary = df_all_evaluations.groupby(['model', 'Relevance']).size().unstack(fill_value=0)\n",
    "    print(evaluation_summary)\n",
    "    \n",
    "    # Calculate relevance percentages\n",
    "    evaluation_percentages = df_all_evaluations.groupby('model')['Relevance'].value_counts(normalize=True).unstack(fill_value=0)\n",
    "    print(\"\\nRelevance Percentages by Model:\")\n",
    "    print(\"=\"*50)\n",
    "    print(evaluation_percentages.round(3))\n",
    "    \n",
    "    # Create visualization of evaluation results\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot 1: Count of evaluations by model and relevance\n",
    "    plt.subplot(2, 2, 1)\n",
    "    evaluation_summary.plot(kind='bar', ax=plt.gca())\n",
    "    plt.title('Evaluation Counts by Model and Relevance')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title='Relevance')\n",
    "    \n",
    "    # Plot 2: Percentage of evaluations by model and relevance\n",
    "    plt.subplot(2, 2, 2)\n",
    "    evaluation_percentages.plot(kind='bar', ax=plt.gca())\n",
    "    plt.title('Evaluation Percentages by Model and Relevance')\n",
    "    plt.ylabel('Percentage')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title='Relevance')\n",
    "    \n",
    "    # Plot 3: Cosine similarity vs Relevance for all models\n",
    "    plt.subplot(2, 2, 3)\n",
    "    for model_name in df_all_evaluations['model'].unique():\n",
    "        model_data = df_all_evaluations[df_all_evaluations['model'] == model_name]\n",
    "        if len(model_data) > 0:\n",
    "            relevant_mask = model_data['Relevance'] == 'RELEVANT'\n",
    "            if relevant_mask.any():\n",
    "                plt.scatter(model_data.index[relevant_mask], [model_name] * relevant_mask.sum(), \n",
    "                           alpha=0.6, label=f'{model_name} (Relevant)', s=30)\n",
    "    plt.title('Model Performance Overview')\n",
    "    plt.ylabel('Model')\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot 4: Summary metrics\n",
    "    plt.subplot(2, 2, 4)\n",
    "    relevant_percentages = evaluation_percentages['RELEVANT'] if 'RELEVANT' in evaluation_percentages.columns else pd.Series()\n",
    "    if not relevant_percentages.empty:\n",
    "        relevant_percentages.plot(kind='bar', ax=plt.gca(), color='green', alpha=0.7)\n",
    "        plt.title('Percentage of RELEVANT Responses by Model')\n",
    "        plt.ylabel('Percentage')\n",
    "        plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../data/results/evaluation_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaefc0ab",
   "metadata": {},
   "source": [
    "#### Final Model Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d8afd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_performance_summary():\n",
    "    \"\"\"Create a comprehensive performance summary of all models\"\"\"\n",
    "    \n",
    "    summary_data = []\n",
    "    \n",
    "    for model_name, df in model_results.items():\n",
    "        # Basic stats\n",
    "        cosine_stats = df['cosine'].describe()\n",
    "        \n",
    "        # Evaluation stats (if available)\n",
    "        eval_stats = {}\n",
    "        try:\n",
    "            eval_df = pd.read_csv(f'../data/results/evaluations-{model_name}.csv')\n",
    "            relevance_counts = eval_df['Relevance'].value_counts()\n",
    "            total_evals = len(eval_df)\n",
    "            \n",
    "            eval_stats = {\n",
    "                'relevant_pct': relevance_counts.get('RELEVANT', 0) / total_evals * 100,\n",
    "                'partly_relevant_pct': relevance_counts.get('PARTLY_RELEVANT', 0) / total_evals * 100,\n",
    "                'non_relevant_pct': relevance_counts.get('NON_RELEVANT', 0) / total_evals * 100,\n",
    "                'error_pct': relevance_counts.get('ERROR', 0) / total_evals * 100,\n",
    "                'total_evaluated': total_evals\n",
    "            }\n",
    "        except FileNotFoundError:\n",
    "            eval_stats = {\n",
    "                'relevant_pct': 0,\n",
    "                'partly_relevant_pct': 0,\n",
    "                'non_relevant_pct': 0,\n",
    "                'error_pct': 0,\n",
    "                'total_evaluated': 0\n",
    "            }\n",
    "        \n",
    "        summary_data.append({\n",
    "            'model': model_name,\n",
    "            'total_responses': len(df),\n",
    "            'cosine_mean': cosine_stats['mean'],\n",
    "            'cosine_std': cosine_stats['std'],\n",
    "            'cosine_median': cosine_stats['50%'],\n",
    "            'cosine_min': cosine_stats['min'],\n",
    "            'cosine_max': cosine_stats['max'],\n",
    "            **eval_stats\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_df.to_csv('../data/results/model_performance_summary.csv', index=False)\n",
    "    \n",
    "    print(\"Model Performance Summary:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(summary_df.round(4))\n",
    "    \n",
    "    # Create a ranking based on multiple criteria\n",
    "    summary_df['combined_score'] = (\n",
    "        summary_df['cosine_mean'] * 0.4 +  # 40% weight on cosine similarity\n",
    "        (summary_df['relevant_pct'] / 100) * 0.4 +  # 40% weight on relevance\n",
    "        (1 - summary_df['cosine_std']) * 0.2  # 20% weight on consistency (lower std is better)\n",
    "    )\n",
    "    \n",
    "    summary_df_ranked = summary_df.sort_values('combined_score', ascending=False)\n",
    "    \n",
    "    print(f\"\\nModel Ranking (Combined Score):\")\n",
    "    print(\"=\" * 50)\n",
    "    for i, (_, row) in enumerate(summary_df_ranked.iterrows(), 1):\n",
    "        print(f\"{i}. {row['model']}: {row['combined_score']:.4f}\")\n",
    "        print(f\"   Cosine Mean: {row['cosine_mean']:.4f}, Relevant: {row['relevant_pct']:.1f}%\")\n",
    "    \n",
    "    return summary_df_ranked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82f9484",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_summary = create_performance_summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brahman_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
