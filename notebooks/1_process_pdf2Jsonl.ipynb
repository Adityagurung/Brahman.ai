{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f664789",
   "metadata": {},
   "source": [
    "--------\n",
    "Processes all PDF files in the specified folder:\n",
    "1. Ingest PDF files from raw and Converts PDFs to Markdown using Marker.\n",
    "2. Splits text into smaller chunks using LangChain splitters.\n",
    "3. Assigns stable document IDs via MD5 hashing.\n",
    "4. Saves processed chunks to a JSONL file in processed directory\n",
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35887d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "import jsonlines\n",
    "from tqdm import tqdm\n",
    "from langchain_text_splitters import (\n",
    "    MarkdownHeaderTextSplitter,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "from marker.convert import convert_single_pdf\n",
    "from marker.logger import configure_logging\n",
    "from marker.models import load_all_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e43a428",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09a6962e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 1Ô∏è‚É£ Utility: Clean up PDF filenames (remove spaces/dots)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def parse_file_name(filename):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        name_part = filename[:-4]\n",
    "        name_part = name_part.replace(\" \", \"\").replace(\".\", \"\")\n",
    "        cleaned_filename = name_part + \".pdf\"\n",
    "    else:\n",
    "        cleaned_filename = filename.replace(\" \", \"\").replace(\".\", \"\")\n",
    "    return cleaned_filename\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34039db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 2Ô∏è‚É£ Convert PDF ‚Üí Markdown using Marker\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def convert_pdf_to_markdown(fname, reference_folder, model_lst):\n",
    "    \"\"\"\n",
    "    Converts a PDF to markdown text using Marker models.\n",
    "    \"\"\"\n",
    "    md_filename = fname.rsplit(\".\", 1)[0] + \".md\"\n",
    "    pdf_filename = os.path.join(reference_folder, fname)\n",
    "\n",
    "    print(f\"üìÑ Converting PDF: {pdf_filename}\")\n",
    "    full_text, _, _ = convert_single_pdf(pdf_filename, model_lst, batch_multiplier=1)\n",
    "\n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20941509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 3Ô∏è‚É£ Process a single PDF into smaller text chunks with metadata\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def download_and_process_pdf_file(\n",
    "    f_key, text_splitter, markdown_splitter, model_lst, reference_folder=\"./raw/\"\n",
    "):\n",
    "    temp_file_name = parse_file_name(f_key)\n",
    "    temp_file_path = os.path.join(reference_folder, temp_file_name)\n",
    "\n",
    "    # Convert PDF to Markdown\n",
    "    mdfile = convert_pdf_to_markdown(temp_file_name, reference_folder, model_lst)\n",
    "\n",
    "    # Split markdown by headers (H1, H2, H3)\n",
    "    md_header_split = markdown_splitter.split_text(mdfile)\n",
    "\n",
    "    documents = []\n",
    "    for split in md_header_split:\n",
    "        # Further split into smaller overlapping chunks\n",
    "        split_texts = text_splitter.split_text(split.page_content)\n",
    "\n",
    "        for i, split_text in enumerate(split_texts):\n",
    "            # Create unique document ID based on filename + part number\n",
    "            document_id = f\"{f_key}_part_{i}\"\n",
    "            hash_object = hashlib.md5(document_id.encode())\n",
    "            hash_hex = hash_object.hexdigest()\n",
    "            document_id = hash_hex[:10]\n",
    "\n",
    "            # Metadata for tracking source & chunk position\n",
    "            metadata_dict = {\n",
    "                \"document_id\": document_id,\n",
    "                \"pdf_name\": f_key,\n",
    "                \"pdf_part\": i,\n",
    "            }\n",
    "            metadata_dict.update(split.metadata)\n",
    "\n",
    "            # Store final structured document\n",
    "            documents.append({\"metadata\": metadata_dict, \"content\": split_text})\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6af96145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Found PDF files: ['Andhra_Pradesh.pdf', 'Karnataka.pdf']\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 4Ô∏è‚É£ List all PDF files in a given folder\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def list_pdf_files(directory_path):\n",
    "    filenames = [f for f in os.listdir(directory_path) if f.lower().endswith(\".pdf\")]\n",
    "    return filenames\n",
    "\n",
    "# Use the correct folder where PDFs actually exist\n",
    "pdf_folder = os.path.join(\"..\", \"data\", \"raw\") if os.getcwd().endswith(\"notebooks\") else os.path.join(\".\", \"data\", \"raw\")\n",
    "\n",
    "print(\"üìÇ Found PDF files:\", list_pdf_files(pdf_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1e91db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 PDF(s) in ..\\data\\raw\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 5Ô∏è‚É£ Main processing logic\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # No folder creation here ‚Äî we assume PDFs already exist\n",
    "    if not os.path.exists(pdf_folder):\n",
    "        raise FileNotFoundError(f\"PDF folder not found: {pdf_folder}\")\n",
    "\n",
    "    # List PDF files\n",
    "    filenames = list_pdf_files(pdf_folder)\n",
    "\n",
    "    if not filenames:\n",
    "        print(f\"No PDF files found in {pdf_folder}\")\n",
    "    else:\n",
    "        print(f\"Found {len(filenames)} PDF(s) in {pdf_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe003930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded detection model vikp/surya_det3 on device cpu with dtype torch.float32\n",
      "Loaded detection model vikp/surya_layout3 on device cpu with dtype torch.float32\n",
      "Loaded reading order model vikp/surya_order on device cpu with dtype torch.float32\n",
      "Loaded recognition model vikp/surya_rec2 on device cpu with dtype torch.float32\n",
      "Loaded texify model to cpu with torch.float32 dtype\n"
     ]
    }
   ],
   "source": [
    "# Load Marker models for PDF ‚Üí Markdown conversion\n",
    "configure_logging()\n",
    "model_lst = load_all_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "089321c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup splitters for text segmentation\n",
    "headers_to_split_on = [(\"#\", \"Header 1\"), (\"##\", \"Header 2\"), (\"###\", \"Header 3\")]\n",
    "chunk_size = 500\n",
    "chunk_overlap = 100\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    ")\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b37964a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìë Processing: Andhra_Pradesh.pdf\n",
      "üìÑ Converting PDF: ..\\data\\raw\\Andhra_Pradesh.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:50<00:00, 25.25s/it]\n",
      "Detecting bboxes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:28<00:00, 14.02s/it]\n",
      "Finding reading order: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [01:32<00:00, 46.05s/it]\n",
      "Processing PDFs:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [03:34<03:34, 214.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìë Processing: Karnataka.pdf\n",
      "üìÑ Converting PDF: ..\\data\\raw\\Karnataka.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [01:25<00:00, 21.37s/it]\n",
      "Detecting bboxes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [01:05<00:00, 21.87s/it]\n",
      "Finding reading order: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [02:57<00:00, 59.17s/it]\n",
      "Processing PDFs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [10:00<00:00, 300.48s/it]\n"
     ]
    }
   ],
   "source": [
    "# Store ALL chunks in a single flat list\n",
    "documents = []\n",
    "\n",
    "# Process each PDF ‚Üí chunks\n",
    "for filename in tqdm(filenames, desc=\"Processing PDFs\"):\n",
    "    print(f\"\\nüìë Processing: {filename}\")\n",
    "    splitted_doc = download_and_process_pdf_file(\n",
    "        filename,\n",
    "        text_splitter,\n",
    "        markdown_splitter,\n",
    "        model_lst,\n",
    "        reference_folder=pdf_folder,\n",
    "    )\n",
    "    documents.extend(splitted_doc)  # Keep list flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8af609a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Processing complete. Saved 149 chunks to ..\\data\\processed\\docs_processed.jsonl\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 6Ô∏è‚É£ Save processed chunks to JSONL\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "output_dir = os.path.join(os.path.dirname(pdf_folder), \"processed\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "output_path = os.path.join(output_dir, \"docs_processed.jsonl\")\n",
    "with jsonlines.open(output_path, mode=\"w\") as writer:\n",
    "    writer.write_all(documents)\n",
    "\n",
    "print(f\"\\n‚úÖ Processing complete. Saved {len(documents)} chunks to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brahman_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
