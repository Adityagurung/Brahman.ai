{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Evaluation - End-to-End\n",
    "This notebook evaluates the complete RAG system performance using both Perplexity and Phi3 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import random\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "Initialize models and API configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your Perplexity API key (set this in your environment)\n",
    "PERPLEXITY_API_KEY = os.getenv('PERPLEXITY_API_KEY')\n",
    "\n",
    "# Check if API key is set\n",
    "if not PERPLEXITY_API_KEY:\n",
    "    print(\"Warning: PERPLEXITY_API_KEY not set. Please set it as environment variable.\")\n",
    "    print(\"You can continue with local Phi3 evaluation only.\")\n",
    "\n",
    "# Initialize sentence transformer model for embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Initialize variables\n",
    "documents = None\n",
    "doc_dict = {}\n",
    "best_retrieval_method = None\n",
    "\n",
    "print(\"Setup completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results directory if it doesn't exist\n",
    "os.makedirs('results', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Determine Best Retrieval Method\n",
    "Load documents and find the best performing retrieval method from previous evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents\n",
    "with open('data/processed/documents-with-ids.json', 'r') as f:\n",
    "    documents = json.load(f)\n",
    "\n",
    "# Create document lookup dictionary\n",
    "doc_dict = {doc['id']: doc for doc in documents}\n",
    "\n",
    "# Load ground truth\n",
    "df_ground_truth = pd.read_csv('data/ground-truth-retrieval.csv')\n",
    "ground_truth = df_ground_truth.to_dict(orient='records')\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents and {len(ground_truth)} ground truth questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the best retrieval method from previous evaluations\n",
    "methods = ['keyword_search', 'vector_search', 'hybrid_search']\n",
    "best_mrr = 0\n",
    "best_method = 'vector_search'  # Default fallback\n",
    "\n",
    "for method in methods:\n",
    "    try:\n",
    "        with open(f'results/{method}_results.json', 'r') as f:\n",
    "            results = json.load(f)\n",
    "        \n",
    "        if method == 'hybrid_search':\n",
    "            # Get best alpha result\n",
    "            best_alpha = results['best_alpha']\n",
    "            mrr = results['alpha_results'][best_alpha]['metrics']['mrr']\n",
    "        else:\n",
    "            mrr = results['metrics']['mrr']\n",
    "        \n",
    "        print(f\"{method}: MRR = {mrr:.4f}\")\n",
    "        \n",
    "        if mrr > best_mrr:\n",
    "            best_mrr = mrr\n",
    "            best_method = method\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Results file for {method} not found\")\n",
    "\n",
    "best_retrieval_method = best_method\n",
    "print(f\"\\nResults saved to: results/rag_evaluation_results.csv\")
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}nBest retrieval method: {best_method} (MRR: {best_mrr:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Best Retrieval Method\n",
    "Configure the best performing retrieval method for RAG evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the best performing retrieval method\n",
    "if best_retrieval_method == 'vector_search':\n",
    "    print(\"Setting up vector search...\")\n",
    "    \n",
    "    # Generate embeddings\n",
    "    texts = [doc['content'] for doc in documents]\n",
    "    document_vectors = model.encode(texts, show_progress_bar=True)\n",
    "    \n",
    "    def search_function(query, num_results=5):\n",
    "        \"\"\"Vector search function\"\"\"\n",
    "        query_vector = model.encode([query])\n",
    "        similarities = cosine_similarity(query_vector, document_vectors)[0]\n",
    "        top_indices = np.argsort(similarities)[::-1][:num_results]\n",
    "        return [documents[idx] for idx in top_indices]\n",
    "        \n",
    "elif best_retrieval_method == 'keyword_search':\n",
    "    print(\"Setting up keyword search...\")\n",
    "    import minsearch\n",
    "    \n",
    "    index = minsearch.Index(\n",
    "        text_fields=[\"content\"],\n",
    "        keyword_fields=[\"location\", \"doc_id\", \"id\"]\n",
    "    )\n",
    "    index.fit(documents)\n",
    "    \n",
    "    def search_function(query, num_results=5):\n",
    "        \"\"\"Keyword search function\"\"\"\n",
    "        return index.search(query=query, num_results=num_results)\n",
    "        \n",
    "else:  # hybrid_search\n",
    "    print(\"Setting up hybrid search (simplified version)...\")\n",
    "    # For simplicity, using vector search as fallback\n",
    "    texts = [doc['content'] for doc in documents]\n",
    "    document_vectors = model.encode(texts, show_progress_bar=True)\n",
    "    \n",
    "    def search_function(query, num_results=5):\n",
    "        \"\"\"Hybrid search function (simplified)\"\"\"\n",
    "        query_vector = model.encode([query])\n",
    "        similarities = cosine_similarity(query_vector, document_vectors)[0]\n",
    "        top_indices = np.argsort(similarities)[::-1][:num_results]\n",
    "        return [documents[idx] for idx in top_indices]\n",
    "\n",
    "print(f\"Retrieval method '{best_retrieval_method}' is ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define RAG Components\n",
    "Build prompt template and API calling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query, search_results):\n",
    "    \"\"\"Build RAG prompt with query and retrieved context\"\"\"\n",
    "    prompt_template = \"\"\"\n",
    "You are a helpful assistant. Answer the QUESTION based on the CONTEXT provided.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT: \n",
    "{context}\n",
    "\n",
    "Please provide a clear and concise answer based on the context provided.\n",
    "\"\"\".strip()\n",
    "\n",
    "    # Build context from search results\n",
    "    context = \"\"\n",
    "    for i, doc in enumerate(search_results, 1):\n",
    "        context += f\"Document {i}: {doc['content']}\\n\\n\"\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt\n",
    "\n",
    "print(\"RAG prompt builder defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_perplexity(prompt):\n",
    "    \"\"\"Call Perplexity API for answer generation\"\"\"\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            'https://api.perplexity.ai/chat/completions',\n",
    "            headers={\n",
    "                'Authorization': f'Bearer {PERPLEXITY_API_KEY}',\n",
    "                'Content-Type': 'application/json'\n",
    "            },\n",
    "            json={\n",
    "                'model': 'sonar',\n",
    "                'messages': [{\"role\": \"user\", \"content\": prompt}]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return response.json()['choices'][0]['message']['content']\n",
    "        else:\n",
    "            print(f\"Perplexity API error: {response.status_code}\")\n",
    "            return \"Error: Could not generate response\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Perplexity API error: {e}\")\n",
    "        return \"Error: Could not generate response\"\n",
    "\n",
    "def call_ollama(prompt):\n",
    "    \"\"\"Call local Ollama API (Phi3) for answer generation\"\"\"\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            'http://localhost:11434/api/generate',\n",
    "            json={\n",
    "                'model': 'phi3',\n",
    "                'prompt': prompt,\n",
    "                'stream': False\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return response.json()['response']\n",
    "        else:\n",
    "            print(f\"Ollama API error: {response.status_code}\")\n",
    "            return \"Error: Could not generate response\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Ollama API error: {e}\")\n",
    "        return \"Error: Could not generate response\"\n",
    "\n",
    "print(\"API calling functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Evaluation Functions\n",
    "Functions for computing similarity and LLM-as-judge evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine_similarity(answer1, answer2):\n",
    "    \"\"\"Compute cosine similarity between two answers using embeddings\"\"\"\n",
    "    try:\n",
    "        v1 = model.encode(answer1)\n",
    "        v2 = model.encode(answer2)\n",
    "        return float(np.dot(v1, v2))\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def evaluate_with_llm_judge(question, original_answer, generated_answer, llm_function):\n",
    "    \"\"\"Use LLM as judge for evaluation\"\"\"\n",
    "    judge_prompt = f\"\"\"\n",
    "You are an expert evaluator. Compare the GENERATED ANSWER with the ORIGINAL ANSWER for the given QUESTION.\n",
    "Rate the relevance as: RELEVANT, PARTLY_RELEVANT, or NON_RELEVANT.\n",
    "\n",
    "QUESTION: {question}\n",
    "ORIGINAL ANSWER: {original_answer}\n",
    "GENERATED ANSWER: {generated_answer}\n",
    "\n",
    "Respond with only one word: RELEVANT, PARTLY_RELEVANT, or NON_RELEVANT\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        judgment = llm_function(judge_prompt).strip().upper()\n",
    "        if judgment in ['RELEVANT', 'PARTLY_RELEVANT', 'NON_RELEVANT']:\n",
    "            return judgment\n",
    "        else:\n",
    "            return 'PARTLY_RELEVANT'  # Default fallback\n",
    "    except:\n",
    "        return 'PARTLY_RELEVANT'  # Default fallback\n",
    "\n",
    "print(\"Evaluation functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Complete RAG Evaluation\n",
    "Sample questions and evaluate both Perplexity and Phi3 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample questions for evaluation (set sample_size as needed)\n",
    "sample_size = 150\n",
    "random.seed(42)  # For reproducibility\n",
    "sampled_questions = random.sample(ground_truth, min(sample_size, len(ground_truth)))\n",
    "\n",
    "print(f\"Evaluating RAG with {len(sampled_questions)} sampled questions...\")\n",
    "print(f\"Using retrieval method: {best_retrieval_method}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize results list\n",
    "results = []\n",
    "\n",
    "# Process each sampled question\n",
    "for i, record in enumerate(tqdm(sampled_questions, desc=\"Processing questions\")):\n",
    "    question = record['question']\n",
    "    doc_id = record['id']\n",
    "    \n",
    "    # Get original answer from ground truth document\n",
    "    original_doc = doc_dict[doc_id]\n",
    "    original_answer = original_doc['content']\n",
    "    \n",
    "    # Retrieve context using best retrieval method\n",
    "    search_results = search_function(question)\n",
    "    \n",
    "    # Build RAG prompt\n",
    "    prompt = build_prompt(question, search_results)\n",
    "    \n",
    "    # Generate answers with both LLMs\n",
    "    answer_perplexity = call_perplexity(prompt) if PERPLEXITY_API_KEY else \"API key not available\"\n",
    "    answer_phi3 = call_ollama(prompt)\n",
    "    \n",
    "    # Calculate cosine similarities\n",
    "    cosine_perplexity = compute_cosine_similarity(original_answer, answer_perplexity)\n",
    "    cosine_phi3 = compute_cosine_similarity(original_answer, answer_phi3)\n",
    "    \n",
    "    # LLM-as-judge evaluations (using Perplexity as judge)\n",
    "    judge_perplexity = evaluate_with_llm_judge(\n",
    "        question, original_answer, answer_perplexity, call_perplexity\n",
    "    ) if PERPLEXITY_API_KEY else \"API key not available\"\n",
    "    \n",
    "    judge_phi3 = evaluate_with_llm_judge(\n",
    "        question, original_answer, answer_phi3, call_perplexity\n",
    "    ) if PERPLEXITY_API_KEY else \"API key not available\"\n",
    "    \n",
    "    # Store result\n",
    "    result = {\n",
    "        'question': question,\n",
    "        'doc_id': doc_id,\n",
    "        'original_answer': original_answer,\n",
    "        'answer_perplexity': answer_perplexity,\n",
    "        'answer_phi3': answer_phi3,\n",
    "        'cosine_perplexity': cosine_perplexity,\n",
    "        'cosine_phi3': cosine_phi3,\n",
    "        'judge_perplexity': judge_perplexity,\n",
    "        'judge_phi3': judge_phi3,\n",
    "        'retrieval_method': best_retrieval_method\n",
    "    }\n",
    "    \n",
    "    results.append(result)\n",
    "    \n",
    "    # Save intermediate results every 50 questions\n",
    "    if (i + 1) % 50 == 0:\n",
    "        df_temp = pd.DataFrame(results)\n",
    "        df_temp.to_csv('results/rag_evaluation_temp.csv', index=False)\n",
    "        print(f\"Saved intermediate results after {i + 1} questions\")\n",
    "\n",
    "print(\"RAG evaluation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Final Results and Display Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final results\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.to_csv('results/rag_evaluation_results.csv', index=False)\n",
    "\n",
    "# Calculate and display summary statistics\n",
    "print(\"\\n=== RAG Evaluation Summary ===\")\n",
    "print(f\"Total questions evaluated: {len(results)}\")\n",
    "print(f\"Retrieval method used: {results[0]['retrieval_method']}\")\n",
    "\n",
    "print(\"\\n--- Cosine Similarity Results ---\")\n",
    "if PERPLEXITY_API_KEY:\n",
    "    print(f\"Perplexity - Mean: {df_results['cosine_perplexity'].mean():.4f}, Std: {df_results['cosine_perplexity'].std():.4f}\")\n",
    "print(f\"Phi3 - Mean: {df_results['cosine_phi3'].mean():.4f}, Std: {df_results['cosine_phi3'].std():.4f}\")\n",
    "\n",
    "print(\"\\n--- LLM-as-Judge Results ---\")\n",
    "if PERPLEXITY_API_KEY:\n",
    "    print(\"Perplexity:\")\n",
    "    print(df_results['judge_perplexity'].value_counts())\n",
    "    print(\"\\nPhi3:\")\n",
    "    print(df_results['judge_phi3'].value_counts())\n",
    "else:\n",
    "    print(\"LLM-as-Judge evaluation requires Perplexity API key\")\n",
    "\n",
    "print(f\"\\