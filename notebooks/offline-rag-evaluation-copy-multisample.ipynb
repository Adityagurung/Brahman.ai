{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "724cef1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9e46bcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "41f74b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.auto import tqdm\n",
    "from openai import OpenAI\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e777fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at jinaai/jina-embeddings-v2-small-en and are newly initialized: ['embeddings.position_embeddings.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SentenceTransformer model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    model = SentenceTransformer(\"jinaai/jina-embeddings-v2-small-en\")\n",
    "    print(\"✅ SentenceTransformer model loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to load SentenceTransformer: {e}\")\n",
    "    model = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "abecb06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 149 documents and 735 ground truth questions\n"
     ]
    }
   ],
   "source": [
    "# Load documents from processed JSON file\n",
    "with open('../data/processed/documents-with-ids.json', 'r') as f:\n",
    "    documents = json.load(f)\n",
    "\n",
    "# Load ground truth dataset for evaluation from CSV file\n",
    "df_ground_truth = pd.read_csv('../data/processed/ground-truth-retrieval.csv')\n",
    "ground_truth = df_ground_truth.to_dict(orient='records')\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents and {len(ground_truth)} ground truth questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "979776da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'location': 'Andhra_Pradesh',\n",
       " 'doc_id': 'd4402d82c0',\n",
       " 'content': \"Andhra Pradesh is known for its rich history, architecture and culture. Andhra Pradesh has a variety of tourist attractions including beaches, hills, wildlife, forests and temples. Like rest of the Southern India, the culture of Andhra Pradesh is essentially Dravidian, quite different from North India's Sanskrit Hindu culture. Andhra Pradesh was part of the British Madras presidency and then independent India's Madras State until 1953, when Andhra State was formed, with the capital being\",\n",
       " 'id': '450a9d36'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "938a7f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What are the must-see religious temples to visit in Andhra Pradesh?',\n",
       " 'id': 'db0beb52'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d92f8c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created merged dataset with 745 question-document pairs\n",
      "Sample merged record keys: ['question', 'id', 'location', 'doc_id', 'content']\n"
     ]
    }
   ],
   "source": [
    "# Create a merged dataframe by joining ground-truth with documents on 'id' column\n",
    "df_documents = pd.DataFrame(documents)\n",
    "df_merged = df_ground_truth.merge(df_documents, on='id', how='inner')\n",
    "merged_data = df_merged.to_dict(orient='records')\n",
    "with open('../data/processed/ques-docs-pairs.json', 'w') as f:\n",
    "    json.dump(merged_data, f, indent=2)\n",
    "    \n",
    "print(f\"Created merged dataset with {len(merged_data)} question-document pairs\")\n",
    "print(f\"Sample merged record keys: {list(merged_data[0].keys()) if merged_data else 'No data'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f7da208f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create document index for quick lookup\n",
    "doc_idx = {d['id']: d for d in documents}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "314ed3f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Northern Coast (Alluri Sitharama Raju, Anakapalli, East Godavari, Kakinada, Konaseema, Parvathipuram Manyam, Srikakulam, Visakhapatnam, Vizianagaram, Yanam) Central Coast (Eluru, Krishna, NTR, West Godavari) Southern Coast (Bapatla, Guntur, Nellore, Palnadu, Prakasam, Tirupati) Rayalaseema (Annamayya, Anantapur, Chittoor, Kadapa, Kurnool, Nandyal, Sri Sathya Sai)  \\n![0_image_1.png](0_image_1.png) interactive map'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_idx['db0beb52']['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e562ce",
   "metadata": {},
   "source": [
    " #### Index data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8cc82898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize sentence transformer model\n",
    "model_name = 'multi-qa-MiniLM-L6-cos-v1'\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4a2ba7e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c6253bbfa984e4ab95583a8b52b9d16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/149 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate vectors for all documents\n",
    "vectors = []\n",
    "for doc in tqdm(documents):\n",
    "    # Use content field instead of question + text\n",
    "    content = doc['content']\n",
    "    vector = model.encode(content)\n",
    "    vectors.append(vector)\n",
    "\n",
    "vectors = np.array(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e7f50e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.vector.VectorSearch at 0x1c62691a650>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create vector search index\n",
    "from minsearch import VectorSearch\n",
    "\n",
    "vindex = VectorSearch(keyword_fields=['location'])\n",
    "vindex.fit(vectors, documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cb6f1c",
   "metadata": {},
   "source": [
    "### Retrieval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4e2a3ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minsearch_vector_search(vector, location):\n",
    "    return vindex.search(\n",
    "        vector,\n",
    "        filter_dict={'location': location},\n",
    "        num_results=5\n",
    "    )\n",
    "\n",
    "def question_text_vector(q):\n",
    "    question = q['question']\n",
    "    location = q.get('location', None)  # Get location if available, otherwise None\n",
    "    v_q = model.encode(question)\n",
    "    \n",
    "    if location:\n",
    "        return minsearch_vector_search(v_q, location)\n",
    "    else:\n",
    "        # If no location specified, search without filter\n",
    "        return vindex.search(v_q, num_results=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "671e5606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 relevant documents\n"
     ]
    }
   ],
   "source": [
    "# Test vector search functionality\n",
    "test_question = {\n",
    "    'question': 'What are the must-see religious sites in Andhra Pradesh?',\n",
    "}\n",
    "search_results = question_text_vector(test_question)\n",
    "print(f\"Found {len(search_results)} relevant documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b44dfc4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'location': 'Andhra_Pradesh',\n",
       "  'doc_id': 'd4402d82c0',\n",
       "  'content': 'Andhra Pradesh (AP) is a state in Southern India, with Bay of Bengal on the east and shares boundaries with Telangana on the north, Chhattisgarh and Odisha on the north-east, Tamil Nadu on the south and Karnataka on the west. Vijayawada is the capital of this state.',\n",
       "  'id': 'a411c9aa'},\n",
       " {'location': 'Andhra_Pradesh',\n",
       "  'doc_id': 'd4402d82c0',\n",
       "  'content': \"Andhra Pradesh is known for its rich history, architecture and culture. Andhra Pradesh has a variety of tourist attractions including beaches, hills, wildlife, forests and temples. Like rest of the Southern India, the culture of Andhra Pradesh is essentially Dravidian, quite different from North India's Sanskrit Hindu culture. Andhra Pradesh was part of the British Madras presidency and then independent India's Madras State until 1953, when Andhra State was formed, with the capital being\",\n",
       "  'id': '450a9d36'},\n",
       " {'location': 'Andhra_Pradesh',\n",
       "  'doc_id': '9eb5245053',\n",
       "  'content': 'the most attractive city in Andhra Pradesh, with exotic resorts near beaches 7 8 9',\n",
       "  'id': '3ed17fb0'},\n",
       " {'location': 'Andhra_Pradesh',\n",
       "  'doc_id': 'ed240be0ed',\n",
       "  'content': 'Amaravati - the capital of Andhra Pradesh which covers the areas that have Buddhist sites Anantapur - a city well connected by road and rail with the city and nearby attractions Guntur - the third largest city in Andhra Pradesh and home to many ancient and archaeological sites and temples Kurnool - the site of Konda Reddy Fort Nellore - a combination of religion, nature, and history Rajahmundry - known as the cultural capital of Andhra Pradesh, noted for its versatile Vedic culture and',\n",
       "  'id': '4de0caea'},\n",
       " {'location': 'Andhra_Pradesh',\n",
       "  'doc_id': 'd4402d82c0',\n",
       "  'content': 'Odisha Karnataka Tamil Nadu Telangana Retrieved from \"https://en.wikivoyage.org/w/index.php?title=Andhra_Pradesh&oldid=5081274\"',\n",
       "  'id': 'b184cf08'}]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b17141",
   "metadata": {},
   "source": [
    "#### The RAG flow with Travel Assistant Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "176dbdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query, search_results):\n",
    "    prompt_template = \"\"\"\n",
    "You're a travel assistant bot that helps users plan their itinerary and discover amazing places to visit.\n",
    "Answer the QUESTION based on the CONTEXT from the travel database.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "When answering, consider:\n",
    "- Must-visit tourist attractions and landmarks\n",
    "- Cultural experiences and local traditions  \n",
    "- Historical significance of places\n",
    "- Best times to visit and travel tips\n",
    "- Local cuisine and specialties (if mentioned in context)\n",
    "- Transportation and accessibility information (if available)\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT: \n",
    "{context}\n",
    "\"\"\".strip()\n",
    "\n",
    "    context = \"\"\n",
    "    for doc in search_results:\n",
    "        location = doc.get('location', 'Unknown')\n",
    "        content = doc.get('content', doc.get('text', ''))  # Handle both content and text fields\n",
    "        context = context + f\"location: {location}\\ncontent: {content}\\n\\n\"\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b8b6c7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "854e24eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(prompt, model='gpt-4o'):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fa4f7ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query: dict, model='gpt-4o') -> str:\n",
    "    search_results = question_text_vector(query)\n",
    "    prompt = build_prompt(query['question'], search_results)\n",
    "    answer = llm(prompt, model=model)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33872cf6",
   "metadata": {},
   "source": [
    "#### Single Example Evaluation Across All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fa0bb0ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What are the must-see religious temples to visit in Andhra Pradesh?',\n",
       " 'id': 'db0beb52'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "261ddc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7a056c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 25 random questions for evaluation:\n",
      "1. Question: Do foreign travelers need a visa and passport to visit Karnataka?...\n",
      "   Document ID: 7adcc584\n",
      "2. Question: What is the historical importance of Penukonda in the context of the Vijayanagara empire?...\n",
      "   Document ID: 032385d9\n",
      "3. Question: What are the main cultural and spiritual centers to visit in Andhra Pradesh?...\n",
      "   Document ID: dceae12c\n",
      "4. Question: Which species of animals and birds might I encounter in Bandipur National Park?...\n",
      "   Document ID: 2ffbfa3d\n",
      "5. Question: What are some must-visit cultural and natural attractions in Karnataka?...\n",
      "   Document ID: 56d1a83b\n"
     ]
    }
   ],
   "source": [
    "# Select 25 random examples for evaluation across all models\n",
    "sample_size = 25\n",
    "sample_questions = random.sample(ground_truth, min(sample_size, len(ground_truth)))\n",
    "\n",
    "print(f\"Selected {len(sample_questions)} random questions for evaluation:\")\n",
    "for i, q in enumerate(sample_questions[:5]):  # Show first 5 as preview\n",
    "    print(f\"{i+1}. Question: {q['question'][:100]}...\")\n",
    "    print(f\"   Document ID: {q['id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b82bb306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 25 question dictionaries for RAG evaluation\n"
     ]
    }
   ],
   "source": [
    "# Prepare all question dictionaries for RAG\n",
    "sample_question_dicts = []\n",
    "for sample_question in sample_questions:\n",
    "    doc_id = sample_question['id']\n",
    "    original_doc = doc_idx[doc_id]\n",
    "    sample_location = original_doc.get('location', 'Unknown')\n",
    "    \n",
    "    question_dict = {\n",
    "        'question': sample_question['question'],\n",
    "        'location': sample_location,\n",
    "        'id': doc_id\n",
    "    }\n",
    "    sample_question_dicts.append(question_dict)\n",
    "\n",
    "print(f\"Prepared {len(sample_question_dicts)} question dictionaries for RAG evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3baef524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing model: gpt-3.5-turbo\n",
      "  Processing question 1/25...\n",
      "  Processing question 2/25...\n",
      "  Processing question 3/25...\n",
      "  Processing question 4/25...\n",
      "  Processing question 5/25...\n",
      "  Processing question 6/25...\n",
      "  Processing question 7/25...\n",
      "  Processing question 8/25...\n",
      "  Processing question 9/25...\n",
      "  Processing question 10/25...\n",
      "  Processing question 11/25...\n",
      "  Processing question 12/25...\n",
      "  Processing question 13/25...\n",
      "  Processing question 14/25...\n",
      "  Processing question 15/25...\n",
      "  Processing question 16/25...\n",
      "  Processing question 17/25...\n",
      "  Processing question 18/25...\n",
      "  Processing question 19/25...\n",
      "  Processing question 20/25...\n",
      "  Processing question 21/25...\n",
      "  Processing question 22/25...\n",
      "  Processing question 23/25...\n",
      "  Processing question 24/25...\n",
      "  Processing question 25/25...\n",
      "\n",
      "Testing model: gpt-4o\n",
      "  Processing question 1/25...\n",
      "  Processing question 2/25...\n",
      "  Processing question 3/25...\n",
      "  Processing question 4/25...\n",
      "  Processing question 5/25...\n",
      "  Processing question 6/25...\n",
      "  Processing question 7/25...\n",
      "  Processing question 8/25...\n",
      "  Processing question 9/25...\n",
      "  Processing question 10/25...\n",
      "  Processing question 11/25...\n",
      "  Processing question 12/25...\n",
      "  Processing question 13/25...\n",
      "  Processing question 14/25...\n",
      "  Processing question 15/25...\n",
      "  Processing question 16/25...\n",
      "  Processing question 17/25...\n",
      "  Processing question 18/25...\n",
      "  Processing question 19/25...\n",
      "  Processing question 20/25...\n",
      "  Processing question 21/25...\n",
      "  Processing question 22/25...\n",
      "  Processing question 23/25...\n",
      "  Processing question 24/25...\n",
      "  Processing question 25/25...\n",
      "\n",
      "Testing model: gpt-4o-mini\n",
      "  Processing question 1/25...\n",
      "  Processing question 2/25...\n",
      "  Processing question 3/25...\n",
      "  Processing question 4/25...\n",
      "  Processing question 5/25...\n",
      "  Processing question 6/25...\n",
      "  Processing question 7/25...\n",
      "  Processing question 8/25...\n",
      "  Processing question 9/25...\n",
      "  Processing question 10/25...\n",
      "  Processing question 11/25...\n",
      "  Processing question 12/25...\n",
      "  Processing question 13/25...\n",
      "  Processing question 14/25...\n",
      "  Processing question 15/25...\n",
      "  Processing question 16/25...\n",
      "  Processing question 17/25...\n",
      "  Processing question 18/25...\n",
      "  Processing question 19/25...\n",
      "  Processing question 20/25...\n",
      "  Processing question 21/25...\n",
      "  Processing question 22/25...\n",
      "  Processing question 23/25...\n",
      "  Processing question 24/25...\n",
      "  Processing question 25/25...\n",
      "\n",
      "Completed evaluation. Generated 75 results.\n"
     ]
    }
   ],
   "source": [
    "# Test all GPT models on the 25 examples\n",
    "models_to_test = ['gpt-3.5-turbo', 'gpt-4o', 'gpt-4o-mini']\n",
    "multiple_examples_results = []\n",
    "\n",
    "for model_name in models_to_test:\n",
    "    print(f\"\\nTesting model: {model_name}\")\n",
    "    \n",
    "    for i, (sample_question, question_dict) in enumerate(zip(sample_questions, sample_question_dicts)):\n",
    "        try:\n",
    "            print(f\"  Processing question {i+1}/{len(sample_questions)}...\")\n",
    "            \n",
    "            # Get original answer from documents\n",
    "            doc_id = sample_question['id']\n",
    "            original_doc = doc_idx[doc_id]\n",
    "            answer_orig = original_doc['content']\n",
    "            \n",
    "            # Generate answer using RAG\n",
    "            answer_llm = rag(question_dict, model=model_name)\n",
    "            \n",
    "            result = {\n",
    "                'model': model_name,\n",
    "                'question_id': i+1,\n",
    "                'question': question_dict['question'],\n",
    "                'location': question_dict['location'],\n",
    "                'document_id': doc_id,\n",
    "                'answer_orig': answer_orig,\n",
    "                'answer_llm': answer_llm\n",
    "            }\n",
    "            multiple_examples_results.append(result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    Error with model {model_name}, question {i+1}: {e}\")\n",
    "\n",
    "print(f\"\\nCompleted evaluation. Generated {len(multiple_examples_results)} results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "35531657",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_multiple_examples = pd.DataFrame(multiple_examples_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6e04864e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results summary:\n",
      "Total results: 75\n",
      "Results per model:\n",
      "  gpt-3.5-turbo: 25 results\n",
      "  gpt-4o: 25 results\n",
      "  gpt-4o-mini: 25 results\n"
     ]
    }
   ],
   "source": [
    "# Display summary statistics\n",
    "print(f\"\\nResults summary:\")\n",
    "print(f\"Total results: {len(multiple_examples_results)}\")\n",
    "print(f\"Results per model:\")\n",
    "for model in models_to_test:\n",
    "    count = len(df_multiple_examples[df_multiple_examples['model'] == model])\n",
    "    print(f\"  {model}: {count} results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "af8b79d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved multiple examples results to: results/multiple_examples_comparison.csv\n"
     ]
    }
   ],
   "source": [
    "# Save multiple examples results\n",
    "df_multiple_examples.to_csv('../results/multiple_examples_comparison.csv', index=False)\n",
    "print(\"Saved multiple examples results to: results/multiple_examples_comparison.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d810719e",
   "metadata": {},
   "source": [
    "#### Cosine Similarity Metric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ba1bb080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity(record):\n",
    "    \"\"\"Calculate cosine similarity between original and LLM answers\"\"\"\n",
    "    answer_orig = record['answer_orig']\n",
    "    answer_llm = record['answer_llm']\n",
    "    \n",
    "    v_llm = model.encode(answer_llm)\n",
    "    v_orig = model.encode(answer_orig)\n",
    "    \n",
    "    return v_llm.dot(v_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88879519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity(record):\n",
    "    try:\n",
    "        answer_orig = record['answer_orig']\n",
    "        answer_llm = record['answer_llm']\n",
    "        \n",
    "        # Convert to strings and handle None values\n",
    "        answer_llm = str(answer_llm) if answer_llm is not None else \"\"\n",
    "        answer_orig = str(answer_orig) if answer_orig is not None else \"\"\n",
    "        \n",
    "        # Skip if either is empty\n",
    "        if not answer_llm.strip() or not answer_orig.strip():\n",
    "            return 0.0\n",
    "        \n",
    "        v_llm = model.encode(answer_llm)\n",
    "        v_orig = model.encode(answer_orig)\n",
    "        \n",
    "        return v_llm.dot(v_orig)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing record: {record}\")\n",
    "        print(f\"Error: {e}\")\n",
    "        return 0.0  # or return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "804f1b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating cosine similarities for all examples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dd760ff7bc241c18bc01710058612f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "LookupError",
     "evalue": "unknown encoding: Foreign travelers visiting Karnataka will need a visa and passport to enter the state, as with entering any state in India. It is important to have the necessary travel documents when planning a trip to Karnataka to avoid any issues upon arrival.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalculating cosine similarities for all examples...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, result \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(multiple_examples_results)):\n\u001b[1;32m----> 6\u001b[0m     similarity \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     cosine_similarities_all\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion_id\u001b[39m\u001b[38;5;124m'\u001b[39m: result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion_id\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcosine_similarity\u001b[39m\u001b[38;5;124m'\u001b[39m: similarity,\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m: result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     12\u001b[0m     })\n",
      "Cell \u001b[1;32mIn[72], line 6\u001b[0m, in \u001b[0;36mcompute_similarity\u001b[1;34m(record)\u001b[0m\n\u001b[0;32m      3\u001b[0m answer_orig \u001b[38;5;241m=\u001b[39m record[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer_orig\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      4\u001b[0m answer_llm \u001b[38;5;241m=\u001b[39m record[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer_llm\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 6\u001b[0m v_llm \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43manswer_llm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m v_orig \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(answer_orig)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m v_llm\u001b[38;5;241m.\u001b[39mdot(v_orig)\n",
      "\u001b[1;31mLookupError\u001b[0m: unknown encoding: Foreign travelers visiting Karnataka will need a visa and passport to enter the state, as with entering any state in India. It is important to have the necessary travel documents when planning a trip to Karnataka to avoid any issues upon arrival."
     ]
    }
   ],
   "source": [
    "# Calculate cosine similarities for all models and all examples\n",
    "cosine_similarities_all = []\n",
    "\n",
    "print(\"Calculating cosine similarities for all examples...\")\n",
    "for i, result in enumerate(tqdm(multiple_examples_results)):\n",
    "    similarity = compute_similarity(result)\n",
    "    cosine_similarities_all.append({\n",
    "        'model': result['model'],\n",
    "        'question_id': result['question_id'],\n",
    "        'cosine_similarity': similarity,\n",
    "        'question': result['question']\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5484f65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "df_similarities_all = pd.DataFrame(cosine_similarities_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cb3617d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cosine Similarity Statistics by Model:\n",
      "               count        mean        std         min         max  \\\n",
      "model                                                                 \n",
      "gpt-3.5-turbo     25  339.673889  40.448502  231.639099  416.498413   \n",
      "gpt-4o            25  337.053406  41.856098  242.224396  397.559601   \n",
      "gpt-4o-mini       25  333.478210  40.722000  232.546295  409.327606   \n",
      "\n",
      "                   median  \n",
      "model                      \n",
      "gpt-3.5-turbo  344.909698  \n",
      "gpt-4o         337.513702  \n",
      "gpt-4o-mini    339.849396  \n"
     ]
    }
   ],
   "source": [
    "# Calculate summary statistics\n",
    "similarity_stats = df_similarities_all.groupby('model')['cosine_similarity'].agg([\n",
    "    'count', 'mean', 'std', 'min', 'max', 'median'\n",
    "]).round(4)\n",
    "print(\"\\nCosine Similarity Statistics by Model:\")\n",
    "print(similarity_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa0e9e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9362e2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "\n",
    "# Define the three models to compare\n",
    "models_to_compare = ['gpt-3.5-turbo', 'gpt-4o', 'gpt-4o-mini']\n",
    "colors = ['green', 'blue', 'orange']\n",
    "model_labels = ['GPT-3.5 Turbo', 'GPT-4o', 'GPT-4o Mini']\n",
    "\n",
    "# Sample data for each model\n",
    "sampled_data = {}\n",
    "\n",
    "for model in models_to_compare:\n",
    "    model_data = df_similarities_all[df_similarities_all['model'] == model]['cosine_similarity']\n",
    "    if len(model_data) >= sample_size:\n",
    "        sampled_data[model] = model_data.sample(n=sample_size, random_state=42).values\n",
    "    else:\n",
    "        sampled_data[model] = model_data.values\n",
    "        print(f\"Warning: Only {len(model_data)} samples available for {model}\")\n",
    "\n",
    "plt.figure(figsize=figsize)\n",
    "\n",
    "# Create histograms for each model\n",
    "for i, (model, color, label) in enumerate(zip(models_to_compare, colors, model_labels)):\n",
    "    if model in sampled_data:\n",
    "        plt.hist(sampled_data[model], bins=20, density=True, alpha=0.4, \n",
    "                label=label, color=color, edgecolor='black', linewidth=0.5)\n",
    "\n",
    "# Create smooth KDE curves\n",
    "x_vals = np.linspace(0, 1, 300)\n",
    "\n",
    "for i, (model, color) in enumerate(zip(models_to_compare, colors)):\n",
    "    if model in sampled_data and len(sampled_data[model]) > 1:\n",
    "        kde = gaussian_kde(sampled_data[model])\n",
    "        plt.plot(x_vals, kde(x_vals), color=color, lw=2.5)\n",
    "\n",
    "# Add interpretation lines\n",
    "plt.axvline(x=0.5, color='red', linestyle='--', alpha=0.7, \n",
    "            label='Moderate Similarity', linewidth=1)\n",
    "plt.axvline(x=0.7, color='darkred', linestyle='--', alpha=0.7, \n",
    "            label='High Similarity', linewidth=1)\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Travel Assistant RAG - Model Performance Comparison\\n(Cosine Similarity Between Original and Generated Answers)', \n",
    "          fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xlabel('Cosine Similarity Score', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Density', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add sample information\n",
    "plt.figtext(0.5, 0.02, f'Sample Size: {sample_size} questions per model (Total: {sample_size * len(models_to_compare)} samples)', \n",
    "            ha='center', fontsize=10, style='italic')\n",
    "\n",
    "interpretation_text = \n",
    "plt.figtext(0.02, 0.65, interpretation_text, fontsize=9, \n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\", alpha=0.8))\n",
    "\n",
    "plt.legend(loc='upper right', fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(0, 1)\n",
    "\n",
    "# Calculate and display summary statistics\n",
    "print(\"\\n=== SUMMARY STATISTICS ===\")\n",
    "for i, model in enumerate(models_to_compare):\n",
    "    if model in sampled_data:\n",
    "        data = sampled_data[model]\n",
    "        print(f\"\\n{model_labels[i]}:\")\n",
    "        print(f\"  Mean: {np.mean(data):.3f}\")\n",
    "        print(f\"  Std:  {np.std(data):.3f}\")\n",
    "        print(f\"  Median: {np.median(data):.3f}\")\n",
    "        print(f\"  Samples: {len(data)}\")\n",
    "        \n",
    "        # Performance classification\n",
    "        excellent = np.sum(data > 0.7)\n",
    "        good = np.sum((data >= 0.5) & (data <= 0.7))\n",
    "        needs_improvement = np.sum(data < 0.5)\n",
    "        \n",
    "        print(f\"  Excellent (>0.7): {excellent}/{len(data)} ({excellent/len(data)*100:.1f}%)\")\n",
    "        print(f\"  Good (0.5-0.7): {good}/{len(data)} ({good/len(data)*100:.1f}%)\")\n",
    "        print(f\"  Needs Improvement (<0.5): {needs_improvement}/{len(data)} ({needs_improvement/len(data)*100:.1f}%)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "return sampled_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af67dd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbbfcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_data = create_rag_comparison_histogram(df_similarities_all, sample_size=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d25c960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save similarities to CSV\n",
    "# df_similarities_all.to_csv('../results/cosine_similarities.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7f33be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DETAILED COSINE SIMILARITY ANALYSIS ===\n",
      "\n",
      "Summary Statistics:\n",
      "               count  mean  std  min  max  median\n",
      "model                                            \n",
      "gpt-3.5-turbo     15   0.0  0.0  0.0  0.0     0.0\n",
      "gpt-4o            15   0.0  0.0  0.0  0.0     0.0\n",
      "gpt-4o-mini       15   0.0  0.0  0.0  0.0     0.0\n",
      "\n",
      "Performance Classification:\n",
      "\n",
      "gpt-3.5-turbo:\n",
      "  Excellent (>0.7): 0/15 (0.0%)\n",
      "  Good (0.5-0.7): 0/15 (0.0%)\n",
      "  Needs Improvement (<0.5): 15/15 (100.0%)\n",
      "\n",
      "gpt-4o:\n",
      "  Excellent (>0.7): 0/15 (0.0%)\n",
      "  Good (0.5-0.7): 0/15 (0.0%)\n",
      "  Needs Improvement (<0.5): 15/15 (100.0%)\n",
      "\n",
      "gpt-4o-mini:\n",
      "  Excellent (>0.7): 0/15 (0.0%)\n",
      "  Good (0.5-0.7): 0/15 (0.0%)\n",
      "  Needs Improvement (<0.5): 15/15 (100.0%)\n"
     ]
    }
   ],
   "source": [
    "#Create detailed statistical comparison\n",
    "print(\"=== DETAILED COSINE SIMILARITY ANALYSIS ===\")\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(similarity_stats)\n",
    "\n",
    "print(\"\\nPerformance Classification:\")\n",
    "for model_name in models_to_test:\n",
    "    model_similarities = df_similarities_all[df_similarities_all['model'] == model_name]['cosine_similarity']\n",
    "    \n",
    "    excellent = len(model_similarities[model_similarities > 0.7])\n",
    "    good = len(model_similarities[(model_similarities >= 0.5) & (model_similarities <= 0.7)])\n",
    "    needs_improvement = len(model_similarities[model_similarities < 0.5])\n",
    "    total = len(model_similarities)\n",
    "    \n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Excellent (>0.7): {excellent}/{total} ({excellent/total*100:.1f}%)\")\n",
    "    print(f\"  Good (0.5-0.7): {good}/{total} ({good/total*100:.1f}%)\")\n",
    "    print(f\"  Needs Improvement (<0.5): {needs_improvement}/{total} ({needs_improvement/total*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb0a9cf",
   "metadata": {},
   "source": [
    "#### LLM-as-a-Judge Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308cd76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM-as-a-Judge evaluation prompts\n",
    "prompt1_template = \"\"\"\n",
    "You are an expert evaluator for a Retrieval-Augmented Generation (RAG) system.\n",
    "Your task is to analyze the relevance of the generated answer compared to the original answer provided.\n",
    "Based on the relevance and similarity of the generated answer to the original answer, you will classify\n",
    "it as \"NON_RELEVANT\", \"PARTLY_RELEVANT\", or \"RELEVANT\".\n",
    "\n",
    "Here is the data for evaluation:\n",
    "\n",
    "Original Answer: {answer_orig}\n",
    "Generated Question: {question}\n",
    "Generated Answer: {answer_llm}\n",
    "\n",
    "Please analyze the content and context of the generated answer in relation to the original\n",
    "answer and provide your evaluation in parsable JSON without using code blocks:\n",
    "\n",
    "{{\n",
    "  \"Relevance\": \"NON_RELEVANT\" | \"PARTLY_RELEVANT\" | \"RELEVANT\",\n",
    "  \"Explanation\": \"[Provide a brief explanation for your evaluation]\"\n",
    "}}\n",
    "\"\"\".strip()\n",
    "\n",
    "prompt2_template = \"\"\"\n",
    "You are an expert evaluator for a Retrieval-Augmented Generation (RAG) system.\n",
    "Your task is to analyze the relevance of the generated answer to the given question.\n",
    "Based on the relevance of the generated answer, you will classify it\n",
    "as \"NON_RELEVANT\", \"PARTLY_RELEVANT\", or \"RELEVANT\".\n",
    "\n",
    "Here is the data for evaluation:\n",
    "\n",
    "Question: {question}\n",
    "Generated Answer: {answer_llm}\n",
    "\n",
    "Please analyze the content and context of the generated answer in relation to the question\n",
    "and provide your evaluation in parsable JSON without using code blocks:\n",
    "\n",
    "{{\n",
    "  \"Relevance\": \"NON_RELEVANT\" | \"PARTLY_RELEVANT\" | \"RELEVANT\",\n",
    "  \"Explanation\": \"[Provide a brief explanation for your evaluation]\"\n",
    "}}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b0bfd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing LLM-as-a-Judge evaluation on sample of results...\n",
      "\n",
      "Evaluating 5 examples for gpt-3.5-turbo...\n",
      "\n",
      "Evaluating 5 examples for gpt-4o...\n",
      "\n",
      "Evaluating 5 examples for gpt-4o-mini...\n"
     ]
    }
   ],
   "source": [
    "# Evaluate subset of examples using LLM-as-a-Judge (due to cost considerations)\n",
    "# Select 5 examples from each model (15 total evaluations)\n",
    "evaluation_sample_size = 5\n",
    "evaluations_aqa = []\n",
    "evaluations_qa = []\n",
    "\n",
    "print(\"Performing LLM-as-a-Judge evaluation on sample of results...\")\n",
    "\n",
    "for model_name in models_to_test:\n",
    "    model_results = [r for r in multiple_examples_results if r['model'] == model_name]\n",
    "    sample_results = random.sample(model_results, min(evaluation_sample_size, len(model_results)))\n",
    "    \n",
    "    print(f\"\\nEvaluating {len(sample_results)} examples for {model_name}...\")\n",
    "    \n",
    "    # A>Q>A evaluation\n",
    "    for result in sample_results:\n",
    "        try:\n",
    "            prompt = prompt1_template.format(**result)\n",
    "            evaluation_str = llm(prompt, model='gpt-4o-mini')\n",
    "            evaluation = json.loads(evaluation_str)\n",
    "            evaluation['model'] = result['model']\n",
    "            evaluation['question_id'] = result['question_id']\n",
    "            evaluation['evaluation_type'] = 'Answer-Question-Answer'\n",
    "            evaluations_aqa.append(evaluation)\n",
    "        except (json.JSONDecodeError, Exception) as e:\n",
    "            print(f\"  Failed AQA evaluation for {model_name}, question {result['question_id']}: {e}\")\n",
    "    \n",
    "    # Q>A evaluation\n",
    "    for result in sample_results:\n",
    "        try:\n",
    "            prompt = prompt2_template.format(**result)\n",
    "            evaluation_str = llm(prompt, model='gpt-4o-mini')\n",
    "            evaluation = json.loads(evaluation_str)\n",
    "            evaluation['model'] = result['model']\n",
    "            evaluation['question_id'] = result['question_id']\n",
    "            evaluation['evaluation_type'] = 'Question-Answer'\n",
    "            evaluations_qa.append(evaluation)\n",
    "        except (json.JSONDecodeError, Exception) as e:\n",
    "            print(f\"  Failed QA evaluation for {model_name}, question {result['question_id']}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587906e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all evaluations\n",
    "all_evaluations_multiple = evaluations_aqa + evaluations_qa\n",
    "df_evaluations_multiple = pd.DataFrame(all_evaluations_multiple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e0a95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed LLM-as-a-Judge evaluation:\n",
      "Total evaluations: 30\n",
      "AQA evaluations: 15\n",
      "QA evaluations: 15\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nCompleted LLM-as-a-Judge evaluation:\")\n",
    "print(f\"Total evaluations: {len(all_evaluations_multiple)}\")\n",
    "print(f\"AQA evaluations: {len(evaluations_aqa)}\")\n",
    "print(f\"QA evaluations: {len(evaluations_qa)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b43d585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LLM-as-a-Judge Evaluation Summary:\n",
      "Relevance                             NON_RELEVANT  PARTLY_RELEVANT  RELEVANT\n",
      "model         evaluation_type                                                \n",
      "gpt-3.5-turbo Answer-Question-Answer             0                2         3\n",
      "              Question-Answer                    0                1         4\n",
      "gpt-4o        Answer-Question-Answer             2                1         2\n",
      "              Question-Answer                    0                1         4\n",
      "gpt-4o-mini   Answer-Question-Answer             1                3         1\n",
      "              Question-Answer                    0                1         4\n"
     ]
    }
   ],
   "source": [
    "# Display evaluation summary\n",
    "print(\"\\nLLM-as-a-Judge Evaluation Summary:\")\n",
    "eval_summary = df_evaluations_multiple.groupby(['model', 'evaluation_type', 'Relevance']).size().unstack(fill_value=0)\n",
    "print(eval_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f417831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save LLM-as-a-Judge evaluations\n",
    "# df_evaluations_multiple.to_csv('../results/llm_as_a_judge_evaluations.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66377b1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0.98, 'LLM-as-a-Judge Evaluation Results - Multiple Examples')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create visualization for LLM-as-a-Judge results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "fig.suptitle('LLM-as-a-Judge Evaluation Results - Multiple Examples', fontsize=16, fontweight='bold')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55293b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot AQA evaluations\n",
    "aqa_data = df_evaluations_multiple[df_evaluations_multiple['evaluation_type'] == 'Answer-Question-Answer']\n",
    "if len(aqa_data) > 0:\n",
    "    aqa_counts = aqa_data.groupby(['model', 'Relevance']).size().unstack(fill_value=0)\n",
    "    aqa_counts.plot(kind='bar', ax=axes[0], title='Answer-Question-Answer Evaluation', \n",
    "                    color=['#FF6B6B', '#FFD93D', '#6BCF7F'])\n",
    "    axes[0].set_xlabel('Model')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].legend(title='Relevance')\n",
    "    axes[0].tick_params(axis='x', rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6bb546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot QA evaluations\n",
    "qa_data = df_evaluations_multiple[df_evaluations_multiple['evaluation_type'] == 'Question-Answer']\n",
    "if len(qa_data) > 0:\n",
    "    qa_counts = qa_data.groupby(['model', 'Relevance']).size().unstack(fill_value=0)\n",
    "    qa_counts.plot(kind='bar', ax=axes[1], title='Question-Answer Evaluation',\n",
    "                   color=['#FF6B6B', '#FFD93D', '#6BCF7F'])\n",
    "    axes[1].set_xlabel('Model')\n",
    "    axes[1].set_ylabel('Count')\n",
    "    axes[1].legend(title='Relevance')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4a1faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.savefig('../images/llm_as_a_judge_evaluation_multiple.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d1ef86",
   "metadata": {},
   "source": [
    "#### Comprehensive summary Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPREHENSIVE EVALUATION SUMMARY (15 Examples) ===\n",
      "\n",
      "1. DATASET INFORMATION:\n",
      "   • Total examples evaluated: 15\n",
      "   • Models tested: gpt-3.5-turbo, gpt-4o, gpt-4o-mini\n",
      "   • Total RAG generations: 45\n",
      "\n",
      "2. COSINE SIMILARITY ANALYSIS:\n",
      "   Mean ± Std Dev by Model:\n",
      "   • gpt-3.5-turbo: 0.0000 ± 0.0000\n",
      "   • gpt-4o: 0.0000 ± 0.0000\n",
      "   • gpt-4o-mini: 0.0000 ± 0.0000\n",
      "\n",
      "   Performance Distribution:\n",
      "   • gpt-3.5-turbo:\n",
      "     - Excellent (>0.7): 0/15 (0.0%)\n",
      "     - Good (0.5-0.7): 0/15 (0.0%)\n",
      "     - Needs Improvement (<0.5): 15/15 (100.0%)\n",
      "   • gpt-4o:\n",
      "     - Excellent (>0.7): 0/15 (0.0%)\n",
      "     - Good (0.5-0.7): 0/15 (0.0%)\n",
      "     - Needs Improvement (<0.5): 15/15 (100.0%)\n",
      "   • gpt-4o-mini:\n",
      "     - Excellent (>0.7): 0/15 (0.0%)\n",
      "     - Good (0.5-0.7): 0/15 (0.0%)\n",
      "     - Needs Improvement (<0.5): 15/15 (100.0%)\n",
      "\n",
      "3. LLM-as-a-JUDGE EVALUATION:\n",
      "   Samples evaluated per model: 5\n",
      "\n",
      "   Answer-Question-Answer Relevance:\n",
      "   • gpt-3.5-turbo: {'RELEVANT': np.int64(3), 'PARTLY_RELEVANT': np.int64(2)}\n",
      "   • gpt-4o: {'NON_RELEVANT': np.int64(2), 'RELEVANT': np.int64(2), 'PARTLY_RELEVANT': np.int64(1)}\n",
      "   • gpt-4o-mini: {'PARTLY_RELEVANT': np.int64(3), 'RELEVANT': np.int64(1), 'NON_RELEVANT': np.int64(1)}\n",
      "\n",
      "   Question-Answer Relevance:\n",
      "   • gpt-3.5-turbo: {'RELEVANT': np.int64(4), 'PARTLY_RELEVANT': np.int64(1)}\n",
      "   • gpt-4o: {'RELEVANT': np.int64(4), 'PARTLY_RELEVANT': np.int64(1)}\n",
      "   • gpt-4o-mini: {'RELEVANT': np.int64(4), 'PARTLY_RELEVANT': np.int64(1)}\n",
      "\n",
      "4. FILES GENERATED:\n",
      "   • results/multiple_examples_comparison.csv\n",
      "   • results/cosine_similarities_all_examples.csv\n",
      "   • results/llm_as_a_judge_evaluations_multiple.csv\n",
      "   • images/cosine_similarity_histograms.png\n",
      "   • images/cosine_similarity_violin_plot.png\n",
      "   • images/llm_as_a_judge_evaluation_multiple.png\n",
      "   • data/processed/ques-docs-pairs.json\n",
      "\n",
      "5. KEY INSIGHTS:\n",
      "   • Best performing model (avg. similarity): gpt-3.5-turbo\n",
      "   • Most consistent model (lowest std dev): gpt-3.5-turbo\n",
      "   • Largest similarity range: 0.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"=== COMPREHENSIVE EVALUATION SUMMARY (15 Examples) ===\")\n",
    "\n",
    "print(f\"\\n1. DATASET INFORMATION:\")\n",
    "print(f\"   • Total examples evaluated: {len(sample_questions)}\")\n",
    "print(f\"   • Models tested: {', '.join(models_to_test)}\")\n",
    "print(f\"   • Total RAG generations: {len(multiple_examples_results)}\")\n",
    "\n",
    "print(f\"\\n2. COSINE SIMILARITY ANALYSIS:\")\n",
    "print(\"   Mean ± Std Dev by Model:\")\n",
    "for model in models_to_test:\n",
    "    model_data = df_similarities_all[df_similarities_all['model'] == model]['cosine_similarity']\n",
    "    print(f\"   • {model}: {model_data.mean():.4f} ± {model_data.std():.4f}\")\n",
    "\n",
    "print(f\"\\n   Performance Distribution:\")\n",
    "for model_name in models_to_test:\n",
    "    model_similarities = df_similarities_all[df_similarities_all['model'] == model_name]['cosine_similarity']\n",
    "    excellent = len(model_similarities[model_similarities > 0.7])\n",
    "    good = len(model_similarities[(model_similarities >= 0.5) & (model_similarities <= 0.7)])\n",
    "    needs_improvement = len(model_similarities[model_similarities < 0.5])\n",
    "    total = len(model_similarities)\n",
    "    \n",
    "    print(f\"   • {model_name}:\")\n",
    "    print(f\"     - Excellent (>0.7): {excellent}/{total} ({excellent/total*100:.1f}%)\")\n",
    "    print(f\"     - Good (0.5-0.7): {good}/{total} ({good/total*100:.1f}%)\")\n",
    "    print(f\"     - Needs Improvement (<0.5): {needs_improvement}/{total} ({needs_improvement/total*100:.1f}%)\")\n",
    "\n",
    "if len(all_evaluations_multiple) > 0:\n",
    "    print(f\"\\n3. LLM-as-a-JUDGE EVALUATION:\")\n",
    "    print(f\"   Samples evaluated per model: {evaluation_sample_size}\")\n",
    "    \n",
    "    print(f\"\\n   Answer-Question-Answer Relevance:\")\n",
    "    for model in models_to_test:\n",
    "        model_aqa = aqa_data[aqa_data['model'] == model]\n",
    "        if len(model_aqa) > 0:\n",
    "            relevance_dist = model_aqa['Relevance'].value_counts()\n",
    "            print(f\"   • {model}: {dict(relevance_dist)}\")\n",
    "    \n",
    "    print(f\"\\n   Question-Answer Relevance:\")\n",
    "    for model in models_to_test:\n",
    "        model_qa = qa_data[qa_data['model'] == model]\n",
    "        if len(model_qa) > 0:\n",
    "            relevance_dist = model_qa['Relevance'].value_counts()\n",
    "            print(f\"   • {model}: {dict(relevance_dist)}\")\n",
    "\n",
    "print(f\"\\n4. FILES GENERATED:\")\n",
    "print(\"   • results/multiple_examples_comparison.csv\")\n",
    "print(\"   • results/cosine_similarities_all_examples.csv\")\n",
    "print(\"   • results/llm_as_a_judge_evaluations_multiple.csv\") \n",
    "print(\"   • images/cosine_similarity_histograms.png\")\n",
    "print(\"   • images/cosine_similarity_violin_plot.png\")\n",
    "print(\"   • images/llm_as_a_judge_evaluation_multiple.png\")\n",
    "print(\"   • data/processed/ques-docs-pairs.json\")\n",
    "\n",
    "print(f\"\\n5. KEY INSIGHTS:\")\n",
    "best_model_idx = similarity_stats['mean'].idxmax()\n",
    "worst_model_idx = similarity_stats['mean'].idxmin()\n",
    "print(f\"   • Best performing model (avg. similarity): {best_model_idx}\")\n",
    "print(f\"   • Most consistent model (lowest std dev): {similarity_stats['std'].idxmin()}\")\n",
    "print(f\"   • Largest similarity range: {similarity_stats['max'].max() - similarity_stats['min'].min():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brahman_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
