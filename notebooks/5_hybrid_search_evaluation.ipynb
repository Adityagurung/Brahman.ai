{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Search Evaluation\n",
    "This notebook evaluates the performance of hybrid search combining keyword and vector search methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import minsearch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results directory if it doesn't exist\n",
    "os.makedirs('results', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Hybrid Search Components\n",
    "Setup both vector and keyword search components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize sentence transformer model for vector search\n",
    "model_name = 'all-MiniLM-L6-v2'\n",
    "model = SentenceTransformer(model_name)\n",
    "print(f\"Loaded sentence transformer model: {model_name}\")\n",
    "\n",
    "# Initialize variables for search components\n",
    "documents = None\n",
    "document_vectors = None\n",
    "keyword_index = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "Loading documents and ground truth data for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading data...\")\n",
    "\n",
    "# Load documents from processed JSON file\n",
    "with open('data/processed/documents-with-ids.json', 'r') as f:\n",
    "    documents = json.load(f)\n",
    "\n",
    "# Load ground truth dataset for evaluation\n",
    "df_ground_truth = pd.read_csv('data/ground-truth-retrieval.csv')\n",
    "ground_truth = df_ground_truth.to_dict(orient='records')\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents and {len(ground_truth)} ground truth questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Keyword Search Index\n",
    "Configure and build the keyword search component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Setting up keyword search index...\")\n",
    "\n",
    "# Initialize MinSearch index for keyword search\n",
    "keyword_index = minsearch.Index(\n",
    "    text_fields=[\"content\"],\n",
    "    keyword_fields=[\"location\", \"doc_id\", \"id\"]\n",
    ")\n",
    "\n",
    "# Fit the index with document data\n",
    "keyword_index.fit(documents)\n",
    "\n",
    "print(\"Keyword search index ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Vector Search\n",
    "Generate embeddings for all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating document embeddings...\")\n",
    "\n",
    "# Extract text content from documents\n",
    "texts = [doc['content'] for doc in documents]\n",
    "\n",
    "# Generate embeddings for all documents\n",
    "document_vectors = model.encode(texts, show_progress_bar=True)\n",
    "\n",
    "print(f\"Generated embeddings for {len(document_vectors)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Hybrid Search Function\n",
    "Create the hybrid search logic that combines both methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search(query, num_results=5, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Hybrid search combining keyword and vector search\n",
    "    alpha: weight for vector search (1-alpha for keyword search)\n",
    "    \"\"\"\n",
    "    # Get keyword search results (more results for better fusion)\n",
    "    keyword_results = keyword_index.search(\n",
    "        query=query,\n",
    "        num_results=num_results * 2\n",
    "    )\n",
    "    \n",
    "    # Get vector search results\n",
    "    query_vector = model.encode([query])\n",
    "    similarities = cosine_similarity(query_vector, document_vectors)[0]\n",
    "    \n",
    "    # Create document scores dictionary\n",
    "    doc_scores = {}\n",
    "    \n",
    "    # Add keyword scores (normalize by position)\n",
    "    for i, doc in enumerate(keyword_results):\n",
    "        doc_id = doc['id']\n",
    "        keyword_score = 1.0 / (i + 1)  # Reciprocal rank\n",
    "        doc_scores[doc_id] = {\n",
    "            'doc': doc,\n",
    "            'keyword_score': keyword_score,\n",
    "            'vector_score': 0.0\n",
    "        }\n",
    "    \n",
    "    # Add vector scores for all documents\n",
    "    for i, doc in enumerate(documents):\n",
    "        doc_id = doc['id']\n",
    "        vector_score = float(similarities[i])\n",
    "        \n",
    "        if doc_id in doc_scores:\n",
    "            doc_scores[doc_id]['vector_score'] = vector_score\n",
    "        else:\n",
    "            # Only add if vector score is reasonable (threshold to avoid noise)\n",
    "            if vector_score > 0.1:\n",
    "                doc_scores[doc_id] = {\n",
    "                    'doc': doc,\n",
    "                    'keyword_score': 0.0,\n",
    "                    'vector_score': vector_score\n",
    "                }\n",
    "    \n",
    "    # Calculate hybrid scores\n",
    "    for doc_id in doc_scores:\n",
    "        keyword_score = doc_scores[doc_id]['keyword_score']\n",
    "        vector_score = doc_scores[doc_id]['vector_score']\n",
    "        \n",
    "        # Combine scores using alpha weighting\n",
    "        hybrid_score = (1 - alpha) * keyword_score + alpha * vector_score\n",
    "        doc_scores[doc_id]['hybrid_score'] = hybrid_score\n",
    "    \n",
    "    # Sort by hybrid score and return top results\n",
    "    sorted_docs = sorted(\n",
    "        doc_scores.items(),\n",
    "        key=lambda x: x[1]['hybrid_score'],\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    # Prepare final results with scores\n",
    "    results = []\n",
    "    for doc_id, scores in sorted_docs[:num_results]:\n",
    "        doc = scores['doc'].copy()\n",
    "        doc['hybrid_score'] = scores['hybrid_score']\n",
    "        doc['keyword_score'] = scores['keyword_score']\n",
    "        doc['vector_score'] = scores['vector_score']\n",
    "        results.append(doc)\n",
    "        \n",
    "    return results\n",
    "\n",
    "print(\"Hybrid search function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Different Alpha Values\n",
    "Evaluate hybrid search with different weight combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different alpha values for hybrid search weighting\n",
    "alpha_values = [0.3, 0.5, 0.7]  # Different weights for vector vs keyword\n",
    "results = {}\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    print(f\"\\nEvaluating hybrid search with alpha={alpha}...\")\n",
    "    \n",
    "    # Initialize list to store relevance results for this alpha\n",
    "    relevance_total = []\n",
    "    \n",
    "    # Evaluate each ground truth question\n",
    "    for q in tqdm(ground_truth, desc=f\"Evaluating alpha={alpha}\"):\n",
    "        doc_id = q['id']  # Ground truth document ID\n",
    "        \n",
    "        # Perform hybrid search with current alpha\n",
    "        search_results = hybrid_search(q['question'], alpha=alpha)\n",
    "        \n",
    "        # Check if correct document is in results\n",
    "        relevance = [d['id'] == doc_id for d in search_results]\n",
    "        relevance_total.append(relevance)\n",
    "    \n",
    "    # Calculate metrics for this alpha\n",
    "    hit_count = sum(1 for line in relevance_total if True in line)\n",
    "    hit_rate = hit_count / len(relevance_total)\n",
    "    \n",
    "    # Calculate Mean Reciprocal Rank (MRR)\n",
    "    total_score = 0.0\n",
    "    for line in relevance_total:\n",
    "        for rank in range(len(line)):\n",
    "            if line[rank] == True:\n",
    "                total_score += 1 / (rank + 1)\n",
    "                break\n",
    "    \n",
    "    mrr = total_score / len(relevance_total)\n",
    "    \n",
    "    # Store results for this alpha\n",
    "    metrics = {\n",
    "        'hit_rate': hit_rate,\n",
    "        'mrr': mrr,\n",
    "        'total_questions': len(relevance_total)\n",
    "    }\n",
    "    \n",
    "    results[f'alpha_{alpha}'] = {\n",
    "        'metrics': metrics,\n",
    "        'relevance_results': relevance_total\n",
    "    }\n",
    "    \n",
    "    print(f\"Alpha {alpha} - Hit Rate: {metrics['hit_rate']:.4f}, MRR: {metrics['mrr']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Best Alpha and Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best alpha based on MRR score\n",
    "best_alpha = max(results.keys(), key=lambda k: results[k]['metrics']['mrr'])\n",
    "\n",
    "# Prepare final results for saving\n",
    "final_results = {\n",
    "    'method': 'hybrid_search',\n",
    "    'model_name': model_name,\n",
    "    'best_alpha': best_alpha,\n",
    "    'alpha_results': results\n",
    "}\n",
    "\n",
    "# Save results to JSON file\n",
    "with open('results/hybrid_search_results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "\n",
    "# Display final results\n",
    "print(f\"\\nBest Hybrid Search Results (Alpha: {best_alpha}):\")\n",
    "best_metrics = results[best_alpha]['metrics']\n",
    "print(f\"Hit Rate: {best_metrics['hit_rate']:.4f}\")\n",
    "print(f\"MRR: {best_metrics['mrr']:.4f}\")\n",
    "print(f\"Total Questions: {best_metrics['total_questions']}\")\n",
    "print(\"\\nResults saved to: results/hybrid_search_results.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}