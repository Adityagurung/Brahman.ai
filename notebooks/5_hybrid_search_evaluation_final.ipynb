{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40d70962",
   "metadata": {},
   "source": [
    "## Hybrid Search Evaluation with Qdrant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d57343f",
   "metadata": {},
   "source": [
    "This notebook demonstrates hybrid search combining dense embeddings and sparse BM25 vectors using Qdrant. We'll evaluate different search approaches and compare their performance using RRF (Reciprocal Rank Fusion).\n",
    "\n",
    "Overview\n",
    "Dense vectors: Capture semantic meaning, good for natural language queries\n",
    "Sparse vectors (BM25): Excel at exact keyword matches, fast and lightweight\n",
    "Hybrid Search: Combines both approaches for better overall performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "997ba70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!python -m pip install -q \"qdrant-client[fastembed]>=1.14.2\" pandas numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5893de0d",
   "metadata": {},
   "source": [
    "### Start Qdrant server in Docker (run this once)\n",
    "!docker run -d -p 6333:6333 -p 6334:6334 \\\n",
    "   -v \"./qdrant_storage:/qdrant/storage:z\" \\\n",
    "   qdrant/qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "360ad04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import os\n",
    "import pickle\n",
    "from typing import List, Dict, Any\n",
    "import numpy as np\n",
    "from qdrant_client import QdrantClient, models\n",
    "import time\n",
    "import hashlib\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117c36b8",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "Loading documents and ground truth data for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33940e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 149 documents and 735 ground truth questions\n",
      "Document keys: ['location', 'doc_id', 'content', 'id']\n",
      "Ground truth columns: ['question', 'id']\n"
     ]
    }
   ],
   "source": [
    "# Load documents and ground truth data\n",
    "with open('../data/processed/documents-with-ids.json', 'r') as f:\n",
    "    documents = json.load(f)\n",
    "\n",
    "df_ground_truth = pd.read_csv('../data/processed/ground-truth-retrieval.csv')\n",
    "ground_truth = df_ground_truth.to_dict(orient='records')\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents and {len(ground_truth)} ground truth questions\")\n",
    "print(f\"Document keys: {list(documents[0].keys()) if documents else 'No documents'}\")\n",
    "print(f\"Ground truth columns: {df_ground_truth.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19a14169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Qdrant. Existing collections: ['vector-search-jinaai-jina-embeddings-v2-small-en']\n"
     ]
    }
   ],
   "source": [
    "# Connect to Qdrant server\n",
    "client = QdrantClient(\"http://localhost:6333\")\n",
    "\n",
    "# Test connection by listing collections\n",
    "try:\n",
    "    collections = client.get_collections()\n",
    "    print(f\"Connected to Qdrant. Existing collections: {[c.name for c in collections.collections]}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to connect to Qdrant: {e}\")\n",
    "    print(\"Make sure Qdrant is running on localhost:6333\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebc462f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample document:\n",
      "{\n",
      "  \"location\": \"Andhra_Pradesh\",\n",
      "  \"doc_id\": \"d4402d82c0\",\n",
      "  \"content\": \"Asia > South Asia > India > Southern India > Andhra Pradesh  \\n![0_image_0.png](0_image_0.png)\",\n",
      "  \"id\": \"4f80b327\"\n",
      "}\n",
      "\n",
      "Sample ground truth entry:\n",
      "{'question': 'What are the must-see religious sites in Andhra Pradesh for pilgrims?', 'id': '4f80b327'}\n"
     ]
    }
   ],
   "source": [
    "# Examine data structure\n",
    "print(\"Sample document:\")\n",
    "print(json.dumps(documents[0], indent=2))\n",
    "print(\"\\nSample ground truth entry:\")\n",
    "print(ground_truth[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66db6608",
   "metadata": {},
   "source": [
    "### Create Sparse Vector Collection (BM25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3de6c3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create collection for sparse vectors only\n",
    "collection_name_sparse = \"sparse-vector-bm25\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0aa54149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existing collection: sparse-vector-bm25\n",
      "Created sparse collection: sparse-vector-bm25\n"
     ]
    }
   ],
   "source": [
    "# Delete collection if it exists\n",
    "try:\n",
    "    client.delete_collection(collection_name_sparse)\n",
    "    print(f\"Deleted existing collection: {collection_name_sparse}\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(f\"Created sparse collection: {collection_name_sparse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "32211657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create new collection with sparse vector configuration\n",
    "client.create_collection(\n",
    "    collection_name=collection_name_sparse,\n",
    "    sparse_vectors_config={\n",
    "        \"bm25\": models.SparseVectorParams(\n",
    "            modifier=models.Modifier.IDF,  # Enable IDF calculation for BM25\n",
    "        )\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ac99895a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 149 points from 149 documents\n",
      "Uploaded batch 1/2\n",
      "Uploaded batch 2/2\n",
      "Successfully uploaded 149 documents to sparse-vector-bm25\n",
      "Sparse vector upload took 0.35 seconds\n"
     ]
    }
   ],
   "source": [
    "def upload_sparse_vectors(documents: List[Dict], collection_name: str) -> None:\n",
    "    \"\"\"Upload documents as sparse vectors using BM25 model\"\"\"\n",
    "    \n",
    "    points = []\n",
    "    for doc in documents:\n",
    "        text_content = doc[\"content\"]\n",
    "        original_id = doc[\"id\"]\n",
    "        \n",
    "        # Convert string ID to valid UUID by hashing\n",
    "        hash_object = hashlib.md5(original_id.encode())\n",
    "        uuid_string = str(uuid.UUID(hash_object.hexdigest()))\n",
    "        \n",
    "        point = models.PointStruct(\n",
    "            id=uuid_string,  # Use the generated UUID for Qdrant\n",
    "            vector={\n",
    "                \"bm25\": models.Document(\n",
    "                    text=text_content, \n",
    "                    model=\"Qdrant/bm25\",\n",
    "                ),\n",
    "            },\n",
    "            payload={\n",
    "                \"content\": text_content,\n",
    "                \"location\": doc.get(\"location\", \"\"),\n",
    "                \"doc_id\": doc.get(\"doc_id\", \"\"),\n",
    "                \"id\": original_id  # Store original ID as 'id' in payload\n",
    "            }\n",
    "        )\n",
    "        points.append(point)\n",
    "    \n",
    "    # Move this print OUTSIDE the loop\n",
    "    print(f\"Created {len(points)} points from {len(documents)} documents\")\n",
    "    \n",
    "    # Upload in batches for better performance\n",
    "    batch_size = 100\n",
    "    for i in range(0, len(points), batch_size):\n",
    "        batch = points[i:i+batch_size]\n",
    "        client.upsert(collection_name=collection_name, points=batch)\n",
    "        print(f\"Uploaded batch {i//batch_size + 1}/{(len(points)-1)//batch_size + 1}\")\n",
    "    \n",
    "    print(f\"Successfully uploaded {len(points)} documents to {collection_name}\")\n",
    "\n",
    "# Then call the function\n",
    "start_time = time.time()\n",
    "upload_sparse_vectors(documents, collection_name_sparse)\n",
    "upload_time_sparse = time.time() - start_time\n",
    "print(f\"Sparse vector upload took {upload_time_sparse:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346e034a",
   "metadata": {},
   "source": [
    "### Sparse Vector Search Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "25ad8219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_search(query: str, collection_name: str, limit: int = 5) -> List[models.ScoredPoint]:\n",
    "    \"\"\"Perform sparse vector search using BM25\"\"\"\n",
    "    \n",
    "    results = client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=models.Document(\n",
    "            text=query,\n",
    "            model=\"Qdrant/bm25\",\n",
    "        ),\n",
    "        using=\"bm25\",\n",
    "        limit=limit,\n",
    "        with_payload=True,\n",
    "    )\n",
    "    \n",
    "    return results.points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c32b1ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse search results for: 'Is there any Unesco world heritage site in Karnataka or Andra Pradesh?'\n",
      "\n",
      "1. Score: 22.8500\n",
      "   Content: Thanks to its long and varied history Karnataka has an interesting mix of religious sites, remnants of historical empires, UNESCO World Heritage Sites...\n",
      "   Location: Karnataka\n",
      "   Doc ID: 9a1dcdf649\n",
      "   Original ID: N/A\n",
      "\n",
      "2. Score: 20.0857\n",
      "   Content: 1  \n",
      "![1_image_0.png](1_image_0.png)  \n",
      "Hampi - the ruins of the fabulous Vijayanagar Empire. Considered to be one of the finest cities in its time circ...\n",
      "   Location: Karnataka\n",
      "   Doc ID: 0bbac20900\n",
      "   Original ID: N/A\n",
      "\n",
      "3. Score: 10.2497\n",
      "   Content: religious and archeological site of Hampi. Full of other-worldly geology and nature. Northwestern Karnataka Good transport connections, but interior i...\n",
      "   Location: Karnataka\n",
      "   Doc ID: 0bbac20900\n",
      "   Original ID: N/A\n",
      "\n",
      "4. Score: 9.8457\n",
      "   Content: Bangalore The state capital as well as the information technology capital of India. Cauvery Basin Southern Karnataka, including Mysore. Rolling hills,...\n",
      "   Location: Karnataka\n",
      "   Doc ID: 9a1dcdf649\n",
      "   Original ID: N/A\n",
      "\n",
      "5. Score: 9.3963\n",
      "   Content: See also: South Asian cuisine Like almost every other state in India, Andhra Pradesh has a rich variety of cuisines and change widely from region to r...\n",
      "   Location: Andhra_Pradesh\n",
      "   Doc ID: d4402d82c0\n",
      "   Original ID: N/A\n"
     ]
    }
   ],
   "source": [
    "# Test sparse search\n",
    "test_query = \"Is there any Unesco world heritage site in Karnataka or Andra Pradesh?\"\n",
    "sparse_results = sparse_search(test_query, collection_name_sparse, limit=5)\n",
    "\n",
    "print(f\"Sparse search results for: '{test_query}'\")\n",
    "for i, result in enumerate(sparse_results, 1):\n",
    "    print(f\"\\n{i}. Score: {result.score:.4f}\")\n",
    "    print(f\"   Content: {result.payload['content'][:150]}...\")  # Changed from 'text' to 'content'\n",
    "    print(f\"   Location: {result.payload.get('location', 'N/A')}\")  # Changed from 'section'\n",
    "    print(f\"   Doc ID: {result.payload.get('doc_id', 'N/A')}\")  # Changed from 'section'\n",
    "    print(f\"   Original ID: {result.payload.get('original_id', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e66783",
   "metadata": {},
   "source": [
    "### Create Hybrid Collection (Dense + Sparse Vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "55385499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create collection for hybrid search (both dense and sparse vectors)\n",
    "collection_name_hybrid = \"documents-hybrid-search\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0ef5087a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existing collection: documents-hybrid-search\n"
     ]
    }
   ],
   "source": [
    "# Delete collection if it exists\n",
    "try:\n",
    "    client.delete_collection(collection_name_hybrid)\n",
    "    print(f\"Deleted existing collection: {collection_name_hybrid}\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2a7f822f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created hybrid collection: documents-hybrid-search\n"
     ]
    }
   ],
   "source": [
    "# Create hybrid collection with both vector types\n",
    "client.create_collection(\n",
    "    collection_name=collection_name_hybrid,\n",
    "    vectors_config={\n",
    "        # Dense vector configuration\n",
    "        \"jina-small\": models.VectorParams(\n",
    "            size=512,  # Dimension size for jina-embeddings-v2-small-en\n",
    "            distance=models.Distance.COSINE,\n",
    "        ),\n",
    "    },\n",
    "    sparse_vectors_config={\n",
    "        # Sparse vector configuration  \n",
    "        \"bm25\": models.SparseVectorParams(\n",
    "            modifier=models.Modifier.IDF,\n",
    "        )\n",
    "    }\n",
    ")\n",
    "print(f\"Created hybrid collection: {collection_name_hybrid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "abada436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded hybrid batch 1/3\n",
      "Uploaded hybrid batch 2/3\n",
      "Uploaded hybrid batch 3/3\n",
      "Successfully uploaded 149 documents with hybrid vectors\n",
      "Hybrid vector upload took 9.21 seconds\n"
     ]
    }
   ],
   "source": [
    "def upload_hybrid_vectors(documents: List[Dict], collection_name: str) -> None:\n",
    "    \"\"\"Upload documents with both dense and sparse vectors\"\"\"\n",
    "    \n",
    "    points = []\n",
    "    for doc in documents:\n",
    "        text_content = doc[\"content\"]\n",
    "        original_id = doc[\"id\"]\n",
    "        \n",
    "        # Convert string ID to valid UUID by hashing\n",
    "        hash_object = hashlib.md5(original_id.encode())\n",
    "        uuid_string = str(uuid.UUID(hash_object.hexdigest()))\n",
    "        \n",
    "        point = models.PointStruct(\n",
    "            id=uuid_string,  # Use the generated UUID for Qdrant\n",
    "            vector={\n",
    "                # Dense vector using Jina model\n",
    "                \"jina-small\": models.Document(\n",
    "                    text=text_content,\n",
    "                    model=\"jinaai/jina-embeddings-v2-small-en\",\n",
    "                ),\n",
    "                # Sparse vector using BM25\n",
    "                \"bm25\": models.Document(\n",
    "                    text=text_content, \n",
    "                    model=\"Qdrant/bm25\",\n",
    "                ),\n",
    "            },\n",
    "            payload={\n",
    "                \"content\": text_content,\n",
    "                \"location\": doc.get(\"location\", \"\"),\n",
    "                \"doc_id\": doc.get(\"doc_id\", \"\"),\n",
    "                \"id\": original_id  # Store original ID as 'id' in payload\n",
    "            }\n",
    "        )\n",
    "        points.append(point)\n",
    "    \n",
    "    # Upload in batches\n",
    "    batch_size = 50  # Smaller batches for hybrid upload\n",
    "    for i in range(0, len(points), batch_size):\n",
    "        batch = points[i:i+batch_size]\n",
    "        client.upsert(collection_name=collection_name, points=batch)\n",
    "        print(f\"Uploaded hybrid batch {i//batch_size + 1}/{(len(points)-1)//batch_size + 1}\")\n",
    "    \n",
    "    print(f\"Successfully uploaded {len(points)} documents with hybrid vectors\")\n",
    "\n",
    "# Upload documents (this will take longer due to dense embeddings)\n",
    "start_time = time.time()\n",
    "upload_hybrid_vectors(documents, collection_name_hybrid)\n",
    "upload_time_hybrid = time.time() - start_time\n",
    "print(f\"Hybrid vector upload took {upload_time_hybrid:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903b21e5",
   "metadata": {},
   "source": [
    "### Hybrid Search Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b65f15e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_search(query: str, collection_name: str, limit: int = 5) -> List[models.ScoredPoint]:\n",
    "    \"\"\"Perform dense vector search using semantic embeddings\"\"\"\n",
    "    \n",
    "    results = client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=models.Document(\n",
    "            text=query,\n",
    "            model=\"jinaai/jina-embeddings-v2-small-en\",\n",
    "        ),\n",
    "        using=\"jina-small\",\n",
    "        limit=limit,\n",
    "        with_payload=True,\n",
    "    )\n",
    "    \n",
    "    return results.points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "38010394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_stage_search(query: str, collection_name: str, limit: int = 5) -> List[models.ScoredPoint]:\n",
    "    \"\"\"Multi-stage search: dense retrieval followed by sparse reranking\"\"\"\n",
    "    \n",
    "    results = client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        prefetch=[\n",
    "            models.Prefetch(\n",
    "                query=models.Document(\n",
    "                    text=query,\n",
    "                    model=\"jinaai/jina-embeddings-v2-small-en\",\n",
    "                ),\n",
    "                using=\"jina-small\",\n",
    "                limit=(10 * limit),  # Prefetch more candidates for reranking\n",
    "            ),\n",
    "        ],\n",
    "        query=models.Document(\n",
    "            text=query,\n",
    "            model=\"Qdrant/bm25\", \n",
    "        ),\n",
    "        using=\"bm25\",\n",
    "        limit=limit,\n",
    "        with_payload=True,\n",
    "    )\n",
    "    \n",
    "    return results.points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "994e9884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rrf_search(query: str, collection_name: str, limit: int = 5) -> List[models.ScoredPoint]:\n",
    "    \"\"\"Reciprocal Rank Fusion combining dense and sparse search\"\"\"\n",
    "    \n",
    "    results = client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        prefetch=[\n",
    "            # Dense vector prefetch\n",
    "            models.Prefetch(\n",
    "                query=models.Document(\n",
    "                    text=query,\n",
    "                    model=\"jinaai/jina-embeddings-v2-small-en\",\n",
    "                ),\n",
    "                using=\"jina-small\",\n",
    "                limit=(5 * limit),  # Get more candidates for fusion\n",
    "            ),\n",
    "            # Sparse vector prefetch\n",
    "            models.Prefetch(\n",
    "                query=models.Document(\n",
    "                    text=query,\n",
    "                    model=\"Qdrant/bm25\",\n",
    "                ),\n",
    "                using=\"bm25\",\n",
    "                limit=(5 * limit),\n",
    "            ),\n",
    "        ],\n",
    "        # Apply RRF fusion to combine results\n",
    "        query=models.FusionQuery(fusion=models.Fusion.RRF),\n",
    "        limit=limit,\n",
    "        with_payload=True,\n",
    "    )\n",
    "    \n",
    "    return results.points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8caff22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing search methods with query: 'Is there a Unesco world heritage site in Karnataka or Andra Pradesh?'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test all search methods with the same query\n",
    "test_query = \"Is there a Unesco world heritage site in Karnataka or Andra Pradesh?\"\n",
    "print(f\"Testing search methods with query: '{test_query}'\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fed5c226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DENSE SEARCH RESULTS ===\n",
      "1. Score: 0.8651 | Thanks to its long and varied history Karnataka has an interesting mix of religious sites, remnants ...\n",
      "2. Score: 0.8552 | Owing to the multi-religious influence in Karnataka through its history, there are a vast number of ...\n",
      "3. Score: 0.8372 | places are well known for beautiful mountain ranges, national parks, forests, wild animals and water...\n"
     ]
    }
   ],
   "source": [
    "# Dense search\n",
    "dense_results = dense_search(test_query, collection_name_hybrid, limit=3)\n",
    "print(\"=== DENSE SEARCH RESULTS ===\")\n",
    "for i, result in enumerate(dense_results, 1):\n",
    "    print(f\"{i}. Score: {result.score:.4f} | {result.payload['content'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f84bcdcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SPARSE SEARCH RESULTS ===\n",
      "1. Score: 22.8500 | Thanks to its long and varied history Karnataka has an interesting mix of religious sites, remnants ...\n",
      "2. Score: 20.0857 | 1  \n",
      "![1_image_0.png](1_image_0.png)  \n",
      "Hampi - the ruins of the fabulous Vijayanagar Empire. Consider...\n",
      "3. Score: 10.2497 | religious and archeological site of Hampi. Full of other-worldly geology and nature. Northwestern Ka...\n"
     ]
    }
   ],
   "source": [
    "# Sparse search on hybrid collection\n",
    "sparse_results_hybrid = sparse_search(test_query, collection_name_hybrid, limit=3)\n",
    "print(\"\\n=== SPARSE SEARCH RESULTS ===\")\n",
    "for i, result in enumerate(sparse_results_hybrid, 1):\n",
    "    print(f\"{i}. Score: {result.score:.4f} | {result.payload['content'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3f688d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MULTI-STAGE SEARCH RESULTS ===\n",
      "1. Score: 22.8500 | Thanks to its long and varied history Karnataka has an interesting mix of religious sites, remnants ...\n",
      "2. Score: 20.0857 | 1  \n",
      "![1_image_0.png](1_image_0.png)  \n",
      "Hampi - the ruins of the fabulous Vijayanagar Empire. Consider...\n",
      "3. Score: 10.2497 | religious and archeological site of Hampi. Full of other-worldly geology and nature. Northwestern Ka...\n"
     ]
    }
   ],
   "source": [
    "# Multi-stage search\n",
    "multi_stage_results = multi_stage_search(test_query, collection_name_hybrid, limit=3)\n",
    "print(\"\\n=== MULTI-STAGE SEARCH RESULTS ===\")\n",
    "for i, result in enumerate(multi_stage_results, 1):\n",
    "    print(f\"{i}. Score: {result.score:.4f} | {result.payload['content'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0faa2d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RRF HYBRID SEARCH RESULTS ===\n",
      "1. Score: 1.0000 | Thanks to its long and varied history Karnataka has an interesting mix of religious sites, remnants ...\n",
      "2. Score: 0.4333 | Owing to the multi-religious influence in Karnataka through its history, there are a vast number of ...\n",
      "3. Score: 0.3500 | religious and archeological site of Hampi. Full of other-worldly geology and nature. Northwestern Ka...\n"
     ]
    }
   ],
   "source": [
    "# RRF search\n",
    "rrf_results = rrf_search(test_query, collection_name_hybrid, limit=3)\n",
    "print(\"\\n=== RRF HYBRID SEARCH RESULTS ===\")\n",
    "for i, result in enumerate(rrf_results, 1):\n",
    "    print(f\"{i}. Score: {result.score:.4f} | {result.payload['content'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed04068d",
   "metadata": {},
   "source": [
    "### Evaluation Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "960a3081",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_search_method(search_function, collection_name: str, ground_truth: List[Dict], \n",
    "                         method_name: str, top_k: int = 5) -> Dict[str, Any]:\n",
    "    \"\"\"Evaluate a search method against ground truth data\"\"\"\n",
    "    \n",
    "    results = {\n",
    "        'method': method_name,\n",
    "        'total_queries': len(ground_truth),\n",
    "        'hits_at_1': 0,\n",
    "        'hits_at_3': 0, \n",
    "        'hits_at_5': 0,\n",
    "        'mrr_scores': [],  # Mean Reciprocal Rank\n",
    "        'search_times': [],\n",
    "        'failed_queries': 0\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nEvaluating {method_name}...\")\n",
    "    \n",
    "    for i, gt_item in enumerate(ground_truth):\n",
    "        if i % 20 == 0:\n",
    "            print(f\"Processed {i}/{len(ground_truth)} queries\")\n",
    "            \n",
    "        query = gt_item['question']\n",
    "        expected_doc_id = gt_item['id']  # From your CSV\n",
    "        \n",
    "        try:\n",
    "            # Measure search time\n",
    "            start_time = time.time()\n",
    "            search_results = search_function(query, collection_name, limit=top_k)\n",
    "            search_time = time.time() - start_time\n",
    "            results['search_times'].append(search_time)\n",
    "            \n",
    "            # Extract document IDs from results (now using 'id' instead of 'original_id')\n",
    "            retrieved_doc_ids = [result.payload.get('id') for result in search_results]\n",
    "            \n",
    "            # Calculate hits@k and MRR\n",
    "            if expected_doc_id in retrieved_doc_ids:\n",
    "                rank = retrieved_doc_ids.index(expected_doc_id) + 1\n",
    "                \n",
    "                # Hits@k calculation\n",
    "                if rank <= 1:\n",
    "                    results['hits_at_1'] += 1\n",
    "                if rank <= 3:\n",
    "                    results['hits_at_3'] += 1\n",
    "                if rank <= 5:\n",
    "                    results['hits_at_5'] += 1\n",
    "                    \n",
    "                # MRR calculation\n",
    "                results['mrr_scores'].append(1.0 / rank)\n",
    "            else:\n",
    "                results['mrr_scores'].append(0.0)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Failed query {i}: {e}\")\n",
    "            results['failed_queries'] += 1\n",
    "            results['mrr_scores'].append(0.0)\n",
    "            results['search_times'].append(0.0)\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    total_queries = results['total_queries']\n",
    "    results['hit_rate_at_1'] = results['hits_at_1'] / total_queries\n",
    "    results['hit_rate_at_3'] = results['hits_at_3'] / total_queries  \n",
    "    results['hit_rate_at_5'] = results['hits_at_5'] / total_queries\n",
    "    results['mean_reciprocal_rank'] = np.mean(results['mrr_scores'])\n",
    "    results['avg_search_time'] = np.mean(results['search_times'])\n",
    "\n",
    "    print(f\"Completed evaluation of {method_name}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "31a9a32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking function signatures...\n",
      "sparse_search signature: (query: str, collection_name: str, limit: int = 5) -> List[qdrant_client.http.models.models.ScoredPoint]\n",
      "dense_search signature: (query: str, collection_name: str, limit: int = 5) -> List[qdrant_client.http.models.models.ScoredPoint]\n",
      "multi_stage_search signature: (query: str, collection_name: str, limit: int = 5) -> List[qdrant_client.http.models.models.ScoredPoint]\n",
      "rrf_search signature: (query: str, collection_name: str, limit: int = 5) -> List[qdrant_client.http.models.models.ScoredPoint]\n",
      "\n",
      "Testing wrapper options...\n",
      "✗ Wrapper 'top_k' failed: sparse_search() got an unexpected keyword argument 'top_k'\n",
      "✗ Wrapper 'k' failed: sparse_search() got an unexpected keyword argument 'k'\n",
      "✗ Wrapper 'size' failed: sparse_search() got an unexpected keyword argument 'size'\n",
      "✗ Wrapper 'n_results' failed: sparse_search() got an unexpected keyword argument 'n_results'\n",
      "✓ Wrapper 'no_limit_param' works!\n",
      "\n",
      "Using working wrapper for evaluation...\n",
      "Starting comprehensive evaluation...\n",
      "Evaluating on 735 ground truth questions\n",
      "\n",
      "Evaluating BM25 Sparse...\n",
      "Processed 0/100 queries\n",
      "Processed 20/100 queries\n",
      "Processed 40/100 queries\n",
      "Processed 60/100 queries\n",
      "Processed 80/100 queries\n",
      "Completed evaluation of BM25 Sparse\n",
      "✓ dense_search accepts 'limit' parameter\n",
      "\n",
      "Evaluating Dense Semantic...\n",
      "Processed 0/100 queries\n",
      "Processed 20/100 queries\n",
      "Processed 40/100 queries\n",
      "Processed 60/100 queries\n",
      "Processed 80/100 queries\n",
      "Completed evaluation of Dense Semantic\n",
      "✓ multi_stage_search accepts 'limit' parameter\n",
      "\n",
      "Evaluating Multi-stage (Dense→Sparse)...\n",
      "Processed 0/100 queries\n",
      "Processed 20/100 queries\n",
      "Processed 40/100 queries\n",
      "Processed 60/100 queries\n",
      "Processed 80/100 queries\n",
      "Completed evaluation of Multi-stage (Dense→Sparse)\n",
      "✓ rrf_search accepts 'limit' parameter\n",
      "\n",
      "Evaluating RRF Hybrid...\n",
      "Processed 0/100 queries\n",
      "Processed 20/100 queries\n",
      "Processed 40/100 queries\n",
      "Processed 60/100 queries\n",
      "Processed 80/100 queries\n",
      "Completed evaluation of RRF Hybrid\n",
      "\n",
      "Evaluation completed!\n"
     ]
    }
   ],
   "source": [
    "# First, let's check the actual signatures of your search functions\n",
    "# Run this to understand what parameters your functions accept:\n",
    "\n",
    "print(\"Checking function signatures...\")\n",
    "\n",
    "# Check sparse_search function signature\n",
    "import inspect\n",
    "try:\n",
    "    sparse_sig = inspect.signature(sparse_search)\n",
    "    print(f\"sparse_search signature: {sparse_sig}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not get sparse_search signature: {e}\")\n",
    "\n",
    "try:\n",
    "    dense_sig = inspect.signature(dense_search)\n",
    "    print(f\"dense_search signature: {dense_sig}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not get dense_search signature: {e}\")\n",
    "\n",
    "try:\n",
    "    multi_sig = inspect.signature(multi_stage_search)\n",
    "    print(f\"multi_stage_search signature: {multi_sig}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not get multi_stage_search signature: {e}\")\n",
    "\n",
    "try:\n",
    "    rrf_sig = inspect.signature(rrf_search)\n",
    "    print(f\"rrf_search signature: {rrf_sig}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not get rrf_search signature: {e}\")\n",
    "\n",
    "# Common fixes based on typical function signatures:\n",
    "\n",
    "# Option 1: If sparse_search takes (query, collection, top_k) or similar\n",
    "def create_sparse_wrapper_v1():\n",
    "    return lambda q, c, limit: sparse_search(q, c, top_k=limit)\n",
    "\n",
    "# Option 2: If sparse_search takes (query, collection, k) \n",
    "def create_sparse_wrapper_v2():\n",
    "    return lambda q, c, limit: sparse_search(q, c, k=limit)\n",
    "\n",
    "# Option 3: If sparse_search takes (query, collection, size)\n",
    "def create_sparse_wrapper_v3():\n",
    "    return lambda q, c, limit: sparse_search(q, c, size=limit)\n",
    "\n",
    "# Option 4: If sparse_search takes (query, collection, n_results)\n",
    "def create_sparse_wrapper_v4():\n",
    "    return lambda q, c, limit: sparse_search(q, c, n_results=limit)\n",
    "\n",
    "# Option 5: If sparse_search only takes (query, collection) and has default limit\n",
    "def create_sparse_wrapper_v5():\n",
    "    # This ignores the limit parameter since the function doesn't support it\n",
    "    return lambda q, c, limit: sparse_search(q, c)\n",
    "\n",
    "# RECOMMENDED SOLUTION - Test each wrapper:\n",
    "print(\"\\nTesting wrapper options...\")\n",
    "\n",
    "# Test with a simple query to see which wrapper works\n",
    "test_query = \"test\"\n",
    "test_collection = collection_name_sparse\n",
    "\n",
    "wrapper_options = [\n",
    "    (\"top_k\", create_sparse_wrapper_v1),\n",
    "    (\"k\", create_sparse_wrapper_v2), \n",
    "    (\"size\", create_sparse_wrapper_v3),\n",
    "    (\"n_results\", create_sparse_wrapper_v4),\n",
    "    (\"no_limit_param\", create_sparse_wrapper_v5)\n",
    "]\n",
    "\n",
    "working_wrapper = None\n",
    "for name, wrapper_func in wrapper_options:\n",
    "    try:\n",
    "        wrapper = wrapper_func()\n",
    "        result = wrapper(test_query, test_collection, 5)\n",
    "        print(f\"✓ Wrapper '{name}' works!\")\n",
    "        working_wrapper = wrapper\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Wrapper '{name}' failed: {e}\")\n",
    "\n",
    "if working_wrapper:\n",
    "    print(f\"\\nUsing working wrapper for evaluation...\")\n",
    "    \n",
    "    # Run comprehensive evaluation with the working wrapper\n",
    "    print(\"Starting comprehensive evaluation...\")\n",
    "    print(f\"Evaluating on {len(ground_truth)} ground truth questions\")\n",
    "\n",
    "    evaluation_results = []\n",
    "\n",
    "    # Evaluate sparse search with working wrapper\n",
    "    sparse_eval = evaluate_search_method(\n",
    "        working_wrapper,\n",
    "        collection_name_sparse,\n",
    "        ground_truth[:100],  # Use subset for faster evaluation\n",
    "        \"BM25 Sparse\"\n",
    "    )\n",
    "    evaluation_results.append(sparse_eval)\n",
    "\n",
    "    # For the other functions, create similar wrappers if needed\n",
    "    # Check if they also have parameter issues:\n",
    "    \n",
    "    # Test dense_search\n",
    "    try:\n",
    "        dense_test = dense_search(test_query, collection_name_hybrid, limit=5)\n",
    "        dense_wrapper = dense_search\n",
    "        print(\"✓ dense_search accepts 'limit' parameter\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ dense_search parameter issue: {e}\")\n",
    "        # Try common alternatives\n",
    "        try:\n",
    "            dense_test = dense_search(test_query, collection_name_hybrid, top_k=5)\n",
    "            dense_wrapper = lambda q, c, limit: dense_search(q, c, top_k=limit)\n",
    "            print(\"✓ dense_search uses 'top_k' parameter\")\n",
    "        except:\n",
    "            try:\n",
    "                dense_test = dense_search(test_query, collection_name_hybrid, k=5)\n",
    "                dense_wrapper = lambda q, c, limit: dense_search(q, c, k=limit)\n",
    "                print(\"✓ dense_search uses 'k' parameter\")\n",
    "            except:\n",
    "                dense_wrapper = lambda q, c, limit: dense_search(q, c)\n",
    "                print(\"✓ dense_search ignores limit parameter\")\n",
    "\n",
    "    # Evaluate dense search\n",
    "    dense_eval = evaluate_search_method(\n",
    "        dense_wrapper,\n",
    "        collection_name_hybrid, \n",
    "        ground_truth[:100],\n",
    "        \"Dense Semantic\"\n",
    "    )\n",
    "    evaluation_results.append(dense_eval)\n",
    "\n",
    "    # Test multi_stage_search\n",
    "    try:\n",
    "        multi_test = multi_stage_search(test_query, collection_name_hybrid, limit=5)\n",
    "        multi_wrapper = multi_stage_search\n",
    "        print(\"✓ multi_stage_search accepts 'limit' parameter\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ multi_stage_search parameter issue: {e}\")\n",
    "        try:\n",
    "            multi_test = multi_stage_search(test_query, collection_name_hybrid, top_k=5)\n",
    "            multi_wrapper = lambda q, c, limit: multi_stage_search(q, c, top_k=limit)\n",
    "            print(\"✓ multi_stage_search uses 'top_k' parameter\")\n",
    "        except:\n",
    "            try:\n",
    "                multi_test = multi_stage_search(test_query, collection_name_hybrid, k=5)\n",
    "                multi_wrapper = lambda q, c, limit: multi_stage_search(q, c, k=limit)\n",
    "                print(\"✓ multi_stage_search uses 'k' parameter\")\n",
    "            except:\n",
    "                multi_wrapper = lambda q, c, limit: multi_stage_search(q, c)\n",
    "                print(\"✓ multi_stage_search ignores limit parameter\")\n",
    "\n",
    "    # Evaluate multi-stage search\n",
    "    multi_stage_eval = evaluate_search_method(\n",
    "        multi_wrapper,\n",
    "        collection_name_hybrid,\n",
    "        ground_truth[:100], \n",
    "        \"Multi-stage (Dense→Sparse)\"\n",
    "    )\n",
    "    evaluation_results.append(multi_stage_eval)\n",
    "\n",
    "    # Test rrf_search\n",
    "    try:\n",
    "        rrf_test = rrf_search(test_query, collection_name_hybrid, limit=5)\n",
    "        rrf_wrapper = rrf_search\n",
    "        print(\"✓ rrf_search accepts 'limit' parameter\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ rrf_search parameter issue: {e}\")\n",
    "        try:\n",
    "            rrf_test = rrf_search(test_query, collection_name_hybrid, top_k=5)\n",
    "            rrf_wrapper = lambda q, c, limit: rrf_search(q, c, top_k=limit)\n",
    "            print(\"✓ rrf_search uses 'top_k' parameter\")\n",
    "        except:\n",
    "            try:\n",
    "                rrf_test = rrf_search(test_query, collection_name_hybrid, k=5)\n",
    "                rrf_wrapper = lambda q, c, limit: rrf_search(q, c, k=limit)\n",
    "                print(\"✓ rrf_search uses 'k' parameter\")\n",
    "            except:\n",
    "                rrf_wrapper = lambda q, c, limit: rrf_search(q, c)\n",
    "                print(\"✓ rrf_search ignores limit parameter\")\n",
    "\n",
    "    # Evaluate RRF hybrid search\n",
    "    rrf_eval = evaluate_search_method(\n",
    "        rrf_wrapper,\n",
    "        collection_name_hybrid,\n",
    "        ground_truth[:100],\n",
    "        \"RRF Hybrid\"\n",
    "    )\n",
    "    evaluation_results.append(rrf_eval)\n",
    "\n",
    "    print(\"\\nEvaluation completed!\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n❌ Could not find working wrapper for sparse_search!\")\n",
    "    print(\"Please check your sparse_search function definition and parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6a705f",
   "metadata": {},
   "source": [
    "### Results Analysis and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e4471224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive results summary\n",
    "def create_results_summary(evaluation_results: List[Dict]) -> pd.DataFrame:\n",
    "    \"\"\"Create a summary DataFrame of all evaluation results\"\"\"\n",
    "    \n",
    "    summary_data = []\n",
    "    \n",
    "    for result in evaluation_results:\n",
    "        summary_data.append({\n",
    "            'Method': result['method'],\n",
    "            'Hit Rate @1': f\"{result['hit_rate_at_1']:.3f}\",\n",
    "            'Hit Rate @3': f\"{result['hit_rate_at_3']:.3f}\", \n",
    "            'Hit Rate @5': f\"{result['hit_rate_at_5']:.3f}\",\n",
    "            'Mean Reciprocal Rank': f\"{result['mean_reciprocal_rank']:.3f}\",\n",
    "            'Avg Search Time (ms)': f\"{result['avg_search_time']*1000:.1f}\",\n",
    "            'Failed Queries': result['failed_queries'],\n",
    "            'Total Queries': result['total_queries']\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(summary_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f50bca0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== HYBRID SEARCH EVALUATION RESULTS ===\n",
      "                    Method Hit Rate @1 Hit Rate @3 Hit Rate @5 Mean Reciprocal Rank Avg Search Time (ms)  Failed Queries  Total Queries\n",
      "               BM25 Sparse       0.180       0.300       0.360                0.246                 23.4               0            100\n",
      "            Dense Semantic       0.190       0.380       0.460                0.291                 26.4               0            100\n",
      "Multi-stage (Dense→Sparse)       0.180       0.300       0.360                0.246                 34.8               0            100\n",
      "                RRF Hybrid       0.220       0.350       0.460                0.298                 38.0               0            100\n"
     ]
    }
   ],
   "source": [
    "# Generate results summary\n",
    "results_df = create_results_summary(evaluation_results)\n",
    "print(\"=== HYBRID SEARCH EVALUATION RESULTS ===\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e15bbeb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DETAILED PERFORMANCE ANALYSIS ===\n",
      "\n",
      "BM25 Sparse:\n",
      "  • Hit Rate @1: 0.180 (18/100)\n",
      "  • Hit Rate @3: 0.300 (30/100)\n",
      "  • Hit Rate @5: 0.360 (36/100)\n",
      "  • Mean Reciprocal Rank: 0.246\n",
      "  • Average Search Time: 23.4 ms\n",
      "  • Failed Queries: 0\n",
      "\n",
      "Dense Semantic:\n",
      "  • Hit Rate @1: 0.190 (19/100)\n",
      "  • Hit Rate @3: 0.380 (38/100)\n",
      "  • Hit Rate @5: 0.460 (46/100)\n",
      "  • Mean Reciprocal Rank: 0.291\n",
      "  • Average Search Time: 26.4 ms\n",
      "  • Failed Queries: 0\n",
      "\n",
      "Multi-stage (Dense→Sparse):\n",
      "  • Hit Rate @1: 0.180 (18/100)\n",
      "  • Hit Rate @3: 0.300 (30/100)\n",
      "  • Hit Rate @5: 0.360 (36/100)\n",
      "  • Mean Reciprocal Rank: 0.246\n",
      "  • Average Search Time: 34.8 ms\n",
      "  • Failed Queries: 0\n",
      "\n",
      "RRF Hybrid:\n",
      "  • Hit Rate @1: 0.220 (22/100)\n",
      "  • Hit Rate @3: 0.350 (35/100)\n",
      "  • Hit Rate @5: 0.460 (46/100)\n",
      "  • Mean Reciprocal Rank: 0.298\n",
      "  • Average Search Time: 38.0 ms\n",
      "  • Failed Queries: 0\n"
     ]
    }
   ],
   "source": [
    "# Display detailed analysis\n",
    "print(\"\\n=== DETAILED PERFORMANCE ANALYSIS ===\")\n",
    "for result in evaluation_results:\n",
    "    print(f\"\\n{result['method']}:\")\n",
    "    print(f\"  • Hit Rate @1: {result['hit_rate_at_1']:.3f} ({result['hits_at_1']}/{result['total_queries']})\")\n",
    "    print(f\"  • Hit Rate @3: {result['hit_rate_at_3']:.3f} ({result['hits_at_3']}/{result['total_queries']})\")\n",
    "    print(f\"  • Hit Rate @5: {result['hit_rate_at_5']:.3f} ({result['hits_at_5']}/{result['total_queries']})\")\n",
    "    print(f\"  • Mean Reciprocal Rank: {result['mean_reciprocal_rank']:.3f}\")\n",
    "    print(f\"  • Average Search Time: {result['avg_search_time']*1000:.1f} ms\")\n",
    "    print(f\"  • Failed Queries: {result['failed_queries']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "72ae339b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== KEY FINDINGS ===\n",
      "Best Overall Performance (MRR): RRF Hybrid (0.298)\n",
      "Fastest Search Method: BM25 Sparse (23.4 ms)\n",
      "Best Hit Rate @5: Dense Semantic (0.460)\n"
     ]
    }
   ],
   "source": [
    "# Find best performing method\n",
    "best_mrr_method = max(evaluation_results, key=lambda x: x['mean_reciprocal_rank'])\n",
    "best_speed_method = min(evaluation_results, key=lambda x: x['avg_search_time'])\n",
    "best_hit5_method = max(evaluation_results, key=lambda x: x['hit_rate_at_5'])\n",
    "\n",
    "print(f\"\\n=== KEY FINDINGS ===\")\n",
    "print(f\"Best Overall Performance (MRR): {best_mrr_method['method']} ({best_mrr_method['mean_reciprocal_rank']:.3f})\")\n",
    "print(f\"Fastest Search Method: {best_speed_method['method']} ({best_speed_method['avg_search_time']*1000:.1f} ms)\")\n",
    "print(f\"Best Hit Rate @5: {best_hit5_method['method']} ({best_hit5_method['hit_rate_at_5']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36693ccb",
   "metadata": {},
   "source": [
    "### Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "775b8bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results will be saved to: C:\\Users\\Adi\\Brahman.ai\\results\\hybrid_search_evaluation\n",
      "Timestamp: 20250816_171653\n"
     ]
    }
   ],
   "source": [
    "# Setup results directory and file paths\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Define base results directory \n",
    "results_base_dir = Path.home() / \"Brahman.ai\" / \"results\"\n",
    "results= results_base_dir / \"hybrid_search_evaluation\"\n",
    "\n",
    "# Generate timestamp for unique file naming\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Define output file paths\n",
    "results_summary_path = results / f\"evaluation_summary_{timestamp}.csv\"\n",
    "detailed_results_path = results / f\"detailed_results_{timestamp}.json\"\n",
    "analysis_report_path = results / f\"analysis_report_{timestamp}.txt\"\n",
    "metrics_comparison_path = results / f\"metrics_comparison_{timestamp}.csv\"\n",
    "\n",
    "print(f\"Results will be saved to: {results}\")\n",
    "print(f\"Timestamp: {timestamp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bc71caae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved summary results to: evaluation_summary_20250816_171653.csv\n",
      "✅ Saved detailed results to: detailed_results_20250816_171653.json\n"
     ]
    }
   ],
   "source": [
    "# Save evaluation results summary as CSV\n",
    "results_df.to_csv(results_summary_path, index=False)\n",
    "print(f\"✅ Saved summary results to: {results_summary_path.name}\")\n",
    "\n",
    "# Save detailed results as JSON for future analysis\n",
    "detailed_results = {\n",
    "    \"evaluation_metadata\": {\n",
    "        \"timestamp\": timestamp,\n",
    "        \"total_ground_truth_questions\": len(ground_truth),\n",
    "        \"evaluation_subset_size\": 100,\n",
    "        \"qdrant_collections\": {\n",
    "            \"sparse_only\": collection_name_sparse,\n",
    "            \"hybrid\": collection_name_hybrid\n",
    "        },\n",
    "        \"embedding_models\": {\n",
    "            \"dense\": \"jinaai/jina-embeddings-v2-small-en\",\n",
    "            \"sparse\": \"Qdrant/bm25\"\n",
    "        }\n",
    "    },\n",
    "    \"evaluation_results\": evaluation_results,\n",
    "    \"performance_summary\": {\n",
    "        \"best_mrr_method\": {\n",
    "            \"method\": best_mrr_method['method'],\n",
    "            \"score\": best_mrr_method['mean_reciprocal_rank']\n",
    "        },\n",
    "        \"fastest_method\": {\n",
    "            \"method\": best_speed_method['method'], \n",
    "            \"time_ms\": best_speed_method['avg_search_time'] * 1000\n",
    "        },\n",
    "        \"best_hit5_method\": {\n",
    "            \"method\": best_hit5_method['method'],\n",
    "            \"hit_rate\": best_hit5_method['hit_rate_at_5']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(detailed_results_path, 'w') as f:\n",
    "    json.dump(detailed_results, f, indent=2)\n",
    "print(f\"✅ Saved detailed results to: {detailed_results_path.name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brahman_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
