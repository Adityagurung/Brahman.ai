{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Summary and Report Generation\n",
    "This notebook creates comprehensive visualizations and reports from all evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup paths and initialize variables\n",
    "results_dir = Path('results')\n",
    "retrieval_results = {}\n",
    "rag_results = None\n",
    "\n",
    "# Create results directory if it doesn't exist\n",
    "os.makedirs('results', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load All Retrieval Results\n",
    "Load and process results from all retrieval evaluation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading retrieval evaluation results...\")\n",
    "\n",
    "# List of retrieval methods to load\n",
    "methods = ['keyword_search', 'vector_search', 'hybrid_search']\n",
    "\n",
    "for method in methods:\n",
    "    result_file = results_dir / f'{method}_results.json'\n",
    "    \n",
    "    if result_file.exists():\n",
    "        with open(result_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        if method == 'hybrid_search':\n",
    "            # Get best alpha result for hybrid search\n",
    "            best_alpha = data['best_alpha']\n",
    "            metrics = data['alpha_results'][best_alpha]['metrics']\n",
    "            retrieval_results[method] = {\n",
    "                'hit_rate': metrics['hit_rate'],\n",
    "                'mrr': metrics['mrr'],\n",
    "                'total_questions': metrics['total_questions'],\n",
    "                'best_alpha': best_alpha\n",
    "            }\n",
    "        else:\n",
    "            # Standard retrieval method results\n",
    "            metrics = data['metrics']\n",
    "            retrieval_results[method] = {\n",
    "                'hit_rate': metrics['hit_rate'],\n",
    "                'mrr': metrics['mrr'],\n",
    "                'total_questions': metrics['total_questions']\n",
    "            }\n",
    "                \n",
    "        print(f\"Loaded {method} results\")\n",
    "    else:\n",
    "        print(f\"Warning: {result_file} not found\")\n",
    "\n",
    "print(f\"Loaded {len(retrieval_results)} retrieval methods\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load RAG Evaluation Results\n",
    "Load end-to-end RAG evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load RAG evaluation results\n",
    "rag_file = results_dir / 'rag_evaluation_results.csv'\n",
    "\n",
    "if rag_file.exists():\n",
    "    rag_results = pd.read_csv(rag_file)\n",
    "    print(f\"Loaded RAG results: {len(rag_results)} evaluations\")\n",
    "else:\n",
    "    print(f\"Warning: {rag_file} not found\")\n",
    "    rag_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Retrieval Comparison Visualizations\n",
    "Generate bar plots comparing different retrieval methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retrieval comparison plots\n",
    "if not retrieval_results:\n",
    "    print(\"No retrieval results to plot\")\n",
    "else:\n",
    "    # Extract data for plotting\n",
    "    methods = list(retrieval_results.keys())\n",
    "    hit_rates = [retrieval_results[method]['hit_rate'] for method in methods]\n",
    "    mrrs = [retrieval_results[method]['mrr'] for method in methods]\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Hit Rate comparison\n",
    "    bars1 = ax1.bar(methods, hit_rates, color=['skyblue', 'lightgreen', 'lightcoral'])\n",
    "    ax1.set_title('Hit Rate Comparison')\n",
    "    ax1.set_ylabel('Hit Rate')\n",
    "    ax1.set_ylim(0, 1)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars1, hit_rates):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{value:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # MRR comparison\n",
    "    bars2 = ax2.bar(methods, mrrs, color=['skyblue', 'lightgreen', 'lightcoral'])\n",
    "    ax2.set_title('Mean Reciprocal Rank Comparison')\n",
    "    ax2.set_ylabel('MRR')\n",
    "    ax2.set_ylim(0, 1)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars2, mrrs):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{value:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(results_dir / 'retrieval_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Retrieval comparison plots created and saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create RAG Comparison Visualizations\n",
    "Generate plots comparing RAG performance across different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RAG comparison plots\n",
    "if rag_results is None:\n",
    "    print(\"No RAG results to plot\")\n",
    "else:\n",
    "    # Create 2x2 subplot layout\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Cosine similarity distributions\n",
    "    if 'cosine_perplexity' in rag_results.columns:\n",
    "        sns.histplot(data=rag_results, x='cosine_perplexity', ax=ax1, alpha=0.7, color='blue')\n",
    "        ax1.set_title('Perplexity - Cosine Similarity Distribution')\n",
    "        ax1.set_xlabel('Cosine Similarity')\n",
    "        mean_perplexity = rag_results['cosine_perplexity'].mean()\n",
    "        ax1.axvline(mean_perplexity, color='red', linestyle='--', \n",
    "                   label=f'Mean: {mean_perplexity:.3f}')\n",
    "        ax1.legend()\n",
    "    \n",
    "    if 'cosine_phi3' in rag_results.columns:\n",
    "        sns.histplot(data=rag_results, x='cosine_phi3', ax=ax2, alpha=0.7, color='green')\n",
    "        ax2.set_title('Phi3 - Cosine Similarity Distribution')\n",
    "        ax2.set_xlabel('Cosine Similarity')\n",
    "        mean_phi3 = rag_results['cosine_phi3'].mean()\n",
    "        ax2.axvline(mean_phi3, color='red', linestyle='--',\n",
    "                   label=f'Mean: {mean_phi3:.3f}')\n",
    "        ax2.legend()\n",
    "    \n",
    "    # LLM-as-Judge results\n",
    "    if 'judge_perplexity' in rag_results.columns:\n",
    "        judge_perplexity_counts = rag_results['judge_perplexity'].value_counts()\n",
    "        ax3.bar(judge_perplexity_counts.index, judge_perplexity_counts.values, color='blue', alpha=0.7)\n",
    "        ax3.set_title('Perplexity - LLM Judge Evaluation')\n",
    "        ax3.set_ylabel('Count')\n",
    "        ax3.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    if 'judge_phi3' in rag_results.columns:\n",
    "        judge_phi3_counts = rag_results['judge_phi3'].value_counts()\n",
    "        ax4.bar(judge_phi3_counts.index, judge_phi3_counts.values, color='green', alpha=0.7)\n",
    "        ax4.set_title('Phi3 - LLM Judge Evaluation')\n",
    "        ax4.set_ylabel('Count')\n",
    "        ax4.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(results_dir / 'rag_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"RAG comparison plots created and saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Summary Table\n",
    "Generate comprehensive comparison table of all methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary table\n",
    "summary_data = []\n",
    "\n",
    "# Add retrieval results\n",
    "for method, results in retrieval_results.items():\n",
    "    row = {\n",
    "        'Method': method.replace('_', ' ').title(),\n",
    "        'Type': 'Retrieval',\n",
    "        'Hit Rate': f\"{results['hit_rate']:.4f}\",\n",
    "        'MRR': f\"{results['mrr']:.4f}\",\n",
    "        'Total Questions': results['total_questions']\n",
    "    }\n",
    "    \n",
    "    # Add alpha information for hybrid search\n",
    "    if 'best_alpha' in results:\n",
    "        alpha_value = results['best_alpha'].split('_')[1]\n",
    "        row['Method'] = f\"{row['Method']} (Î±={alpha_value})\"\n",
    "    \n",
    "    summary_data.append(row)\n",
    "\n",
    "# Add RAG results\n",
    "if rag_results is not None:\n",
    "    total_questions = len(rag_results)\n",
    "    \n",
    "    # Perplexity results\n",
    "    if 'judge_perplexity' in rag_results.columns:\n",
    "        perplexity_relevant = (rag_results['judge_perplexity'] == 'RELEVANT').sum()\n",
    "        perplexity_partly = (rag_results['judge_perplexity'] == 'PARTLY_RELEVANT').sum()\n",
    "        \n",
    "        summary_data.append({\n",
    "            'Method': 'RAG + Perplexity',\n",
    "            'Type': 'End-to-End',\n",
    "            'Cosine Mean': f\"{rag_results['cosine_perplexity'].mean():.4f}\",\n",
    "            'Cosine Std': f\"{rag_results['cosine_perplexity'].std():.4f}\",\n",
    "            'Relevant %': f\"{perplexity_relevant/total_questions*100:.1f}%\",\n",
    "            'Partly Relevant %': f\"{perplexity_partly/total_questions*100:.1f}%\"\n",
    "        })\n",
    "    \n",
    "    # Phi3 results\n",
    "    if 'judge_phi3' in rag_results.columns:\n",
    "        phi3_relevant = (rag_results['judge_phi3'] == 'RELEVANT').sum()\n",
    "        phi3_partly = (rag_results['judge_phi3'] == 'PARTLY_RELEVANT').sum()\n",
    "        \n",
    "        summary_data.append({\n",
    "            'Method': 'RAG + Phi3',\n",
    "            'Type': 'End-to-End',\n",
    "            'Cosine Mean': f\"{rag_results['cosine_phi3'].mean():.4f}\",\n",
    "            'Cosine Std': f\"{rag_results['cosine_phi3'].std():.4f}\",\n",
    "            'Relevant %': f\"{phi3_relevant/total_questions*100:.1f}%\",\n",
    "            'Partly Relevant %': f\"{phi3_partly/total_questions*100:.1f}%\"\n",
    "        })\n",
    "\n",
    "# Create DataFrame and display\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"Summary Table:\")\n",
    "print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate HTML Report\n",
    "Create a comprehensive HTML report with all results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive HTML report\n",
    "html_content = f\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>RAG Evaluation Report</title>\n",
    "    <style>\n",
    "        body {{ font-family: Arial, sans-serif; margin: 20px; }}\n",
    "        .header {{ background-color: #f0f0f0; padding: 20px; border-radius: 5px; }}\n",
    "        .section {{ margin: 20px 0; }}\n",
    "        table {{ border-collapse: collapse; width: 100%; }}\n",
    "        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}\n",
    "        th {{ background-color: #f2f2f2; }}\n",
    "        .metric {{ background-color: #e8f4f8; padding: 10px; margin: 10px 0; border-radius: 5px; }}\n",
    "        .best {{ background-color: #d4edda; }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"header\">\n",
    "        <h1>RAG System Evaluation Report</h1>\n",
    "        <p>Complete evaluation of retrieval methods and end-to-end RAG performance</p>\n",
    "    </div>\n",
    "    \n",
    "    <div class=\"section\">\n",
    "        <h2>Executive Summary</h2>\n",
    "        <div class=\"metric\">\n",
    "            <h3>Best Performing Methods</h3>\n",
    "\"\"\"\n",
    "\n",
    "# Find best retrieval method\n",
    "if retrieval_results:\n",
    "    best_retrieval = max(retrieval_results.items(), key=lambda x: x[1]['mrr'])\n",
    "    html_content += f\"\"\"\n",
    "            <p><strong>Best Retrieval Method:</strong> {best_retrieval[0].replace('_', ' ').title()} \n",
    "            (MRR: {best_retrieval[1]['mrr']:.4f}, Hit Rate: {best_retrieval[1]['hit_rate']:.4f})</p>\n",
    "\"\"\"\n",
    "\n",
    "# Best LLM if RAG results exist\n",
    "if rag_results is not None and 'cosine_perplexity' in rag_results.columns:\n",
    "    perplexity_mean = rag_results['cosine_perplexity'].mean()\n",
    "    phi3_mean = rag_results['cosine_phi3'].mean()\n",
    "    best_llm = \"Perplexity\" if perplexity_mean > phi3_mean else \"Phi3\"\n",
    "    best_score = max(perplexity_mean, phi3_mean)\n",
    "    \n",
    "    html_content += f\"\"\"\n",
    "            <p><strong>Best RAG LLM:</strong> {best_llm} (Cosine Similarity: {best_score:.4f})</p>\n",
    "\"\"\"\n",
    "\n",
    "html_content += \"\"\"\n",
    "        </div>\n",
    "    </div>\n",
    "\"\"\"\n",
    "\n",
    "# Add summary table\n",
    "html_content += f\"\"\"\n",
    "    <div class=\"section\">\n",
    "        <h2>Results Summary</h2>\n",
    "        {summary_df.to_html(index=False, classes='summary-table')}\n",
    "    </div>\n",
    "\"\"\"\n",
    "\n",
    "# Detailed results sections\n",
    "html_content += \"\"\"\n",
    "    <div class=\"section\">\n",
    "        <h2>Detailed Results</h2>\n",
    "        <h3>Retrieval Methods Comparison</h3>\n",
    "\"\"\"\n",
    "\n",
    "for method, results in retrieval_results.items():\n",
    "    html_content += f\"\"\"\n",
    "        <div class=\"metric\">\n",
    "            <h4>{method.replace('_', ' ').title()}</h4>\n",
    "            <p>Hit Rate: {results['hit_rate']:.4f} | MRR: {results['mrr']:.4f} | Questions: {results['total_questions']}</p>\n",
    "        </div>\n",
    "\"\"\"\n",
    "\n",
    "if rag_results is not None:\n",
    "    html_content += \"\"\"\n",
    "        <h3>RAG Performance</h3>\n",
    "\"\"\"\n",
    "    \n",
    "    total = len(rag_results)\n",
    "    \n",
    "    # Perplexity stats\n",
    "    if 'judge_perplexity' in rag_results.columns:\n",
    "        perp_relevant = (rag_results['judge_perplexity'] == 'RELEVANT').sum()\n",
    "        perp_partly = (rag_results['judge_perplexity'] == 'PARTLY_RELEVANT').sum()\n",
    "        perp_non = (rag_results['judge_perplexity'] == 'NON_RELEVANT').sum()\n",
    "        \n",
    "        html_content += f\"\"\"\n",
    "        <div class=\"metric\">\n",
    "            <h4>Perplexity (Sonar Model)</h4>\n",
    "            <p>Cosine Similarity: {rag_results['cosine_perplexity'].mean():.4f} Â± {rag_results['cosine_perplexity'].std():.4f}</p>\n",
    "            <p>LLM Judge: Relevant: {perp_relevant} ({perp_relevant/total*100:.1f}%), \n",
    "               Partly Relevant: {perp_partly} ({perp_partly/total*100:.1f}%), \n",
    "               Non-Relevant: {perp_non} ({perp_non/total*100:.1f}%)</p>\n",
    "        </div>\n",
    "\"\"\"\n",
    "    \n",
    "    # Phi3 stats\n",
    "    if 'judge_phi3' in rag_results.columns:\n",
    "        phi3_relevant = (rag_results['judge_phi3'] == 'RELEVANT').sum()\n",
    "        phi3_partly = (rag_results['judge_phi3'] == 'PARTLY_RELEVANT').sum()\n",
    "        phi3_non = (rag_results['judge_phi3'] == 'NON_RELEVANT').sum()\n",
    "        \n",
    "        html_content += f\"\"\"\n",
    "        <div class=\"metric\">\n",
    "            <h4>Phi3 (Local Model)</h4>\n",
    "            <p>Cosine Similarity: {rag_results['cosine_phi3'].mean():.4f} Â± {rag_results['cosine_phi3'].std():.4f}</p>\n",
    "            <p>LLM Judge: Relevant: {phi3_relevant} ({phi3_relevant/total*100:.1f}%), \n",
    "               Partly Relevant: {phi3_partly} ({phi3_partly/total*100:.1f}%), \n",
    "               Non-Relevant: {phi3_non} ({phi3_non/total*100:.1f}%)</p>\n",
    "        </div>\n",
    "\"\"\"\n",
    "\n",
    "html_content += \"\"\"\n",
    "    </div>\n",
    "    \n",
    "    <div class=\"section\">\n",
    "        <h2>Recommendations</h2>\n",
    "        <ul>\n",
    "\"\"\"\n",
    "\n",
    "# Add recommendations based on results\n",
    "if retrieval_results:\n",
    "    best_method = max(retrieval_results.items(), key=lambda x: x[1]['mrr'])[0]\n",
    "    html_content += f\"<li>Use <strong>{best_method.replace('_', ' ')}</strong> for retrieval based on best MRR performance</li>\"\n",
    "\n",
    "if rag_results is not None and 'cosine_perplexity' in rag_results.columns:\n",
    "    perplexity_mean = rag_results['cosine_perplexity'].mean()\n",
    "    phi3_mean = rag_results['cosine_phi3'].mean()\n",
    "    \n",
    "    if perplexity_mean > phi3_mean:\n",
    "        html_content += \"<li>Perplexity shows better semantic similarity - consider for production if API costs are acceptable</li>\"\n",
    "        html_content += \"<li>Phi3 provides reasonable local alternative with lower operational costs</li>\"\n",
    "    else:\n",
    "        html_content += \"<li>Phi3 local model performs competitively - excellent choice for cost-sensitive deployments</li>\"\n",
    "\n",
    "html_content += \"\"\"\n",
    "        </ul>\n",
    "    </div>\n",
    "    \n",
    "    <div class=\"section\">\n",
    "        <p><em>Report generated automatically from evaluation results</em></p>\n",
    "    </div>\n",
    "    \n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Save HTML report\n",
    "with open(results_dir / 'evaluation_report.html', 'w') as f:\n",
    "    f.write(html_content)\n",
    "\n",
    "print(\"HTML report generated and saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Summary Data and Final Report\n",
    "Save all summary data and display final recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summary table to CSV\n",
    "summary_df.to_csv(results_dir / 'evaluation_summary.csv', index=False)\n",
    "\n",
    "# Display completion summary\n",
    "print(f\"\\n=== Evaluation Summary Complete ===\")\n",
    "print(f\"ð Plots saved to: {results_dir}\")\n",
    "print(f\"ð Summary table: {results_dir / 'evaluation_summary.csv'}\")\n",
    "print(f\"ð Full report: {results_dir / 'evaluation_report.html'}\")\n",
    "\n",
    "print(\"\\n=== Final Recommendations ===\")\n",
    "\n",
    "# Print key insights\n",
    "if retrieval_results:\n",
    "    best_retrieval = max(retrieval_results.items(), key=lambda x: x[1]['mrr'])\n",
    "    print(f\"â Best Retrieval: {best_retrieval[0].replace('_', ' ').title()} (MRR: {best_retrieval[1]['mrr']:.4f})\")\n",
    "\n",
    "if rag_results is not None and 'cosine_perplexity' in rag_results.columns:\n",
    "    perp_mean = rag_results['cosine_perplexity'].mean()\n",
    "    phi3_mean = rag_results['cosine_phi3'].mean()\n",
    "    \n",
    "    print(f\"â Perplexity Cosine Similarity: {perp_mean:.4f}\")\n",
    "    print(f\"â Phi3 Cosine Similarity: {phi3_mean:.4f}\")\n",
    "    \n",
    "    if perp_mean > phi3_mean:\n",
    "        print(\"ð¡ Recommendation: Perplexity