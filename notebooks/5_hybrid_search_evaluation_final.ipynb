{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40d70962",
   "metadata": {},
   "source": [
    "## Hybrid Search Evaluation with Qdrant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d57343f",
   "metadata": {},
   "source": [
    "This notebook demonstrates hybrid search combining dense embeddings and sparse BM25 vectors using Qdrant. We'll evaluate different search approaches and compare their performance using RRF (Reciprocal Rank Fusion).\n",
    "\n",
    "Overview\n",
    "Dense vectors: Capture semantic meaning, good for natural language queries\n",
    "Sparse vectors (BM25): Excel at exact keyword matches, fast and lightweight\n",
    "Hybrid Search: Combines both approaches for better overall performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997ba70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!python -m pip install -q \"qdrant-client[fastembed]>=1.14.2\" pandas numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5893de0d",
   "metadata": {},
   "source": [
    "### Start Qdrant server in Docker (run this once)\n",
    "!docker run -d -p 6333:6333 -p 6334:6334 \\\n",
    "   -v \"./qdrant_storage:/qdrant/storage:z\" \\\n",
    "   qdrant/qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360ad04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import os\n",
    "import pickle\n",
    "from typing import List, Dict, Any\n",
    "import numpy as np\n",
    "from qdrant_client import QdrantClient, models\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33940e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents and ground truth data\n",
    "with open('../data/processed/documents-with-ids.json', 'r') as f:\n",
    "    documents = json.load(f)\n",
    "\n",
    "df_ground_truth = pd.read_csv('../data/processed/ground-truth-retrieval.csv')\n",
    "ground_truth = df_ground_truth.to_dict(orient='records')\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents and {len(ground_truth)} ground truth questions\")\n",
    "print(f\"First document keys: {list(documents[0].keys()) if documents else 'No documents'}\")\n",
    "print(f\"Ground truth columns: {df_ground_truth.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a14169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Qdrant server\n",
    "client = QdrantClient(\"http://localhost:6333\")\n",
    "\n",
    "# Test connection by listing collections\n",
    "try:\n",
    "    collections = client.get_collections()\n",
    "    print(f\"Connected to Qdrant. Existing collections: {[c.name for c in collections.collections]}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to connect to Qdrant: {e}\")\n",
    "    print(\"Make sure Qdrant is running on localhost:6333\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc462f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine data structure\n",
    "print(\"Sample document:\")\n",
    "print(json.dumps(documents[0], indent=2))\n",
    "print(\"\\nSample ground truth entry:\")\n",
    "print(ground_truth[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66db6608",
   "metadata": {},
   "source": [
    "### Create Sparse Vector Collection (BM25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa54149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create collection for sparse vectors only\n",
    "collection_name_sparse = \"documents-sparse-bm25\"\n",
    "\n",
    "# Delete collection if it exists\n",
    "try:\n",
    "    client.delete_collection(collection_name_sparse)\n",
    "    print(f\"Deleted existing collection: {collection_name_sparse}\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Create new collection with sparse vector configuration\n",
    "client.create_collection(\n",
    "    collection_name=collection_name_sparse,\n",
    "    sparse_vectors_config={\n",
    "        \"bm25\": models.SparseVectorParams(\n",
    "            modifier=models.Modifier.IDF,  # Enable IDF calculation for BM25\n",
    "        )\n",
    "    }\n",
    ")\n",
    "print(f\"Created sparse collection: {collection_name_sparse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80e4be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload documents to sparse collection\n",
    "def upload_sparse_vectors(documents: List[Dict], collection_name: str) -> None:\n",
    "    \"\"\"Upload documents as sparse vectors using BM25 model\"\"\"\n",
    "    \n",
    "    points = []\n",
    "    for doc in documents:\n",
    "        # Create point with sparse BM25 vector\n",
    "        point = models.PointStruct(\n",
    "            id=doc.get('id', uuid.uuid4().hex),\n",
    "            vector={\n",
    "                \"bm25\": models.Document(\n",
    "                    text=doc[\"text\"], \n",
    "                    model=\"Qdrant/bm25\",\n",
    "                ),\n",
    "            },\n",
    "            payload={\n",
    "                \"text\": doc[\"text\"],\n",
    "                \"section\": doc.get(\"section\", \"\"),\n",
    "                \"course\": doc.get(\"course\", \"\"),\n",
    "                \"question\": doc.get(\"question\", \"\"),\n",
    "                \"id\": doc.get('id', uuid.uuid4().hex)\n",
    "            }\n",
    "        )\n",
    "        points.append(point)\n",
    "    \n",
    "    # Upload in batches for better performance\n",
    "    batch_size = 100\n",
    "    for i in range(0, len(points), batch_size):\n",
    "        batch = points[i:i+batch_size]\n",
    "        client.upsert(collection_name=collection_name, points=batch)\n",
    "        print(f\"Uploaded batch {i//batch_size + 1}/{(len(points)-1)//batch_size + 1}\")\n",
    "    \n",
    "    print(f\"Successfully uploaded {len(points)} documents to {collection_name}\")\n",
    "\n",
    "# Upload documents\n",
    "start_time = time.time()\n",
    "upload_sparse_vectors(documents, collection_name_sparse)\n",
    "upload_time_sparse = time.time() - start_time\n",
    "print(f\"Sparse vector upload took {upload_time_sparse:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346e034a",
   "metadata": {},
   "source": [
    "### Sparse Vector Search Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ad8219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_search(query: str, collection_name: str, limit: int = 5) -> List[models.ScoredPoint]:\n",
    "    \"\"\"Perform sparse vector search using BM25\"\"\"\n",
    "    \n",
    "    results = client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=models.Document(\n",
    "            text=query,\n",
    "            model=\"Qdrant/bm25\",\n",
    "        ),\n",
    "        using=\"bm25\",\n",
    "        limit=limit,\n",
    "        with_payload=True,\n",
    "    )\n",
    "    \n",
    "    return results.points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32b1ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sparse search\n",
    "test_query = \"How do I install dependencies?\"\n",
    "sparse_results = sparse_search(test_query, collection_name_sparse, limit=3)\n",
    "\n",
    "print(f\"Sparse search results for: '{test_query}'\")\n",
    "for i, result in enumerate(sparse_results, 1):\n",
    "    print(f\"\\n{i}. Score: {result.score:.4f}\")\n",
    "    print(f\"   Text: {result.payload['text'][:150]}...\")\n",
    "    print(f\"   Section: {result.payload.get('section', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e66783",
   "metadata": {},
   "source": [
    "### Create Hybrid Collection (Dense + Sparse Vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7f822f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create collection for hybrid search (both dense and sparse vectors)\n",
    "collection_name_hybrid = \"documents-hybrid-search\"\n",
    "\n",
    "# Delete collection if it exists\n",
    "try:\n",
    "    client.delete_collection(collection_name_hybrid)\n",
    "    print(f\"Deleted existing collection: {collection_name_hybrid}\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Create hybrid collection with both vector types\n",
    "client.create_collection(\n",
    "    collection_name=collection_name_hybrid,\n",
    "    vectors_config={\n",
    "        # Dense vector configuration\n",
    "        \"jina-small\": models.VectorParams(\n",
    "            size=512,  # Dimension size for jina-embeddings-v2-small-en\n",
    "            distance=models.Distance.COSINE,\n",
    "        ),\n",
    "    },\n",
    "    sparse_vectors_config={\n",
    "        # Sparse vector configuration  \n",
    "        \"bm25\": models.SparseVectorParams(\n",
    "            modifier=models.Modifier.IDF,\n",
    "        )\n",
    "    }\n",
    ")\n",
    "print(f\"Created hybrid collection: {collection_name_hybrid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace0683e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload documents to hybrid collection with both vector types\n",
    "def upload_hybrid_vectors(documents: List[Dict], collection_name: str) -> None:\n",
    "    \"\"\"Upload documents with both dense and sparse vectors\"\"\"\n",
    "    \n",
    "    points = []\n",
    "    for doc in documents:\n",
    "        # Create point with both dense and sparse vectors\n",
    "        point = models.PointStruct(\n",
    "            id=doc.get('id', uuid.uuid4().hex),\n",
    "            vector={\n",
    "                # Dense vector using Jina model\n",
    "                \"jina-small\": models.Document(\n",
    "                    text=doc[\"text\"],\n",
    "                    model=\"jinaai/jina-embeddings-v2-small-en\",\n",
    "                ),\n",
    "                # Sparse vector using BM25\n",
    "                \"bm25\": models.Document(\n",
    "                    text=doc[\"text\"], \n",
    "                    model=\"Qdrant/bm25\",\n",
    "                ),\n",
    "            },\n",
    "            payload={\n",
    "                \"text\": doc[\"text\"],\n",
    "                \"section\": doc.get(\"section\", \"\"),\n",
    "                \"course\": doc.get(\"course\", \"\"),\n",
    "                \"question\": doc.get(\"question\", \"\"),\n",
    "                \"id\": doc.get('id', uuid.uuid4().hex)\n",
    "            }\n",
    "        )\n",
    "        points.append(point)\n",
    "    \n",
    "    # Upload in batches\n",
    "    batch_size = 50  # Smaller batches for hybrid upload\n",
    "    for i in range(0, len(points), batch_size):\n",
    "        batch = points[i:i+batch_size]\n",
    "        client.upsert(collection_name=collection_name, points=batch)\n",
    "        print(f\"Uploaded hybrid batch {i//batch_size + 1}/{(len(points)-1)//batch_size + 1}\")\n",
    "    \n",
    "    print(f\"Successfully uploaded {len(points)} documents with hybrid vectors\")\n",
    "\n",
    "# Upload documents (this will take longer due to dense embeddings)\n",
    "start_time = time.time()\n",
    "upload_hybrid_vectors(documents, collection_name_hybrid)\n",
    "upload_time_hybrid = time.time() - start_time\n",
    "print(f\"Hybrid vector upload took {upload_time_hybrid:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903b21e5",
   "metadata": {},
   "source": [
    "### Hybrid Search Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994e9884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_search(query: str, collection_name: str, limit: int = 5) -> List[models.ScoredPoint]:\n",
    "    \"\"\"Perform dense vector search using semantic embeddings\"\"\"\n",
    "    \n",
    "    results = client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=models.Document(\n",
    "            text=query,\n",
    "            model=\"jinaai/jina-embeddings-v2-small-en\",\n",
    "        ),\n",
    "        using=\"jina-small\",\n",
    "        limit=limit,\n",
    "        with_payload=True,\n",
    "    )\n",
    "    \n",
    "    return results.points\n",
    "\n",
    "def multi_stage_search(query: str, collection_name: str, limit: int = 5) -> List[models.ScoredPoint]:\n",
    "    \"\"\"Multi-stage search: dense retrieval followed by sparse reranking\"\"\"\n",
    "    \n",
    "    results = client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        prefetch=[\n",
    "            models.Prefetch(\n",
    "                query=models.Document(\n",
    "                    text=query,\n",
    "                    model=\"jinaai/jina-embeddings-v2-small-en\",\n",
    "                ),\n",
    "                using=\"jina-small\",\n",
    "                limit=(10 * limit),  # Prefetch more candidates for reranking\n",
    "            ),\n",
    "        ],\n",
    "        query=models.Document(\n",
    "            text=query,\n",
    "            model=\"Qdrant/bm25\", \n",
    "        ),\n",
    "        using=\"bm25\",\n",
    "        limit=limit,\n",
    "        with_payload=True,\n",
    "    )\n",
    "    \n",
    "    return results.points\n",
    "\n",
    "def rrf_search(query: str, collection_name: str, limit: int = 5) -> List[models.ScoredPoint]:\n",
    "    \"\"\"Reciprocal Rank Fusion combining dense and sparse search\"\"\"\n",
    "    \n",
    "    results = client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        prefetch=[\n",
    "            # Dense vector prefetch\n",
    "            models.Prefetch(\n",
    "                query=models.Document(\n",
    "                    text=query,\n",
    "                    model=\"jinaai/jina-embeddings-v2-small-en\",\n",
    "                ),\n",
    "                using=\"jina-small\",\n",
    "                limit=(5 * limit),  # Get more candidates for fusion\n",
    "            ),\n",
    "            # Sparse vector prefetch\n",
    "            models.Prefetch(\n",
    "                query=models.Document(\n",
    "                    text=query,\n",
    "                    model=\"Qdrant/bm25\",\n",
    "                ),\n",
    "                using=\"bm25\",\n",
    "                limit=(5 * limit),\n",
    "            ),\n",
    "        ],\n",
    "        # Apply RRF fusion to combine results\n",
    "        query=models.FusionQuery(fusion=models.Fusion.RRF),\n",
    "        limit=limit,\n",
    "        with_payload=True,\n",
    "    )\n",
    "    \n",
    "    return results.points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f688d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test all search methods with the same query\n",
    "test_query = \"How to set up environment variables?\"\n",
    "\n",
    "print(f\"Testing search methods with query: '{test_query}'\\n\")\n",
    "\n",
    "# Dense search\n",
    "dense_results = dense_search(test_query, collection_name_hybrid, limit=3)\n",
    "print(\"=== DENSE SEARCH RESULTS ===\")\n",
    "for i, result in enumerate(dense_results, 1):\n",
    "    print(f\"{i}. Score: {result.score:.4f} | {result.payload['text'][:100]}...\")\n",
    "\n",
    "# Sparse search on hybrid collection\n",
    "sparse_results_hybrid = sparse_search(test_query, collection_name_hybrid, limit=3)\n",
    "print(\"\\n=== SPARSE SEARCH RESULTS ===\")\n",
    "for i, result in enumerate(sparse_results_hybrid, 1):\n",
    "    print(f\"{i}. Score: {result.score:.4f} | {result.payload['text'][:100]}...\")\n",
    "\n",
    "# Multi-stage search\n",
    "multi_stage_results = multi_stage_search(test_query, collection_name_hybrid, limit=3)\n",
    "print(\"\\n=== MULTI-STAGE SEARCH RESULTS ===\")\n",
    "for i, result in enumerate(multi_stage_results, 1):\n",
    "    print(f\"{i}. Score: {result.score:.4f} | {result.payload['text'][:100]}...\")\n",
    "\n",
    "# RRF search\n",
    "rrf_results = rrf_search(test_query, collection_name_hybrid, limit=3)\n",
    "print(\"\\n=== RRF HYBRID SEARCH RESULTS ===\")\n",
    "for i, result in enumerate(rrf_results, 1):\n",
    "    print(f\"{i}. Score: {result.score:.4f} | {result.payload['text'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed04068d",
   "metadata": {},
   "source": [
    "### Evaluation Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960a3081",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_search_method(search_function, collection_name: str, ground_truth_data: List[Dict], \n",
    "                         method_name: str, top_k: int = 5) -> Dict[str, Any]:\n",
    "    \"\"\"Evaluate a search method against ground truth data\"\"\"\n",
    "    \n",
    "    results = {\n",
    "        'method': method_name,\n",
    "        'total_queries': len(ground_truth_data),\n",
    "        'hits_at_1': 0,\n",
    "        'hits_at_3': 0, \n",
    "        'hits_at_5': 0,\n",
    "        'mrr_scores': [],  # Mean Reciprocal Rank\n",
    "        'search_times': [],\n",
    "        'failed_queries': 0\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nEvaluating {method_name}...\")\n",
    "    \n",
    "    for i, gt_item in enumerate(ground_truth_data):\n",
    "        if i % 20 == 0:\n",
    "            print(f\"Processed {i}/{len(ground_truth_data)} queries\")\n",
    "            \n",
    "        query = gt_item['question']\n",
    "        expected_doc_id = gt_item['document_id']\n",
    "        \n",
    "        try:\n",
    "            # Measure search time\n",
    "            start_time = time.time()\n",
    "            search_results = search_function(query, collection_name, limit=top_k)\n",
    "            search_time = time.time() - start_time\n",
    "            results['search_times'].append(search_time)\n",
    "            \n",
    "            # Extract document IDs from results\n",
    "            retrieved_doc_ids = [result.payload.get('id') for result in search_results]\n",
    "            \n",
    "            # Calculate hits@k and MRR\n",
    "            if expected_doc_id in retrieved_doc_ids:\n",
    "                rank = retrieved_doc_ids.index(expected_doc_id) + 1\n",
    "                \n",
    "                # Hits@k calculation\n",
    "                if rank <= 1:\n",
    "                    results['hits_at_1'] += 1\n",
    "                if rank <= 3:\n",
    "                    results['hits_at_3'] += 1\n",
    "                if rank <= 5:\n",
    "                    results['hits_at_5'] += 1\n",
    "                    \n",
    "                # MRR calculation\n",
    "                results['mrr_scores'].append(1.0 / rank)\n",
    "            else:\n",
    "                results['mrr_scores'].append(0.0)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Failed query {i}: {e}\")\n",
    "            results['failed_queries'] += 1\n",
    "            results['mrr_scores'].append(0.0)\n",
    "            results['search_times'].append(0.0)\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    total_queries = results['total_queries']\n",
    "    results['hit_rate_at_1'] = results['hits_at_1'] / total_queries\n",
    "    results['hit_rate_at_3'] = results['hits_at_3'] / total_queries  \n",
    "    results['hit_rate_at_5'] = results['hits_at_5'] / total_queries\n",
    "    results['mean_reciprocal_rank'] = np.mean(results['mrr_scores'])\n",
    "    results['avg_search_time'] = np.mean(results['search_times'])\n",
    "    \n",
    "    print(f\"Completed evaluation of {method_name}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aac837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive evaluation on all methods\n",
    "print(\"Starting comprehensive evaluation...\")\n",
    "print(f\"Evaluating on {len(ground_truth)} ground truth questions\")\n",
    "\n",
    "evaluation_results = []\n",
    "\n",
    "# Evaluate sparse search (BM25 only)\n",
    "sparse_eval = evaluate_search_method(\n",
    "    lambda q, c, l: sparse_search(q, collection_name_sparse, l), \n",
    "    collection_name_sparse,\n",
    "    ground_truth[:100],  # Use subset for faster evaluation\n",
    "    \"BM25 Sparse\"\n",
    ")\n",
    "evaluation_results.append(sparse_eval)\n",
    "\n",
    "# Evaluate dense search\n",
    "dense_eval = evaluate_search_method(\n",
    "    dense_search,\n",
    "    collection_name_hybrid, \n",
    "    ground_truth[:100],\n",
    "    \"Dense Semantic\"\n",
    ")\n",
    "evaluation_results.append(dense_eval)\n",
    "\n",
    "# Evaluate multi-stage search\n",
    "multi_stage_eval = evaluate_search_method(\n",
    "    multi_stage_search,\n",
    "    collection_name_hybrid,\n",
    "    ground_truth[:100], \n",
    "    \"Multi-stage (Dense→Sparse)\"\n",
    ")\n",
    "evaluation_results.append(multi_stage_eval)\n",
    "\n",
    "# Evaluate RRF hybrid search\n",
    "rrf_eval = evaluate_search_method(\n",
    "    rrf_search,\n",
    "    collection_name_hybrid,\n",
    "    ground_truth[:100],\n",
    "    \"RRF Hybrid\"\n",
    ")\n",
    "evaluation_results.append(rrf_eval)\n",
    "\n",
    "print(\"\\nEvaluation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6a705f",
   "metadata": {},
   "source": [
    "### Results Analysis and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4471224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive results summary\n",
    "def create_results_summary(evaluation_results: List[Dict]) -> pd.DataFrame:\n",
    "    \"\"\"Create a summary DataFrame of all evaluation results\"\"\"\n",
    "    \n",
    "    summary_data = []\n",
    "    \n",
    "    for result in evaluation_results:\n",
    "        summary_data.append({\n",
    "            'Method': result['method'],\n",
    "            'Hit Rate @1': f\"{result['hit_rate_at_1']:.3f}\",\n",
    "            'Hit Rate @3': f\"{result['hit_rate_at_3']:.3f}\", \n",
    "            'Hit Rate @5': f\"{result['hit_rate_at_5']:.3f}\",\n",
    "            'Mean Reciprocal Rank': f\"{result['mean_reciprocal_rank']:.3f}\",\n",
    "            'Avg Search Time (ms)': f\"{result['avg_search_time']*1000:.1f}\",\n",
    "            'Failed Queries': result['failed_queries'],\n",
    "            'Total Queries': result['total_queries']\n",
    "        })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brahman_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
