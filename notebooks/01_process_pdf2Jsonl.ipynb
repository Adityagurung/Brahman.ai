{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f664789",
   "metadata": {},
   "source": [
    "--------\n",
    "Processes all PDF files in the specified folder:\n",
    "1. Ingest PDF files from raw and Converts PDFs to Markdown using Marker.\n",
    "2. Splits text into smaller chunks using LangChain splitters.\n",
    "3. Assigns stable document IDs via MD5 hashing.\n",
    "4. Saves processed chunks to a JSONL file in processed directory\n",
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35887d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "import jsonlines\n",
    "from tqdm import tqdm\n",
    "from langchain_text_splitters import (\n",
    "    MarkdownHeaderTextSplitter,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "from marker.convert import convert_single_pdf\n",
    "from marker.logger import configure_logging\n",
    "from marker.models import load_all_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e43a428",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a6962e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 1Ô∏è‚É£ Utility: Clean up PDF filenames (remove spaces/dots)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def parse_file_name(filename):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        name_part = filename[:-4]\n",
    "        name_part = name_part.replace(\" \", \"\").replace(\".\", \"\")\n",
    "        cleaned_filename = name_part + \".pdf\"\n",
    "    else:\n",
    "        cleaned_filename = filename.replace(\" \", \"\").replace(\".\", \"\")\n",
    "    return cleaned_filename\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34039db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 2Ô∏è‚É£ Convert PDF ‚Üí Markdown using Marker\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def convert_pdf_to_markdown(fname, reference_folder, model_lst):\n",
    "    \"\"\"\n",
    "    Converts a PDF to markdown text using Marker models.\n",
    "    \"\"\"\n",
    "    md_filename = fname.rsplit(\".\", 1)[0] + \".md\"\n",
    "    pdf_filename = os.path.join(reference_folder, fname)\n",
    "\n",
    "    print(f\"üìÑ Converting PDF: {pdf_filename}\")\n",
    "    full_text, _, _ = convert_single_pdf(pdf_filename, model_lst, batch_multiplier=1)\n",
    "\n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20941509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 3Ô∏è‚É£ Process a single PDF into smaller text chunks with metadata\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def download_and_process_pdf_file(\n",
    "    f_key, text_splitter, markdown_splitter, model_lst, reference_folder=\"./raw/\"\n",
    "):\n",
    "    temp_file_name = parse_file_name(f_key)\n",
    "    temp_file_path = os.path.join(reference_folder, temp_file_name)\n",
    "\n",
    "    # Convert PDF to Markdown\n",
    "    mdfile = convert_pdf_to_markdown(temp_file_name, reference_folder, model_lst)\n",
    "\n",
    "    # Split markdown by headers (H1, H2, H3)\n",
    "    md_header_split = markdown_splitter.split_text(mdfile)\n",
    "\n",
    "    documents = []\n",
    "    for split in md_header_split:\n",
    "        # Further split into smaller overlapping chunks\n",
    "        split_texts = text_splitter.split_text(split.page_content)\n",
    "\n",
    "        for i, split_text in enumerate(split_texts):\n",
    "            # Create unique document ID based on filename + part number\n",
    "            document_id = f\"{f_key}_part_{i}\"\n",
    "            hash_object = hashlib.md5(document_id.encode())\n",
    "            hash_hex = hash_object.hexdigest()\n",
    "            document_id = hash_hex[:10]\n",
    "\n",
    "            # Metadata for tracking source & chunk position\n",
    "            metadata_dict = {\n",
    "                \"document_id\": document_id,\n",
    "                \"pdf_name\": f_key,\n",
    "                \"pdf_part\": i,\n",
    "            }\n",
    "            metadata_dict.update(split.metadata)\n",
    "\n",
    "            # Store final structured document\n",
    "            documents.append({\"metadata\": metadata_dict, \"content\": split_text})\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af96145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 4Ô∏è‚É£ List all PDF files in a given folder\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def list_pdf_files(directory_path):\n",
    "    filenames = [f for f in os.listdir(directory_path) if f.lower().endswith(\".pdf\")]\n",
    "    return filenames\n",
    "\n",
    "# Use the correct folder where PDFs actually exist\n",
    "pdf_folder = os.path.join(\"..\", \"data\", \"raw\") if os.getcwd().endswith(\"notebooks\") else os.path.join(\".\", \"data\", \"raw\")\n",
    "\n",
    "print(\"üìÇ Found PDF files:\", list_pdf_files(pdf_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2496a2d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e91db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 5Ô∏è‚É£ Main processing logic\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # No folder creation here ‚Äî we assume PDFs already exist\n",
    "    if not os.path.exists(pdf_folder):\n",
    "        raise FileNotFoundError(f\"PDF folder not found: {pdf_folder}\")\n",
    "\n",
    "    # List PDF files\n",
    "    filenames = list_pdf_files(pdf_folder)\n",
    "\n",
    "    if not filenames:\n",
    "        print(f\"No PDF files found in {pdf_folder}\")\n",
    "    else:\n",
    "        print(f\"Found {len(filenames)} PDF(s) in {pdf_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe003930",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load Marker models for PDF ‚Üí Markdown conversion\n",
    "configure_logging()\n",
    "model_lst = load_all_models()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089321c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup splitters for text segmentation\n",
    "headers_to_split_on = [(\"#\", \"Header 1\"), (\"##\", \"Header 2\"), (\"###\", \"Header 3\")]\n",
    "chunk_size = 500\n",
    "chunk_overlap = 100\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    ")\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37964a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store ALL chunks in a single flat list\n",
    "documents = []\n",
    "\n",
    "# Process each PDF ‚Üí chunks\n",
    "for filename in tqdm(filenames, desc=\"Processing PDFs\"):\n",
    "    print(f\"\\nüìë Processing: {filename}\")\n",
    "    splitted_doc = download_and_process_pdf_file(\n",
    "        filename,\n",
    "        text_splitter,\n",
    "        markdown_splitter,\n",
    "        model_lst,\n",
    "        reference_folder=pdf_folder,\n",
    "    )\n",
    "    documents.extend(splitted_doc)  # Keep list flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8af609a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 6Ô∏è‚É£ Save processed chunks to JSONL\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "output_dir = os.path.join(os.path.dirname(pdf_folder), \"processed\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "output_path = os.path.join(output_dir, \"docs_processed.jsonl\")\n",
    "with jsonlines.open(output_path, mode=\"w\") as writer:\n",
    "    writer.write_all(documents)\n",
    "\n",
    "print(f\"\\n‚úÖ Processing complete. Saved {len(documents)} chunks to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0d4fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(documents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brahman_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
